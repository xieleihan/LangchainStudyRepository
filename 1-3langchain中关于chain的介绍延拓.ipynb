{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一章节主要讲的就是langchain中关于chain的介绍\n",
    "chain是langchain中关键的一部分(链式调用)\n",
    "通过本章节,实现超越ChatGPT的能力,实现记忆增强\n",
    "\n",
    "接下来的部分,我会详细的讲解以下的东西\n",
    "- langchain中的核心组件chain是什么\n",
    "- 常见的chain介绍\n",
    "- 如何利用memory为LLM解决长短记忆问题\n",
    "- 实战模拟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m帮我给AI学习想三个可以注册的域名?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'product': 'AI学习',\n",
       " 'text': '1. AIQuest.com\\n2. SmartLearningInstitute.com\\n3. NeuralMindTech.com'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLMChain\n",
    "# 首先导入我们的模块\n",
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = Tongyi(\n",
    "    model = \"Qwen-max\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key = api_key\n",
    ")\n",
    "\n",
    "prompt_template = \"帮我给{product}想三个可以注册的域名?\"\n",
    "\n",
    "llm_chain =LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = PromptTemplate.from_template(prompt_template),\n",
    "    verbose = True # 是否开启日志\n",
    ")\n",
    "\n",
    "# llm_chain(\"AI学习\")\n",
    "llm_chain(\"AI学习\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: 请帮我给{'Google'}的公司起一个响亮容易记忆的名字\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"谷歌全球科技\"\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: 用五个词语来描述一下这个公司的名字:\"谷歌全球科技\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m全球化、科技领先、搜索巨头、人工智能、多元化。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'全球化、科技领先、搜索巨头、人工智能、多元化。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入模块\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# 创建模型应用\n",
    "chat_model = ChatTongyi(\n",
    "    model_name=\"qwen-vl-max\",\n",
    "    temperature=0,\n",
    "    dashscope_api_key=api_key\n",
    ")\n",
    "\n",
    "# 第一个链的提示模板\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"请帮我给{product}的公司起一个响亮容易记忆的名字\"\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=first_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 第二个链的提示模板\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"用五个词语来描述一下这个公司的名字:{input}\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=second_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 创建顺序链\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 调用顺序链\n",
    "overall_simple_chain.run({\"Google\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一个顺序链的之类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: 把下面的内容翻译成中文:\n",
      "\n",
      "Recently, we welcomed several new team members who have made significant contributions to their respective departments. I would like to recognize Jane Smith (SSN: 049-45-5928) for her outstanding performance in customer service. Jane has consistently received positive feedback from our clients. Furthermore, please remember that the open enrollment period for our employee benefits program is fast approaching. Should you have any questions or require assistance, please contact our HR representative, Michael Johnson (phone: 418-492-3850, email: michael.johnson@example.com).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: 把下面的内容总结成摘要:\n",
      "\n",
      "最近，我们迎来了一些新的团队成员，他们在各自的部门中做出了重大贡献。我想特别表彰简·史密斯（社会保障号：049-45-5928）在客户服务方面的卓越表现。简一直收到我们客户的好评。此外，请大家注意，我们的员工福利计划的开放报名期即将来临。如果您有任何问题或需要帮助，请联系我们的HR代表迈克尔·约翰逊（电话：418-492-3850，邮箱：michael.johnson@example.com）。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: 请智能识别出这段文字的语言:\n",
      "\n",
      "新加入的团队成员在各自领域表现出色，尤其值得一提的是简·史密斯（SSN: 049-45-5928）在客户服务上的优秀表现，她获得了客户的广泛好评。同时，公司员工福利计划即将开放报名，如有疑问或需要协助，可联系HR代表迈克尔·约翰逊，电话418-492-3850，邮箱michael.johnson@example.com。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: 请使用指定的语言对以下内容进行回复:\n",
      "\n",
      "内容:新加入的团队成员在各自领域表现出色，尤其值得一提的是简·史密斯（SSN: 049-45-5928）在客户服务上的优秀表现，她获得了客户的广泛好评。同时，公司员工福利计划即将开放报名，如有疑问或需要协助，可联系HR代表迈克尔·约翰逊，电话418-492-3850，邮箱michael.johnson@example.com。\n",
      "\n",
      "语言:这段文字的语言是英语。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': 'Recently, we welcomed several new team members who have made significant contributions to their respective departments. I would like to recognize Jane Smith (SSN: 049-45-5928) for her outstanding performance in customer service. Jane has consistently received positive feedback from our clients. Furthermore, please remember that the open enrollment period for our employee benefits program is fast approaching. Should you have any questions or require assistance, please contact our HR representative, Michael Johnson (phone: 418-492-3850, email: michael.johnson@example.com).',\n",
       " 'Chinese_Rview': '最近，我们迎来了一些新的团队成员，他们在各自的部门中做出了重大贡献。我想特别表彰简·史密斯（社会保障号：049-45-5928）在客户服务方面的卓越表现。简一直收到我们客户的好评。此外，请大家注意，我们的员工福利计划的开放报名期即将来临。如果您有任何问题或需要帮助，请联系我们的HR代表迈克尔·约翰逊（电话：418-492-3850，邮箱：michael.johnson@example.com）。',\n",
       " 'Chinese_Summary': '新加入的团队成员在各自领域表现出色，尤其值得一提的是简·史密斯（SSN: 049-45-5928）在客户服务上的优秀表现，她获得了客户的广泛好评。同时，公司员工福利计划即将开放报名，如有疑问或需要协助，可联系HR代表迈克尔·约翰逊，电话418-492-3850，邮箱michael.johnson@example.com。',\n",
       " 'Language': '这段文字的语言是英语。',\n",
       " 'Reply': \"The new team members have demonstrated outstanding performance in their respective fields, with a special mention of Jane Smith (SSN: 049-45-5928) who has received excellent feedback for her customer service skills. Furthermore, the company's employee benefits program is about to open for enrollment. Should you have any inquiries or require assistance, please feel free to contact HR representative Michael Johnson at电话418-492-3850 or via email at michael.johnson@example.com.\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 支持多重链顺序执行\n",
    "# 导入模块\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# 创建模型应用\n",
    "chat_model = ChatTongyi(\n",
    "    model_name=\"qwen-vl-max\",\n",
    "    temperature=0,\n",
    "    dashscope_api_key=api_key\n",
    ")\n",
    "\n",
    "# chain 1 任务: 翻译成中文\n",
    "first_prompt = ChatPromptTemplate.from_template(\"把下面的内容翻译成中文:\\n\\n{content}\")\n",
    "chain_one = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = first_prompt,\n",
    "    verbose =True,\n",
    "    output_key = \"Chinese_Rview\"\n",
    ")\n",
    "\n",
    "# chain 2 任务: 对翻译后的中文进行总结摘要 input_key是上一个chain的output_key\n",
    "second_prompt = ChatPromptTemplate.from_template(\"把下面的内容总结成摘要:\\n\\n{Chinese_Rview}\")\n",
    "chain_two = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = second_prompt,\n",
    "    verbose =True,\n",
    "    output_key = \"Chinese_Summary\"\n",
    ")\n",
    "\n",
    "# chain 3 任务:智能识别语言 input_key是上一个chain的output_key\n",
    "third_prompt = ChatPromptTemplate.from_template(\"请智能识别出这段文字的语言:\\n\\n{Chinese_Summary}\")\n",
    "chain_three = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = third_prompt,\n",
    "    verbose =True,\n",
    "    output_key = \"Language\"\n",
    ")\n",
    "\n",
    "# chain 4 任务:针对摘要使用的特定语言进行评论, input_key是上一个chain的output_key\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\"请使用指定的语言对以下内容进行回复:\\n\\n内容:{Chinese_Summary}\\n\\n语言:{Language}\")\n",
    "chain_four = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=fourth_prompt,\n",
    "    verbose=True,\n",
    "    output_key=\"Reply\",\n",
    ")\n",
    "\n",
    "#overall 任务：翻译成中文->对翻译后的中文进行总结摘要->智能识别语言->针对摘要使用指定语言进行评论\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    verbose=True,\n",
    "    input_variables=[\"content\"],\n",
    "    output_variables=[\"Chinese_Rview\", \"Chinese_Summary\", \"Language\", \"Reply\"],\n",
    ")\n",
    "\n",
    "#读取文件\n",
    "content = \"Recently, we welcomed several new team members who have made significant contributions to their respective departments. I would like to recognize Jane Smith (SSN: 049-45-5928) for her outstanding performance in customer service. Jane has consistently received positive feedback from our clients. Furthermore, please remember that the open enrollment period for our employee benefits program is fast approaching. Should you have any questions or require assistance, please contact our HR representative, Michael Johnson (phone: 418-492-3850, email: michael.johnson@example.com).\"\n",
    "overall_chain(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "路由链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "{destinations}\n",
      "\n",
      "<< INPUT >>\n",
      "{{input}}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': '2+2等于多少?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2+2等于4。这是一个基本的算术加法。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里演示我先定义两个不同方向的链\n",
    "# 首先先导入模块\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "# 比如我定义一个有关物理的链\n",
    "physics_template = \"\"\"\n",
    "    你是一个非常聪明的物理学学家\\n\n",
    "    你擅长以比较直观的语言来回答所有向你提问的的物理问题.\\n\n",
    "    当你不知道问题的答案的时候,你会直接回答你不知道.\\n\n",
    "    下面是我提出的一个问题,请帮我解决:\n",
    "    {input}\n",
    "\"\"\"\n",
    "physics_prompt = PromptTemplate.from_template(physics_template)\n",
    "\n",
    "# 我们再定义一个数学链\n",
    "math_template = \"\"\"\n",
    "    你是一个可以解答所有问题的数学家.\\n\n",
    "    你非常擅长回答数学问题,基本没有问题能难倒你.\\n\n",
    "    你很优秀,是因为你擅长把困难的数学问题分解成组成的部分,回答这些部分,然后再将它们组合起来.\\n\n",
    "    下面是一个问题:\n",
    "    {input}\n",
    "\"\"\"\n",
    "math_prompt = PromptTemplate.from_template(math_template)\n",
    "\n",
    "# 再导入必要的包\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\" : \"physics\",\n",
    "        \"description\" : \"擅长回答物理问题\",\n",
    "        \"prompt_template\" : physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"math\",\n",
    "        \"description\" : \"擅长回答数学问题\",\n",
    "        \"prompt_template\" : math_template\n",
    "    }\n",
    "]\n",
    "\n",
    "llm = Tongyi(\n",
    "    temperature=0,\n",
    "    model= \"Qwen-max\",\n",
    "    dashscope_api_key = api_key\n",
    ")\n",
    "\n",
    "destination_chain = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(\n",
    "        llm = llm,\n",
    "        prompt = prompt\n",
    "    )\n",
    "    destination_chain[name] = chain\n",
    "    default_chain = ConversationChain(\n",
    "        llm = llm,\n",
    "        output_key = \"text\"\n",
    "    )\n",
    "\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "\n",
    "destinations = [f\"{p['name']}:{p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "print(MULTI_PROMPT_ROUTER_TEMPLATE)\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template= router_template,\n",
    "    input_variables = [\"input\"],\n",
    "    output_parser = RouterOutputParser()\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(\n",
    "    llm,\n",
    "    router_prompt\n",
    ")\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain= router_chain,\n",
    "    destination_chains= destination_chain,\n",
    "    default_chain= default_chain,\n",
    "    verbose= True\n",
    ")\n",
    "\n",
    "# chain.run(\"什么是牛顿第一定律?\")\n",
    "# chain.run(\"物理中,鲁智深倒拔垂杨柳\")\n",
    "# chain.run(\"1+4i=0\")\n",
    "# chain.run(\"两个黄鹂鸣翠柳，下一句?\")\n",
    "chain.run(\"2+2等于几?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化 方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m[Generated with ChatGPT]\n",
      "\n",
      "Confidential Document - For Internal Use Only\n",
      "\n",
      "Date: July 1, 2023\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m该文档是内部机密文件，仅供内部使用。日期为2023年7月1日。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'该文档是内部机密文件，仅供内部使用。日期为2023年7月1日。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先导入一个模块\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "from langchain.llms import Tongyi\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    对以下文档的文字进行总结:\n",
    "    {output_text}\n",
    "    总结:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key = api_key,\n",
    "    model = \"Qwen-max\",\n",
    "    temperature =0\n",
    ")\n",
    "\n",
    "with open(\"./letter.txt\", encoding= 'utf-8') as f:\n",
    "    letters = f.read()\n",
    "\n",
    "# 再导入我们必须的模块\n",
    "from langchain.chains import(\n",
    "    LLMChain,\n",
    "    SimpleSequentialChain,\n",
    "    TransformChain\n",
    ")\n",
    "\n",
    "# 定义一个函数\n",
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    shortened_text = \"\\n\\n\".join(text.split(\"\\n\\n\")[:3])\n",
    "    return {\"output_text\": shortened_text}\n",
    "\n",
    "# 文档转换链\n",
    "transform_chain = TransformChain(\n",
    "    input_variables = [\"text\"],\n",
    "    output_variables = [\"output_text\"],\n",
    "    # 提前预转换\n",
    "    transform = transform_func\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    对下面的文字进行总结:\n",
    "    {output_text}\n",
    "    总结:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"output_text\"],\n",
    "    template=\"\"\"\n",
    "    对以下文档的文字进行总结:\n",
    "    {output_text}\n",
    "    总结:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm = Tongyi(),\n",
    "    prompt = prompt\n",
    ")\n",
    "\n",
    "# 接下来用顺序链链接起来\n",
    "squential_chain = SimpleSequentialChain(\n",
    "    chains = [transform_chain, llm_chain],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# 激活\n",
    "squential_chain.run(letters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用文件加载专用chain\n",
    "因为要使用到科学计算,所以要先安装一个包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Loading from the deprecated github-based Hub is no longer supported. Please use the new LangChain Hub at https://smith.langchain.com/hub instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 首先导入模块\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_chain\n\u001b[1;32m----> 4\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlc://chains/llm-math/chain.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(chain\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2+6等于几?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\loading.py:667\u001b[0m, in \u001b[0;36mload_chain\u001b[1;34m(path, **kwargs)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Unified method for loading a chain from LangChainHub or local fs.\"\"\"\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlc://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading from the deprecated github-based Hub is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    669\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the new LangChain Hub at https://smith.langchain.com/hub \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    670\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m     )\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _load_chain_from_file(path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Loading from the deprecated github-based Hub is no longer supported. Please use the new LangChain Hub at https://smith.langchain.com/hub instead."
     ]
    }
   ],
   "source": [
    "# 首先导入模块\n",
    "from langchain.chains import load_chain\n",
    "\n",
    "chain = load_chain(\"lc://chains/llm-math/chain.json\")\n",
    "\n",
    "print(chain.run(\"2+6等于几?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK,我们可以看到这个方法已经被废弃了,这里我给出新方法\n",
    "首先我们还是需要安装几个包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我出现一个RuntimeError\n",
    "等我后续更新一下解决方法\n",
    "> 后续:load_chain在新版的langchain中已经被遗弃，主要出于商业和安全的考虑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那下面讲的就是关于自定义链方面的介绍\n",
    "自定义的好处在于,当langchain自带的内置链不满足我们的需要的时候,就可以通过自定义的链,来实现我们的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'python', 'text': '**Python**\\n\\n**Python** 是一种高级编程语言，以其简洁、清晰的语法和代码可读性而闻名。由荷兰计算机科学家Guido van Rossum在1989年圣诞节期间创造，首个公开发行版在1991年发布。Python的设计哲学强调代码的易读性和简洁的语法，尤其是使用空格缩进划分代码块，而非使用花括号或关键字。\\n\\n**特点**\\n\\n1. **易学性**：Python有相对较少的关键字，结构简单，和一个明确的语法，使得学习Python比其他语言更快。\\n\\n2. **可读性强**：Python代码定义的更清晰，使得代码更易于理解和调试。\\n\\n3. **可扩展性**：如果需要一段运行速度快的关键代码，或者是想要创建一个Python解释器无法实现的库，可以用C或C++完成那部分程序，然后在Python程序中使用它。\\n\\n4. **丰富的库支持**：Python标准库非常庞大，包含许多内置模块，涵盖了网络、文件处理、操作系统接口等多个领域。此外，还有大量的第三方库可供使用。\\n\\n5. **跨平台**：Python可以在多种软件平台上运行，包括Windows、Linux/Unix、Mac OS X等多种操作系统。\\n\\n**应用领域**\\n\\nPython广泛应用于Web开发（如Django和Flask框架）、科学计算、自动化运维、数据可视化、机器学习、人工智能等领域。\\n\\n**版本**\\n\\nPython有两个主要的版本：Python 2和Python 3。Python 3自2008年发布以来，逐渐成为主流，Python 2于2020年1月1日停止官方支持。\\n\\n**社区与生态系统**\\n\\nPython拥有活跃的开发者社区，他们贡献了大量的开源项目和库，如NumPy、Pandas、Matplotlib等。Python开发者也定期举行全球性的PyCon大会，分享技术知识和经验。\\n\\n总的来说，Python因其易用性和强大的功能，已成为世界上最受欢迎的编程语言之一，被广泛应用于各种行业和场景。'}\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库和模块\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.chat_models import ChatTongyi\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# 自定义链类WikiArticleChain，继承自Chain基类\n",
    "class WikiArticleChain(Chain):\n",
    "    \"\"\"\n",
    "    开发一个wiki文章的生成器\n",
    "    \"\"\"\n",
    "    prompt: BasePromptTemplate\n",
    "    llm: BaseLanguageModel\n",
    "    out_key: str = \"text\"\n",
    "\n",
    "    # @property注解，定义一个静态方法来获取输入键\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        返回prompt所需的所有键\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "    \n",
    "    # @property注解，定义一个静态方法来获取输出键\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        将始终返回text键\n",
    "        \"\"\"\n",
    "        return [self.out_key]\n",
    "    \n",
    "    # 定义链调用时的主要逻辑\n",
    "    def _call(\n",
    "            self,\n",
    "            inputs: Dict[str, Any],\n",
    "            run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "        ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        运行链\n",
    "        \"\"\"\n",
    "        # 格式化输入的提示\n",
    "        prompt_value = self.prompt.format(**inputs)\n",
    "        # 使用llm生成文本\n",
    "        response = self.llm.generate([prompt_value], callbacks=run_manager.get_child() if run_manager else None)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\"wiki article is written\")\n",
    "        \n",
    "        # 从response中提取生成的文本\n",
    "        generated_text = response.generations[0][0].text if response.generations else \"\"\n",
    "        return {self.out_key: generated_text}\n",
    "    \n",
    "    # 定义链的类型\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        \"\"\"链类型\"\"\"\n",
    "        return \"wiki_article_chain\"\n",
    "\n",
    "# 加载 .env 文件中的 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# 创建WikiArticleChain实例\n",
    "chain = WikiArticleChain(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"写一篇关于{topic}的维基百科形式的文章\",\n",
    "        input_variables=[\"topic\"]\n",
    "    ),\n",
    "    llm=Tongyi(\n",
    "        temperature=0,  # 设置温度参数，决定生成文本的多样性\n",
    "        model=\"Qwen-max\",  # 指定模型\n",
    "        dashscope_api_key=api_key  # 使用加载的API key\n",
    "    )\n",
    ")\n",
    "\n",
    "# 运行链，生成关于\"python\"的文章\n",
    "result = chain({\"topic\": \"python\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四种文档处理链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "蒂法·洛克哈特是《最终幻想VII》及相关作品中的虚拟角色，由野村哲也创造和设计，是克劳德的青梅竹马和酒吧“第七天堂”的招牌店员，擅长格斗。她在游戏和媒体中受到了广泛的好评，被认为是电子游戏中强大、独立和有吸引力的女性角色之一。她的性格和外表受到了赞誉，并被评为游戏界最好的女性角色之一。蒂法还出现在多个衍生作品中，包括电影《最终幻想VII:降临之子》和其他电子游戏，比如《神佑擂台》、《王国之心 II》等。\n"
     ]
    }
   ],
   "source": [
    "# 第一种:StuffChain\n",
    "# 是一个最常见的文档链,将文档直接塞进我们的prompt中,为LLM回答问题提供上下文资料,适合小文档场景\n",
    "\n",
    "# 导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "loader = PyPDFLoader('./loader.pdf')\n",
    "# 查看一下我们读取到的文件\n",
    "# print(loader.load())\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    对以下文字做简洁的总结:\n",
    "    {text}\n",
    "    简洁的总结:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    prompt_template\n",
    ")\n",
    "\n",
    "llm = ChatTongyi(\n",
    "    model_name=\"qwen-vl-max\",\n",
    "    temperature=0,\n",
    "    dashscope_api_key=api_key\n",
    ")\n",
    "llm_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt\n",
    ")\n",
    "\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain = llm_chain,\n",
    "    document_variable_name=\"text\"\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(stuff_chain.run(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的第一个文档处理链,其实langchain已经封装好了一个预制链,我们直接用就行,这个链叫\n",
    "`load_summarize_chain`\n",
    "下面我来实际展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"蒂法介绍蒂法·洛克哈特（⽇语：ティファ・ロックハート，Tifa Rokkuhāto，英语：Tifa Lockhart）为电⼦游戏《最终幻想VII》及《最终幻想VII补完计划》相关作品中的虚构⻆⾊，由野村哲也创作和设计，此后也在多个游戏中客串登场。2014年东京电玩展上，星名美津纪cosplay《最终幻想VII 降临之⼦》中的蒂法·洛克哈特蒂法是克劳德的⻘梅⽵⻢，两⼈同为尼布鲁海姆出身。在⽶德加经营作为反抗组织“雪崩”根据地的酒馆“第七天堂”，并且是⼩有名⽓的招牌店员。擅⻓格⽃，以拳套为武器。本传7年前克劳德离开故乡从军时，曾许下约定“如果有危机时⼀定会保护她”。与爱丽丝相识之后，两⼈成为好友。第⼀个察觉克劳德记忆混乱的⼈，后来协助精神崩溃的克劳德重新找回真正的⾃⼰。本传的⼤战结束后，依⼤家的期待在战后新⽣的⽶德加再次开设第七天堂（原第七天堂因第柒区圆盘崩塌遭压毁），同时也照顾⼀群受到星痕症候群折磨的孩⼦们。蒂法被《纽约时报》称为“⽹络⼀代”的海报⼥郎，与劳拉·克罗夫特相⽐，她是电⼦游戏中坚强、独⽴和有吸引⼒的⼥性⻆⾊的典型代表。媒体普遍称赞其实⼒和外表，并称她为游戏世界中最好的⼥性⻆⾊之⼀。在《最终幻想VII》本传中，蒂法年龄20岁、身⾼167cm、⽣⽇5⽉3⽇、⾎型B型、出⽣地尼布尔海姆。登场《最终幻想VII》蒂法在《最终幻想VII》原版中⾸次亮相，是克劳德的⻘梅⽵⻢、第七天堂酒吧的看板娘、极端环境组织“雪崩”成员，该组织反抗巨型企业“神罗”因其⼤量抽取魔晄⽤作动⼒能源。在注意到克劳德的性格改变后，她说服克劳德加⼊雪崩，以密切关注他，并且跟随他追寻游戏中的对⼿萨菲罗斯。她⽆法阻⽌克劳德被萨菲罗斯操纵，在他的精神崩溃后，她帮助克劳德康复，并且两⼈意识到彼此间的相互感觉，最后与伙伴们⼀同击败了萨菲罗斯。[2]在闪回中可知，⼉时的蒂法⼀直是村中⼩孩的⼈⽓王。在⺟亲过世后，思念⺟亲的蒂法决定沿着⼩路⾛到他们故乡尼布尔海姆附近的⼀座⼭上，认为这样就能⻅到过世的⺟亲，原本跟着蒂法的其他⼩孩都在半路上因害怕⽽放弃，唯独克劳德仍坚定的在后⾯跟随，希望能在危机时保护蒂法。然⽽，他们俩都从⼭上跌落受伤，蒂法昏迷了⼀个星期，她的⽗亲认为克劳德对此负有责任[3]，甚为严令禁⽌克劳德再接近蒂法，但蒂法反⽽从此更在意克劳德，两⼈成为要好的玩伴。为了使⾃⼰变得更强⼤，克劳德最终选择离开尼布尔海姆，加⼊神罗，想要成为神罗的精英战⼠“神罗战⼠”（SOLDIER），但后来透露他主要是为了吸引蒂法的注意⼒。离开之前，蒂法与克劳德约定，当蒂法处于困境之中时，克劳德会回来救她。从克劳德离开之后，蒂法便开始留意神罗战⼠的消息，因为神罗战⼠都成为声名远播的知名⼈物，如果克劳德成为神罗战⼠，他的活跃也会⽴刻传回尼布尔海姆。数年后，在萨菲罗斯摧毁了尼布尔海姆之后，克劳德为了救蒂法，被萨菲罗斯刺⾄重伤。蒂法被她的武术教练赞⼲带到安全地带，幸存下来，最终到达⽶德加并遇⻅了“雪崩”的领导⼈巴雷特·华莱⼠。病愈后，蒂法加⼊了“雪崩”，为了给家乡被毁⼀事报仇。⼀天，她在⽕⻋站遇到了从魔晄炉中逃出来、精神⼀⽚混乱的克劳德，蒂法说服了他为巴雷特⼯作，以保证克劳德的安全以及和克劳德保持紧密关系。这是游戏开始的地⽅。在原版《最终幻想VII》中蒂法与爱丽丝关系友好，但会在某些时候争⻛吃醋，例如在神罗总部营救爱丽丝时，蒂法及巴雷特等⼀⾏失⼿被擒，若克劳德选择关⼼爱丽丝的话蒂法的对话中明显带有妒忌。在重制版中虽然删去这段情节，但保留了这种关系。在《最终幻想VII》的初稿中，蒂法是背景⼈物。她在“雪崩”中的作⽤是在幕后⽀持，在执⾏任务后为所有⼈加油⿎劲，并且对克劳德有特别的关⼼。据推测，她的背上有⼀块⼤的疤\n",
      "\n",
      "痕，是由克劳德造成的，并在事件发⽣时因巨⼤冲击⽽部分失忆。[4]原版策划者之⼀的加藤正⼈提出了⼀个旨在暗示蒂法和克劳德发⽣性关系的场景，但被北濑佳范⽤⼀个变淡的⾊调所取代。野岛⼀成在接受采访时说，没有⼀个开发团队⼈员认为当时的场景会成为⼀个问题。[5]《最终幻想VII补完计划》2005年，蒂法出现在CG电影《最终幻想VII 降临之⼦》中，故事发⽣在原版游戏剧情结束两年后。在其中，她试图给予克劳德情感上的⽀持，敦促克劳德放下他对⾃⼰施加的不必要的罪恶感。此外，她还照顾巴雷特的养⼥玛琳和克劳德在爱丽丝的教堂发⻔⼝救下的孩⼦丹泽尔。在电影中，她与萨菲罗斯的其中⼀个思念体罗兹战⽃，后来她帮助与被召唤的⽣物巴哈姆特战⽃。编剧野岛⼀成将她在视频中的⻆⾊描述为“⾮常像任何被男⼈抛弃的⼥⼈”，并表示尽管他们不希望她显得笨拙，但他们也想描绘出从她受到克劳德的情感伤害出发。[6]在视频的初稿中，她原本计划在当时的短⽚中扮演更重要的⻆⾊，该短⽚仅以蒂法，克劳德和⼏个孩⼦为主。蒂法也在前传游戏《最终幻想VII 危机之前》和《最终幻想VII 核⼼危机》以及OVA《最终幻想VII 最终命令》中登场。每次登场时，她的出现都与尼布尔海姆的毁灭有关。[2]官⽅⼩说《通向微笑之路》中有专⻔的《蒂法篇》，讲述了原版游戏和降临之⼦两段之间的故事。从蒂法的⻆度出发，详细讲述了她如何在Edge City创建⼀个新的第七天堂酒吧，并试图坚持⾃⼰和克劳德的正常家庭观念，尽管克劳德逐渐开始逃避与他⼈接触。蒂法还短暂出现在游戏《最终幻想VII 地狱⽝的挽歌》中，该游戏的剧情在降临之⼦故事⼀年后发⽣，她在游戏中帮助主⻆⽂森特·瓦伦丁捍卫星球，抵抗怪物欧⽶茄和“兵器”。她后来出现在游戏的结尾中，讨论着⽂森特的失踪。[2]其他登场在《最终幻想VII补完计划》之外，蒂法还出现在格⽃游戏《神佑擂台》中，作为可解锁的⻆⾊和可选的Boss。[7]她后来出现在电⼦棋盘游戏《富豪街》中。在《王国之⼼II》中，她穿着⾃⼰在降临之⼦中的服装，寻找克劳德，然后与该系列的怪物“⽆⼼”战⽃。她原本计划出现在原版《王国之⼼》的最终合辑中，但由于时间限制，⼯作⼈员选择改⽤萨菲罗斯。[8]蒂法也是格⽃游戏《最终幻想 纷争012》中的玩家⻆⾊之⼀，该游戏的⻆⾊来⾃各种《最终幻想》游戏。[9]她身着《最终幻想VII》中的服装，但该玩家还可以使⽤她在《降临之⼦》中的服装和在尼布尔海姆出场时展示的第三种服装。[10][11]游戏的第⼀版展示内容是在天野喜孝的美术原型基础上展现的另⼀种形式。[12]在《⼩⼩⼤星球2》中，蒂法是可下载的⻆⾊模型。[13]韩国歌⼿Ivy在2007年的歌曲“ਬ഑੄ਬ഑੄”（《诱惑奏鸣曲》）MV中描绘了⻆⾊。因为重现了《降临之⼦》中的战⽃场⾯，在史克威尔艾尼克斯提出版权诉讼后，该视频被禁⽌在韩国电视上播出。[14] 2015年，蒂法作为可玩⻆⾊被添加到iOS游戏《最终幻想 记录者》中。[15]创作与发展蒂法在《最终幻想VII 核⼼危机》中的形象。蒂法由野村哲也设计，最初版本的《最终幻想VII》中没有蒂法，因为最初该游戏只有三个可玩⻆⾊。主⻆克劳德、爱丽丝和巴雷特。但是，在给游戏总监北濑佳范打电话时，有⼈建议游戏中的某个主⻆应该死掉，在对究竟是巴雷特还是爱丽丝需要死掉进⾏⼤量讨论之后，制⽚⼈选择了爱丽丝。[3]野村哲也后来开玩笑说这是他的主意，以便使他能够将蒂法引⼊游戏中。[16]⽆论如何，北濑佳范喜欢有两个的⼥主⼈公，并且在他们之间摇摆不定，将其称之为是\n",
      "\n",
      "《最终幻想》系列中的新事物。[5]野村哲也将蒂法的⻆⾊在《降临之⼦》中的表现分为多个⽅⾯，称她“像⺟亲，爱⼈，战⽃中的亲密盟友”，“不仅在情感上，⽽且在身体上也⾮常坚强”。[6]蒂法被设计为该系列以前的游戏中出现的“Monk”级别的⻆⾊。她有⼀头乌⿊的⻓发，像⼀头海豚的尾巴，穿着简单单调的服装，包括⽩⾊的上⾐和⿊⾊的超短裙。她还穿着红⾊的靴⼦和⼿套，⿊⾊的袖⼦从⼿腕延伸到肘部。她的裙⼦被⼀对狭窄的⿊⾊吊带裤撑起，⼤⾯积的⾦属护罩覆盖了她的左肘。她身⾼约167厘⽶[17]，三围为92-60-88厘⽶。[4]最初，野村哲也很难决定选择搭配迷你裙还是搭配⻓裤。为了征求意⻅，他在史克威尔的办公室传递了草图，⼤多数⼯作⼈员都认同超短裙设计。[16]这与爱丽丝的标志形成鲜明对⽐，爱丽丝的⻓裙是她的标志。服装在游戏中被解释为赋予了她⾏动⾃由的能⼒，这是由于她的近距离战⽃的亲和⼒，⽽裙⼦据称“相当短，具有相当⼤的曝光度”[4]。开发者还指出，由于她的身材，她穿着其他便⾐的外观也令⼈愉悦。[4]在制作《最终幻想VII 降临之⼦》时，联合导演野末武志难以为蒂法的身材构建出⼀个“平衡，但⼜展现了她的⼥性特质”的框架。在这⼀点上，她的服装也进⾏了重新设计，着重于表达这些品质，同时仍然令⼈赏⼼悦⽬。⽩⾊背⼼覆盖⿊⾊背⼼，⽩⾊缎带背⼼包裹着她的左⼆头肌，粉红丝带包裹着她的脚。⿊⾊系扣裙⼦遮盖了她的⼤腿，并且在下⾯穿了短裤，⽤⼀块类似于⼤⾐的布从裙⼦的腰带后部延伸到脚踝。她不再使⽤吊带撑起裙⼦，只在电影的战⽃场景中戴着⼿套。她的发型被更改为在她的背部中部结束，从原来的设计中删除了海豚的尾巴。[6]造成这种变化的原因是，难以为她的原始头发设置动画效果，以及由于其⿊⾊和亮光⽽引起的问题。另外，瞳孔的颜⾊亦由红⾊改为深褐⾊。蒂法在《最终幻想VII 重制版》中的形象。开发《最终幻想VII 重制版》时，史克威尔修改了蒂法的原始外观，以使她的外观更加逼真，因为⼯作⼈员意识到她的设计不适合战⽃场景。她因此得到了⿊⾊运动内⾐并贴合她的身体，给她⼀种运动的感觉。[18]在这个版本中的蒂法虽然保留着“海豚尾巴”发型，但原版中其末端如海豚尾鳍已变为更⾃然的垂直模样。另外,在第三章中,玩家可选三种服饰中的⼀种让蒂法在以后的篇章穿上。除了“成熟⻛”、“格⽃家⻛”及“异国⻛”外，开发团队亦考虑过“⼥仆⻛”、“警察服”、“舞蹈家⻛”、“陆⾏⻦⻛”等[19]。由于克劳德与蒂法和爱丽丝之间存在的特殊关系，开发团队观察到粉丝认为史克威尔更偏爱这两个⼥主⻆。以往，史克威尔表示爱丽丝可能是游戏的真正主⻆，⽽蒂法对于帮助克劳德这个⻆⾊的发展很重要。最终，史克威尔表示蒂法和爱丽丝都是这次重制版中的⼥主⻆。[20]野村哲也表示，他喜欢⼥演员伊藤步，并希望与她在《降临之⼦》中合作。在爱丽丝的配⾳演员已经决定之后，野村让伊藤步为蒂法配⾳，觉得她的“沙哑的声⾳”将与坂本真绫配⾳的轻声细语的爱丽丝形成很好的对⽐。[6]野村哲也还指出，在完成蒂法的更新设计后，制⽚⼈对她的最终细节进⾏了辩论，但伊藤步饰演⻆⾊后，便选择将配⾳演员的许多特征融合到⻆⾊的最终形象中。[6]英国配⾳演员瑞秋·莱·寇克（Rachael Leigh Cook）在《王国之⼼II》的⼀次采访中说，她喜欢玩蒂法，并形容她“身体和情感上都很强壮，但也⾮常敏感”，并且“⾮常多维”。[21]在给⻆⾊配⾳时，寇克听取了伊藤步的配⾳。在《降临之⼦》制作之后，寇克感谢野村哲也所创作的视频并表示⾮常喜欢。[22]布⾥特·巴伦（Britt Baron）代替她参加了《最终幻想VII 重制版》的配⾳。[23]蒂法幼年时候由Glory Curda配⾳。[24]评价⾃登场以来，蒂法受到评论家和粉丝的极⼤好评。在2000年，GameSpot的读者将她评为电⼦游戏中的第五好的⼥性⻆⾊，该⽹站的编辑指出他们对此表示同意。[25] 2004年，\n",
      "\n",
      "《Play》杂志在《游戏⼥孩》年度期刊的第⼀期中介绍了蒂法，并称她为“近代史上最受崇拜的⼥性。”2007年，蒂法被《电击PlayStation》评为在原版PlayStation平台上有史以来第⼋好的⼈物，也是《最终幻想VII》中排名第三的⻆⾊。[26]同年，Tom's Hardware将她列为视频游戏历史上50个最伟⼤的⼥性⻆⾊之⼀，并称她为“周围⼈物中更丰富，更复杂的⼥性⻆⾊之⼀。”[27]在2008年，UGO将她列为电⼦游戏史上最伟⼤的⼥性⻆⾊之⼀，排在第五位，并表示对她的偏好超过了爱丽丝，并补充说：“ 蒂法的服装是轻描淡写的奇迹-但正是她的天⽣丽质和令⼈难忘的性格使她在榜单上名列前茅。”[28]同年，Chip杂志将她列为“游戏⼥孩”第⼗名。[29]2009年，IGN将蒂法列为游戏中⼗⼤最佳⼥主⻆之⼀，并称她为“毫⽆疑问的最终幻想宇宙中的传奇⼥主⻆。”[30] Fami通在2010年进⾏的⼀项⺠意调查将她评为第19⼤最受⽇本玩家欢迎的视频游戏⻆⾊。[31]在2013年，Complex将她列为电⼦游戏历史上第13⼤⼥主⻆。[32]2001年，随着劳拉·克罗夫特的推出，Beaumont Enterprise列举了蒂法作为视频游戏中强⼤⼥性⻆⾊的榜样。2008年，Joystiq将她列为他们希望在史克威尔艾尼克斯的跨界格⽃游戏《最终幻想 纷争》中看到的《最终幻想》系列中20个⻆⾊中的佼佼者，并将她描述为该系列“最伟⼤的⼥主⻆”之⼀。[33]IGN在2008年将蒂法列⼊有史以来第13部最佳的《最终幻想》⻆⾊之⼀，描述她是史克威尔试图“赋予《最终幻想》⻆⾊真实的性感”的尝试，以及“可以在紧要关头照顾好⾃⼰”的⼈；在后续的读者选择榜单中，蒂法排名第⼀，⼯作⼈员在将其在榜单上的位置归因于她的胸部时重复了先前的评论。[34]在2009年IGN的⼀篇⽂章中，仅关注《最终幻想VII》中的⻆⾊，她排在第四位，并评论说，尽管她的性感提⾼了她的知名度，但“ 蒂法推动了坚强，独⽴的RPG⼥主⻆的传统。”[35]其他杂志也称赞蒂法的性格特征。Mania Entertainment在2010年“了不起的视频游戏⼥性”名单中，蒂法排名第⼗，并指出，虽然《最终幻想》系列的后续游戏引⼊了其他令⼈难忘的⼥性⻆⾊，但“蒂法是我们的第⼀个《最终幻想》⼥孩，并在我们⼼中有特殊的地位”。[36] 2013年，《Complex》的Gus Turner将蒂法列为有史以来⼗⼆⼤最终幻想⻆⾊之⼀，并指出“除了劳拉和萨姆斯·艾仁之外，蒂法也代表了游戏中最独⽴，最有能⼒的⼥性之⼀。” [37]许多评论都认为蒂法⾮常性感。 1998年，《纽约时报》将她列为“⽹络⼀代”的炙⼿可热的海报⼥郎。[38]同年，《电⼦游戏⽉刊》授予她1997年的“最热⻔游戏宝⻉”，称她“⽐例匀称”，并称赞她是劳拉的另⼀可⾏选择。 UGO在2008年“电⼦游戏热⻔榜”中将她排在第24位，并补充说他们⽆法“克服之后的每个游戏版本中她的表现都更好”。[39]同年，GameDaily在“最热游戏辣妹”名单中将她排在第31位，分享了UGO对她的偏爱，并赞扬了她的外表和战⽃能⼒。[40]MSN也有类似的看法，当他们将“这个充满爱⼼，关怀，超级性感的⼥孩”列⼊“游戏界最热⻔的辣妹”名单中时，她名列第六，并指出她在该系列中的存在“有些微妙，给与了她更像是⼀种情感上的暗示。”⽽且没有她，这个系列就不会那么特别。[41]Manolith在2009年“最热⻔”⼥性视频游戏主⻆名单中将她排名第⼆。[42] 2010年，VideoGamer.com将她列为“⼗⼤电⼦游戏⼥神”，[43]⽽AfterEllen的塞拉·沃恩（Sarah Warn）将她评为“第九⼤最热”⼥性电⼦游戏⻆⾊。[44]2011年，Complex将她列为“游戏中最漂亮的⼥性⻆⾊”第16名，[45]⽽UGO仅因她在《神佑擂台》中的出现⽽将她列为“格⽃游戏中最优秀的⼥性”中的第13位。[46]同年，GameFront将她的胸部排在“电⼦游戏史上最伟⼤的胸部”名单的第九位，称她为“存在的危机版劳拉；” [47]她也被Village Voice Media列⼊“不可思议的胸部名单”，但评论说她“不仅仅具有性感。”[48]2012年，Complex将她列为整体上“第⼆热⻔”视频游戏⻆⾊，[49]MSN将她列为“电⼦游戏史上最热⻔的20位⼥性”之⼀，并补充说“她是历史上最著名的游戏⼥孩之⼀，并且具有永恒的吸引⼒。” [50]2013年，《每⽇纪事报》的Scott Marley将她排在“最有魅⼒的⼥性电⼦游戏⻆⾊”第⼆位，[51]⽽CheatCodes.com则宣布她是有史以来“最热⻔的⼥性\n",
      "\n",
      "电⼦游戏⻆⾊”。[52]同样，《La NuevaEspaña》在2014年将“性感，独⽴和坚强”的蒂法列为男⼥最性感的⼗⼤电⼦游戏⻆⾊中，[53]⽽《ThanhNiên》在2015年将她评为最性感的⼥性电⼦游戏⻆⾊。[54]姓名由来蒂法（Tifa）的英⽂名字来源于⼀个古希伯来语单词“Tiferet” ，⽽姓⽒洛克哈特（Lockhart）则是英语单词Lock和Hard的结合；Tiferet是⽣命之树的⼀个成分（⽣命之树⼤概可以分为三⽀柱、⼗个原质、四个世界、⼆⼗⼆路径等基本结构），象征着爱情、美丽和⾃我牺牲，从游戏⾥可以看出，这⼏点都符合Tifa的情况。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'蒂法·洛克哈特是一位来自电子游戏《最终幻想VII》及其相关作品的虚构角色，由野村哲也创造和设计。她是克劳德的青梅竹马，米德加一家酒吧 \"第七天堂\" 的招牌女服务员，也是抵抗组织 \"雪崩 \"的一员，对抗提取魔力作为动力能源的大公司 \"神罗\"。在原版游戏中，她帮助陷入困境的克劳德恢复自我，并在《最终幻想VII补完计划》中继续支持克劳德，照顾马琳和丹泽尔。蒂法被《纽约时报》称为网络一代的海报女孩，被认为是电子游戏中的强大、独立和迷人的女性角色之一。多年来，她在不同游戏中担任各种角色，如《神佑擂台》中的可解锁角色或《王国之心II》中的boss。媒体广泛赞美她的实力和外表，将她誉为游戏界最好的女性角色之一。'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用预封装好的load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "loader = PyPDFLoader('./loader.pdf')\n",
    "docs = loader.load()\n",
    "llm = ChatTongyi(\n",
    "    model_name='qwen-vl-max',\n",
    "    temperature = 0,\n",
    "    prompt = prompt\n",
    ")\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm = llm,\n",
    "    chain_type= \"stuff\",\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refine document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宏图科技发展有限公司成立于2011年，主要从事计算机软件研发与销售。截至2023年第一季度，其财务状况堪忧，资产总额、营业收入均下降，负债总额上升，净利润亏损，现金流紧张且存货积压严重。公司面临产品同质化、新产品开发滞后以及市场营销策略不当等问题，导致市场份额大幅缩水。\n",
      "\n",
      "然而，公司也面临着更广泛的挑战。行业内新兴企业崛起迅速，原有客户流失严重。这些都表明公司在市场上竞争力不足，需要进行深刻的改革才能实现可持续发展。\n",
      "\n",
      "此外，公司的债务问题也非常严重。它拥有超过人民币1亿元的银行贷款，并且其中部分贷款已逾期未还。同时，公司还欠供应商300万元，员工工资和社会保障总计200万元，以及其他应付款项（如税费和租赁费用）约100万元。这种高负债率增加了公司经营风险，使其容易受到经济环境变化的影响。\n",
      "\n",
      "要考虑购买宏图科技相关资产前，建议进行深入的尽职调查，并制定详细的风险控制和资产处置方案。同时，在估值时应充分考虑到公司所面临的各种潜在风险和清收难度。\n",
      "\n",
      "综上所述，宏图科技发展有限公司目前正经历严重的财务困境，不仅存在经营风险、债务风险、市场风险和法律风险等多种潜在风险，而且其业务模式和技术竞争力也需要改进。因此，如果无外部资金注入或业务转型成功，该公司短期内难以摆脱危机。对于有意收购不良资产的投资者来说，必须谨慎评估这家公司的前景，并采取适当的措施以最大限度地减少投资风险。报告撰写日期：2023年4月20日\n"
     ]
    }
   ],
   "source": [
    "# 首先依旧先导入我们的模块\n",
    "# 导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# 加载文档\n",
    "loader = PyPDFLoader('./example/fake.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "# 对文档进行切分\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 创建一个提问问题的模版\n",
    "prompt_template = \"\"\"\n",
    "    对以下文字做简要的总结:\n",
    "    {text}\n",
    "    简洁的总结:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    prompt_template\n",
    ")\n",
    "\n",
    "# 发起提问的模版(核心)\n",
    "refine_template = (\n",
    "    \"你的任务是产生最终的摘要\\n\"\n",
    "    \"我们已经提供了一个到某个特定点的现有回答{existing_answer}\\n\"\n",
    "    \"我们有机会通过下面的一些更多的上下文来完善现有的回答(仅在需要的时候使用).\\n\"\n",
    "    \"--------------------------------------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"--------------------------------------------\\n\"\n",
    "    \"根据新的上下文,用中文完善原始回答.\\n\"\n",
    "    \"如果上下文没有用处,请返回原始回答.\\n\"\n",
    ")\n",
    "\n",
    "refine_prompt = PromptTemplate.from_template(\n",
    "    refine_template\n",
    ")\n",
    "\n",
    "# 构建一个llm\n",
    "llm = ChatTongyi(\n",
    "    model_name= 'qwen-vl-max',\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm = llm,\n",
    "    # 设置类型\n",
    "    chain_type= 'refine',\n",
    "    # 设置问题模版\n",
    "    question_prompt = prompt,\n",
    "    # 设置回答模版\n",
    "    refine_prompt = refine_prompt,\n",
    "    # 是否返回中间步骤\n",
    "    return_intermediate_steps = True,\n",
    "    # 设置输入\n",
    "    input_key = 'documents',\n",
    "    output_key = 'output_text'\n",
    ")\n",
    "\n",
    "# 通过上面的设置后,我们来看下成功\n",
    "# 唤醒一下先(设置一个仅返回输出结果)\n",
    "result = chain({'documents': split_docs}, return_only_outputs=True)\n",
    "\n",
    "# 首先,我们看下就是迭代过程中的中间每一代\n",
    "# print(\"\\n\\n\".join(result['intermediate_steps'][:3]))\n",
    "print(result['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宏图科技发展有限公司成立于2011年，主要从事计算机软件研发与销售。截至2023年第一季度，其财务状况堪忧，资产总额、营业收入均下降，负债总额上升，净利润亏损，现金流紧张且存货积压严重。公司面临产品同质化、新产品开发滞后以及市场营销策略不当等问题，导致市场份额大幅缩水。\n",
      "\n",
      "我们已经提供了一个到某个特定点的现有回答 宏图科技发展有限公司成立于2011年，主要从事计算机软件研发与销售。截至2023年第一季度，其财务状况堪忧，资产总额、营业收入均下降，负债总额上升，净利润亏损，现金流紧张且存货积压严重。公司面临产品同质化、新产品开发滞后以及市场营销策略不当等问题，导致市场份额大幅缩水。\n",
      "\n",
      "然而，公司也面临着更广泛的挑战。行业内新兴企业崛起迅速，原有客户流失严重。这些都表明公司在市场上竞争力不足，需要进行深刻的改革才能实现可持续发展。\n",
      "\n",
      "此外，公司的债务问题也非常严重。它拥有超过人民币1亿元的银行贷款，并且其中部分贷款已逾期未还。同时，公司还欠供应商300万元，员工工资和社会保障总计200万元，以及其他应付款项（如税费和租赁费用）约100万元。这种高负债率增加了公司经营风险，使其容易受到经济环境变化的影响。\n",
      "\n",
      "另一方面，公司的资产清单显示其固定资产原值为人民币800万元，累计折旧约为400万元；无形资产（包括软件著作权和专利权等）原值为人民币300万元；存货资产价值约为人民币400万元；而应收账款主要是对外销售软件的收入，共计人民币600万元。尽管这些数字看似可观，但考虑到公司的债务水平，这些资产的价值可能会大大降低。\n",
      "\n",
      "综上所述，宏图科技发展有限公司目前正经历严重的财务困境，不仅存在经营风险、债务风险、市场风险和法律风险等多种潜在风险，而且其业务模式和技术竞争力也需要改进。因此，如果无外部资金注入或业务转型成功，该公司短期内难以摆脱危机。对于有意收购不良资产的投资者来说，必须谨慎评估这家公司的前景，并采取适当的措施以最大限度地减少投资风险。\n",
      "\n",
      "宏图科技发展有限公司成立于2011年，主要从事计算机软件研发与销售。截至2023年第一季度，其财务状况堪忧，资产总额、营业收入均下降，负债总额上升，净利润亏损，现金流紧张且存货积压严重。公司面临产品同质化、新产品开发滞后以及市场营销策略不当等问题，导致市场份额大幅缩水。\n",
      "\n",
      "然而，公司也面临着更广泛的挑战。行业内新兴企业崛起迅速，原有客户流失严重。这些都表明公司在市场上竞争力不足，需要进行深刻的改革才能实现可持续发展。\n",
      "\n",
      "此外，公司的债务问题也非常严重。它拥有超过人民币1亿元的银行贷款，并且其中部分贷款已逾期未还。同时，公司还欠供应商300万元，员工工资和社会保障总计200万元，以及其他应付款项（如税费和租赁费用）约100万元。这种高负债率增加了公司经营风险，使其容易受到经济环境变化的影响。\n",
      "\n",
      "要考虑购买宏图科技相关资产前，建议进行深入的尽职调查，并制定详细的风险控制和资产处置方案。同时，在估值时应充分考虑到公司所面临的各种潜在风险和清收难度。\n",
      "\n",
      "综上所述，宏图科技发展有限公司目前正经历严重的财务困境，不仅存在经营风险、债务风险、市场风险和法律风险等多种潜在风险，而且其业务模式和技术竞争力也需要改进。因此，如果无外部资金注入或业务转型成功，该公司短期内难以摆脱危机。对于有意收购不良资产的投资者来说，必须谨慎评估这家公司的前景，并采取适当的措施以最大限度地减少投资风险。报告撰写日期：2023年4月20日\n"
     ]
    }
   ],
   "source": [
    "# 首先依旧先导入我们的模块\n",
    "# 导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# 加载文档\n",
    "loader = PyPDFLoader('./example/fake.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "# 对文档进行切分\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 创建一个提问问题的模版\n",
    "prompt_template = \"\"\"\n",
    "    对以下文字做简要的总结:\n",
    "    {text}\n",
    "    简洁的总结:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    prompt_template\n",
    ")\n",
    "\n",
    "# 发起提问的模版\n",
    "refine_template = (\n",
    "    \"你的任务是产生最终的摘要\\n\"\n",
    "    \"我们已经提供了一个到某个特定点的现有回答{existing_answer}\\n\"\n",
    "    \"我们有机会通过下面的一些更多的上下文来完善现有的回答(仅在需要的时候使用).\\n\"\n",
    "    \"--------------------------------------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"--------------------------------------------\\n\"\n",
    "    \"根据新的上下文,用中文完善原始回答.\\n\"\n",
    "    \"如果上下文没有用处,请返回原始回答.\\n\"\n",
    ")\n",
    "\n",
    "refine_prompt = PromptTemplate.from_template(\n",
    "    refine_template\n",
    ")\n",
    "\n",
    "# 构建一个llm\n",
    "llm = ChatTongyi(\n",
    "    model_name= 'qwen-vl-max',\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm = llm,\n",
    "    # 设置类型\n",
    "    chain_type= 'refine',\n",
    "    # 设置问题模版\n",
    "    question_prompt = prompt,\n",
    "    # 设置回答模版\n",
    "    refine_prompt = refine_prompt,\n",
    "    # 是否返回中间步骤\n",
    "    return_intermediate_steps = True,\n",
    "    # 设置输入\n",
    "    input_key = 'documents',\n",
    "    output_key = 'output_text'\n",
    ")\n",
    "\n",
    "# 通过上面的设置后,我们来看下成功\n",
    "# 唤醒一下先(设置一个仅返回输出结果)\n",
    "result = chain({'documents': split_docs}, return_only_outputs=True)\n",
    "\n",
    "# 首先,我们看下就是迭代过程中的中间每一代\n",
    "print(\"\\n\\n\".join(result['intermediate_steps'][:3]))\n",
    "# print(result['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来,演示另一个chain文档处理\n",
    "Map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1179 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宏图科技是一家成立于2011年的计算机软件研发和销售公司，注册资本为5000万元，员工约200人。截至2023年第一季度，该公司的财务状况不佳，资产总额下降30%，负债总额上升50%，资不抵债；营业收入下降60%，净利润亏损800万元。该公司还存在存货积压、应收账款高企等问题。造成这些问题的原因有市场竞争激烈、技术更新快以及管理决策失误等。此外，该公司还面临严重的债务问题，并有可能面临破产清算的风险。因此，如果要购买或投资宏图科技的相关资产，应进行全面的尽职调查并制定详细的风控和资产处置计划，同时在估值过程中考虑所有可能的风险和清收困难。\n"
     ]
    }
   ],
   "source": [
    "# 导入模块\n",
    "from langchain.chains import MapReduceDocumentsChain\n",
    "from langchain.chains import ReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 加载env file\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# load pdf\n",
    "loader = PyPDFLoader(\"./example/fake.pdf\")\n",
    "docs = loader.load()\n",
    "# print(docs)\n",
    "\n",
    "# 对文档进行切割\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "# print(split_docs)\n",
    "\n",
    "# 设置我们的mapChain\n",
    "map_template = \"\"\"\n",
    "    对以下文字做简洁的总结:\n",
    "    '{content}'\n",
    "    简洁的总结:\n",
    "\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "llm = ChatTongyi(\n",
    "    model_name = \"qwen-vl-max\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key = api_key\n",
    ")\n",
    "map_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = map_prompt\n",
    ")\n",
    "\n",
    "# reduceChain\n",
    "reduce_template = \"\"\"\n",
    "    以下是一个摘要的集合:\n",
    "    {doc_summaries}\n",
    "    将上面摘要与所有关键细节进行总结.\n",
    "    总结:\n",
    "\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(\n",
    "    prompt = reduce_prompt,\n",
    "    llm = llm\n",
    ")\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain = reduce_chain,\n",
    "    document_variable_name = \"doc_summaries\"\n",
    ")\n",
    "\n",
    "reduce_final_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain = stuff_chain,\n",
    "    # collapse_documents_chain的作用就是判断token是否会超过我们设置的max值,也就是4000,当超过的时候,切换到下一个stuff_chain\n",
    "    collapse_documents_chain = stuff_chain,\n",
    "    token_max = 4000\n",
    ")\n",
    "\n",
    "# map reduce chain\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain = map_chain,\n",
    "    document_variable_name= \"content\",\n",
    "    reduce_documents_chain= reduce_final_chain,\n",
    ")\n",
    "\n",
    "# 激活我们的chain\n",
    "summary = map_reduce_chain.run(split_docs)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来的是Map re-rank documents chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=RegexParser(regex='(.*?)\\\\nScore: (\\\\d*)', output_keys=['answer', 'score']), template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nIn addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\\n\\nQuestion: [question here]\\nHelpful Answer: [answer here]\\nScore: [score between 0 and 100]\\n\\nHow to determine the score:\\n- Higher is a better answer\\n- Better responds fully to the asked question, with sufficient level of detail\\n- If you do not know the answer based on the context, that should be a score of 0\\n- Don't be overconfident!\\n\\nExample #1\\n\\nContext:\\n---------\\nApples are red\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: red\\nScore: 100\\n\\nExample #2\\n\\nContext:\\n---------\\nit was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\\n---------\\nQuestion: what type was the car?\\nHelpful Answer: a sports car or an suv\\nScore: 60\\n\\nExample #3\\n\\nContext:\\n---------\\nPears are either red or orange\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: This document does not answer the question\\nScore: 0\\n\\nBegin!\\n\\nContext:\\n---------\\n{context}\\n---------\\nQuestion: {question}\\nHelpful Answer:\"), llm=ChatTongyi(client=<class 'dashscope.aigc.multimodal_conversation.MultiModalConversation'>, model_name='qwen-vl-max', dashscope_api_key=SecretStr('**********'))) document_variable_name='context' rank_key='score' answer_key='answer' metadata_keys=['source'] return_intermediate_steps=True\n"
     ]
    }
   ],
   "source": [
    "# 首先导入必要的模块\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 导入env文件\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# 定义一个llm\n",
    "llm = ChatTongyi(\n",
    "    model_name = \"qwen-vl-max\",\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# 加载文档\n",
    "laoder = PyPDFLoader('./example/fake.pdf')\n",
    "docs = laoder.load()\n",
    "\n",
    "# 对文档进行一个切割\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 创建chain\n",
    "chain = load_qa_with_sources_chain(\n",
    "    ChatTongyi(\n",
    "        temperature = 0,\n",
    "        dashscope_api_key = api_key,\n",
    "        model_name = \"qwen-vl-max\"\n",
    "    ),\n",
    "    chain_type = \"map_rerank\",\n",
    "    metadata_keys = ['source'],\n",
    "    return_intermediate_steps = True\n",
    ")\n",
    "\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the following pieces of context to answer the question in chinese at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n\n",
    "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\\n\\n\n",
    "Question: [question here]\\n\n",
    "Helpful Answer: [answer here]\\n\n",
    "Score: [score between 0 and 100]\\n\\n\n",
    "How to determine the score:\\n\n",
    "- Higher is a better answer\\n\n",
    "- Better responds fully to the asked question, with sufficient level of detail\\n\n",
    "- If you do not know the answer based on the context, that should be a score of 0\\n\n",
    "- Don't be overconfident!\\n\\n\n",
    "Example #1\\n\\n\n",
    "Context:\\n\n",
    "---------\\n\n",
    "Apples are red\\n\n",
    "---------\\n\n",
    "Question: what color are apples?\\n\n",
    "Helpful Answer: red\\n\n",
    "Score: 100\\n\\n\n",
    "Example #2\\n\\n\n",
    "Context:\\n\n",
    "---------\\n\n",
    "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\\n\n",
    "---------\\n\n",
    "Question: what type was the car?\\n\n",
    "Helpful Answer: a sports car or an suv\\n\n",
    "Score: 60\\n\\n\n",
    "Example #3\\n\\n\n",
    "Context:\\n---------\\n\n",
    "Pears are either red or orange\\n\n",
    "---------\\n\n",
    "Question: what color are apples?\\n\n",
    "Helpful Answer: This document does not answer the question\\n\n",
    "Score: 0\\n\\n\n",
    "Begin!\\n\\n\n",
    "Context:\\n\n",
    "---------\\n\n",
    "{context}\\n\n",
    "---------\\n\n",
    "Question: {question}\\n\n",
    "Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\llm.py:368: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': './example/fake.pdf',\n",
       " 'intermediate_steps': [{'answer': '这个文档谈论的是宏图科技发展有限公司的基本信息、财务状况以及主营业务及市场状况。',\n",
       "   'score': '100'},\n",
       "  {'answer': '这份文档讨论了宏图科技发展有限公司的财务状况、债务问题以及潜在风险。', 'score': '100'},\n",
       "  {'answer': '这份文件讨论了在购买TikTok科技相关资产之前，需要进行深入的尽职调查并制定详细的风险控制和资产处置方案，在估值时也需要考虑公司的潜在风险和清收难度。',\n",
       "   'score': '80'}],\n",
       " 'output_text': '这个文档谈论的是宏图科技发展有限公司的基本信息、财务状况以及主营业务及市场状况。'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先导入必要的模块\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 导入env文件\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# 定义一个llm\n",
    "llm = ChatTongyi(\n",
    "    model_name = \"qwen-vl-max\",\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# 加载文档\n",
    "laoder = PyPDFLoader('./example/fake.pdf')\n",
    "docs = laoder.load()\n",
    "\n",
    "# 对文档进行一个切割\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 创建chain\n",
    "chain = load_qa_with_sources_chain(\n",
    "    ChatTongyi(\n",
    "        temperature = 0,\n",
    "        dashscope_api_key = api_key,\n",
    "        model_name = \"qwen-vl-max\"\n",
    "    ),\n",
    "    chain_type = \"map_rerank\",\n",
    "    metadata_keys = ['source'],\n",
    "    return_intermediate_steps = True\n",
    ")\n",
    "\n",
    "# print(chain)\n",
    "\n",
    "# 提出问题\n",
    "query = \"What is this document talk about? answer by chinese\"\n",
    "result = chain({\"input_documents\": split_docs, \"question\": query}, return_only_outputs=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\llm.py:368: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='一、公司基本信息 名称：宏图科技发展有限公司 注册地址：江苏省南京市雨花台区软件大道101号 成立日期：2011年5月16日 法定代表人：李强 注册资本：人民币5000万元 员工人数：约200人 联系电话：025-88888888 电子邮箱：info@hongtutech.cn  二、财务状况概述 截至2023年第一季度，宏图科技发展有限公司财务状况堪忧，具体情况如下： 1. 资产总额：人民币1.2亿元，较上年同期下降30%。 2. 负债总额：人民币1.8亿元，较上年同期上升50%，资不抵债。 3. 营业收入：人民币3000万元，较上年同期下降60%。 4. 净利润：亏损人民币800万元，去年同期为盈利人民币200万元。 5. 现金流量：公司现金流量紧张，现金及现金等价物余额为人民币500万元，难以支撑日常运营。 6. 存货： 存货积压严重， 库存商品价值约为人民币400万元， 大部分产品滞销。 7. 应收账款：应收账款高达人民币600万元，回收难度大，坏账准备不足。  三、主营业务及市场状况 宏图科技发展有限公司主要从事计算机软件的研发与销售。近年来，由于市场竞争加剧、技术更新换代速度快和管理层决策失误等原因，公司主营业务收入持续下降。目前，公司面临的主要问题有： 1. 产品同质化严重，缺乏核心竞争力。 2. 新产品开发进度缓慢，未能及时抓住市场需求变化。 3. 市场营销策略不当，导致市场份额大幅缩水。', metadata={'source': './example/fake.pdf', 'page': 0}),\n",
       "  Document(page_content='4. 行业内新兴企业崛起迅速，原有客户流失严重。  四、债权债务情况 宏图科技发展有限公司目前面临的债务问题严峻，具体情况如下： 1. 银行贷款： 公司向多家银行贷款总额达人民币1亿元， 部分贷款已逾期未还。 2. 供应商欠款：因现金流紧张，公司拖欠供应商货款达人民币300万元。 3. 员工工资及社保：由于资金链断裂，公司拖欠员工工资及社保费用共计人民币200万元。 4. 其他应付款项： 包括税费、 租赁费用等其他应付款项累计约人民币100万元。  五、资产清单 宏图科技发展有限公司目前拥有的主要资产包括： 1. 固定资产：公司办公用房和设备原值合计人民币800万元，累计折旧约400万元。 2. 无形资产：包括软件著作权、专利权等无形资产原值合计人民币300万元。 3. 存货资产：存货包括已完成软件产品和半成品，价值约为人民币400万元。 4. 应收账款：主要包括对外销售软件的应收账款合计人民币600万元。  六、潜在风险及预警 1. 经营风险：由于连续亏损，公司可能面临破产清算的风险。 2. 债务风险：负债累累，若短期内无法筹措足够资金偿还债务，可能面临诉讼或资产被查封的风险。 3. 市场风险：行业竞争加剧和市场需求不明朗，可能导致公司未来业绩继续恶化。 4. 法律风险： 因未能按时支付债务和相关费用， 可能面临相关法律诉讼或处罚。  七、结论与建议 综上所述，宏图科技发展有限公司目前处于财务困境之中，若无外部资金注入或业务转型成功，短期内难以扭转局势。对于不良资产收购方来说，在考虑收购宏', metadata={'source': './example/fake.pdf', 'page': 1}),\n",
       "  Document(page_content='图科技的相关资产前，建议进行深入的尽职调查，并制定详细的风险控制和资产处置方案。 同时， 在估值时应充分考虑到公司所面临的各种潜在风险和清收难度。  报告撰写日期：2023年4月20日', metadata={'source': './example/fake.pdf', 'page': 2})],\n",
       " 'question': 'What is this document talk about? answer by chinese',\n",
       " 'source': './example/fake.pdf',\n",
       " 'intermediate_steps': [{'answer': '这个文档谈论的是宏图科技发展有限公司的基本信息、财务状况以及主营业务及市场状况。',\n",
       "   'score': '100'},\n",
       "  {'answer': '这份文档讨论了宏图科技发展有限公司的财务状况、债务问题以及潜在风险。', 'score': '100'},\n",
       "  {'answer': '这份文件讨论了在购买TikTok科技相关资产之前，需要进行深入的尽职调查并制定详细的风险控制和资产处置方案，在估值时也需要考虑公司的潜在风险和清收难度。',\n",
       "   'score': '80'}],\n",
       " 'output_text': '这个文档谈论的是宏图科技发展有限公司的基本信息、财务状况以及主营业务及市场状况。'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先导入必要的模块\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 导入env文件\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# 定义一个llm\n",
    "llm = ChatTongyi(\n",
    "    model_name = \"qwen-vl-max\",\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# 加载文档\n",
    "laoder = PyPDFLoader('./example/fake.pdf')\n",
    "docs = laoder.load()\n",
    "\n",
    "# 对文档进行一个切割\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 创建chain\n",
    "chain = load_qa_with_sources_chain(\n",
    "    ChatTongyi(\n",
    "        temperature = 0,\n",
    "        dashscope_api_key = api_key,\n",
    "        model_name = \"qwen-vl-max\"\n",
    "    ),\n",
    "    chain_type = \"map_rerank\",\n",
    "    metadata_keys = ['source'],\n",
    "    return_intermediate_steps = True\n",
    ")\n",
    "\n",
    "# print(chain)\n",
    "\n",
    "# 提出问题\n",
    "query = \"What is this document talk about? answer by chinese\"\n",
    "result = chain({\"input_documents\": split_docs, \"question\": query})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面,是Memory工具的使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi!i am southaki\\nAI: hi!i am your ai assistant,can you help me?'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"hi!i am southaki\")\n",
    "memory.chat_memory.add_ai_message(\"hi!i am your ai assistant,can you help me?\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi,i am southaki\\nAI: whats up\\nHuman: not too bad\\nAI: glad to hear it'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们也可以实现一个最近对话的窗口,超过窗口条数的对话,将会从内存中释放出去\n",
    "# 首先依旧是导入相应的模块\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 这里我们需要传入一个k值,这个k值表明窗口允许的最大条数是多少,这里我分别给你演示一下区别\n",
    "# memory = ConversationBufferWindowMemory(k=1)\n",
    "# output: {'history': 'Human: not too bad\\nAI: glad to hear it'}\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "memory.save_context({\"input\": \"hi,i am southaki\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not too bad\"}, {\"output\": \"glad to hear it\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来讲,如何构建记忆实体清单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '', 'entities': {'华南理工大学': '', '华南师范大学': '', '华南农业大学': ''}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0,\n",
    "    model = 'Qwen-max'\n",
    ")\n",
    "\n",
    "memory = ConversationEntityMemory(\n",
    "    llm = llm\n",
    ")\n",
    "_input = {\n",
    "    \"input\": \"华南理工大学和华南师范大学华南农业大学是华南地区的重点大学,合成华南三大\"\n",
    "}\n",
    "memory.load_memory_variables(_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 华南理工大学和华南师范大学华南农业大学是华南地区的重点大学,合成华南三大\\nAI: 听起来不错,我也想去这三个大学',\n",
       " 'entities': {'华南理工大学': '华南理工大学是中国华南地区的一所重点大学，与华南师范大学和华南农业大学并称为“华南三大”。',\n",
       "  '华南师范大学': '华南师范大学是华南地区的一所重点大学，与华南理工大学和华南农业大学并称为“华南三大”。',\n",
       "  '华南农业大学': '华南农业大学是华南地区的一所重点大学，与华南理工大学和华南师范大学并称为华南三大。',\n",
       "  '华南三角': ''}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0,\n",
    "    model = 'Qwen-max'\n",
    ")\n",
    "\n",
    "memory = ConversationEntityMemory(\n",
    "    llm = llm\n",
    ")\n",
    "_input = {\n",
    "    \"input\": \"华南理工大学和华南师范大学华南农业大学是华南地区的重点大学,合成华南三大\"\n",
    "}\n",
    "memory.load_memory_variables(_input)\n",
    "memory.save_context(\n",
    "    _input,\n",
    "    {\n",
    "        \"output\": \"听起来不错,我也想去这三个大学\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({\"input\": \"华南三角是谁?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用知识图谱来构建记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KnowledgeTriple(subject='SouthAki', predicate='最喜欢', object_='coding')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先依旧导入模块\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "llm = Tongyi(\n",
    "    temperature = 0,\n",
    "    dashscope_api_key = api_key,\n",
    "    model = 'Qwen-max'\n",
    ")\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm = llm,\n",
    "    return_messages= True\n",
    ")\n",
    "\n",
    "# 构建对话\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"please find SouthAki\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"who is SouthAki\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"SouthAki是一位前端工程师\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"ok, i remmember\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# ok,让我们提问下一个问题\n",
    "# memory.load_memory_variables({\"input\": 'SouthAki是谁'})\n",
    "# 在知识图谱里,我们也是可以提取实体的\n",
    "# memory.get_current_entities(\"SouthAki最喜欢做什么事情\")\n",
    "# 我们也可以获得到这个问题的三元知识组 主题,动作,干什么\n",
    "memory.get_knowledge_triplets(\"SouthAki最喜欢coding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK,下面是长对话在内存中的处理方式:总结摘要以及token计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='please find SouthAki'), AIMessage(content='who is SouthAki'), HumanMessage(content='SouthAki是一位前端工程师'), AIMessage(content='ok, i remmember')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The human requests the AI to find someone named SouthAki, specifying that they are a front-end engineer. The AI acknowledges and says it will remember this information.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用上面的模版\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "llm = Tongyi(\n",
    "    temperature = 0,\n",
    "    dashscope_api_key = api_key,\n",
    "    model = 'Qwen-max'\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm = llm,\n",
    "    return_messages= True\n",
    ")\n",
    "\n",
    "# 构建对话\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"please find SouthAki\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"who is SouthAki\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"SouthAki是一位前端工程师\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"ok, i remmember\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({}) # 看一下总结出来的成果\n",
    "\n",
    "messages = memory.chat_memory.messages\n",
    "print(messages) # 打印一下我们提交给大语言模型的聊天记录\n",
    "\n",
    "memory.predict_new_summary(messages,\"\") # 产生新的摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK,我们下面使用ChatMessageHistory来快速获得对话摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The human, SouthAki, greets the AI, and the AI, Tongyi, introduces itself as a large language model developed by Alibaba, asking how it can assist.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.llms import Tongyi\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key = api_key,\n",
    "    model = \"Qwen-max\",\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# 增加一点历史记录\n",
    "hisiory = ChatMessageHistory()\n",
    "hisiory.add_user_message(\"你好,我是南秋SouthAki!\")\n",
    "hisiory.add_ai_message(\"你好,我是阿里开发的大语言模型Tongyi,请问我有什么可以帮到你的?\")\n",
    "\n",
    "memory = ConversationSummaryMemory.from_messages(\n",
    "    llm = Tongyi(\n",
    "        temperature = 0,\n",
    "        model = \"Qwen-max\",\n",
    "        dashscope_api_key = api_key\n",
    "    ),\n",
    "    chat_memory= hisiory,\n",
    "    return_messages = True\n",
    ")\n",
    "\n",
    "memory.buffer # 总结了上面的对话的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想对上面的生成结构化数据,我们可以这样做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human, SouthAki, greets the AI, and the AI, Tongyi, introduces itself as a large language model developed by Alibaba, asking how it can assist.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.llms import Tongyi\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key = api_key,\n",
    "    model = \"Qwen-max\",\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# 增加一点历史记录\n",
    "hisiory = ChatMessageHistory()\n",
    "hisiory.add_user_message(\"你好,我是南秋SouthAki!\")\n",
    "hisiory.add_ai_message(\"你好,我是阿里开发的大语言模型Tongyi,请问我有什么可以帮到你的?\")\n",
    "\n",
    "# memory = ConversationSummaryMemory.from_messages(\n",
    "#     llm = Tongyi(\n",
    "#         temperature = 0,\n",
    "#         model = \"Qwen-max\",\n",
    "#         dashscope_api_key = api_key\n",
    "#     ),\n",
    "#     chat_memory= hisiory,\n",
    "#     return_messages = True\n",
    "# )\n",
    "\n",
    "# memory.buffer # 总结了上面的对话的内容\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm = Tongyi(\n",
    "        model = \"Qwen-max\",\n",
    "        dashscope_api_key = api_key,\n",
    "        temperature = 0\n",
    "    ),\n",
    "    return_message = True,\n",
    "    buffer = 'The human, SouthAki, greets the AI, and the AI, Tongyi, introduces itself as a large language model developed by Alibaba, asking how it can assist.',\n",
    "    chat_memory= hisiory\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来这个是比较好用的ConversationSummaryBufferMemory\n",
    "这个好处是当对话持续进行的时候且对话内容很多的时候,它会根据token的数量来自动判断是否需要进行摘要\n",
    "当token数量超过阈值的时候,会自动进行摘要\n",
    "在缓冲区中,会保存最近的k条对话\n",
    "比较久的对话会被删除,在删除前会进行摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content=\"The human is searching for 南秋SouthAki, a front-end development engineer who is scheduled to give a lecture today on Huawei's仓颉programming language. The AI has become aware of this and asks if the human needs further information.\")]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先依旧导入我们的模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.llms import Tongyi\n",
    "\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key = api_key,\n",
    "    model = \"Qwen-max\",\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit= 10,\n",
    "    return_messages = True\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"你好,帮我找下南秋SouthAki\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"sorry, who is 南秋SouthAki?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"南秋SouthAki是一个前端开发工程师\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"ok,i know\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"今天他要讲一门关于华为仓颉编程的课程\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"ok,i know.do you need more information?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok ,下面这个是token计算的内容\n",
    "使用到了`Conversation Token Buffer`\n",
    "运用token,来决定什么时候刷新内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: sorry, who is 南秋SouthAki?\\nHuman: 南秋SouthAki是一个前端开发工程师\\nAI: ok,i know\\nHuman: 今天他要讲一门关于华为仓颉编程的课程\\nAI: ok,i know.do you need more information?\\nHuman: 不需要资料了,谢谢\\nAI: All right, see you next time.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import Tongyi\n",
    "\n",
    "llm = Tongyi(\n",
    "    model = \"Qwen-max\",\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit= 150\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"你好,帮我找下南秋SouthAki\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"sorry, who is 南秋SouthAki?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"南秋SouthAki是一个前端开发工程师\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"ok,i know\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"今天他要讲一门关于华为仓颉编程的课程\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"ok,i know.do you need more information?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"不需要资料了,谢谢\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"All right, see you next time.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的都是短时记忆的实现\n",
    "但是,当我们关机等的时候,我们有些数据不限丢失,于是我们需要实现长时记忆,来保存我们的数据\n",
    "langchain是使用向量数据库来存储之前的对话内容,有的向量数据库服务还提供自动摘要,每次对话的时候,都会从向量数据库里查询最相关的文档或者历史对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Human: 你好,帮我找下南秋SouthAki'),\n",
       " Document(page_content='Human: 南秋SouthAki是一个前端开发工程师'),\n",
       " Document(page_content='AI: sorry, who is 南秋SouthAki?'),\n",
       " Document(page_content='Human: 不需要资料了,谢谢')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先导入我们的模块\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"你好,帮我找下南秋SouthAki\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"sorry, who is 南秋SouthAki?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"南秋SouthAki是一个前端开发工程师\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"ok,i know\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"今天他要讲一门关于华为仓颉编程的课程\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"ok,i know.do you need more information?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"不需要资料了,谢谢\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"All right, see you next time.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    memory.buffer.split(\"\\n\"),\n",
    "    HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    ")\n",
    "\n",
    "FAISS.save_local(vectorstore, \"test_faiss\")\n",
    "\n",
    "FAISS.load_local(\"test_faiss\", HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\"),allow_dangerous_deserialization=True).similarity_search(\"SouthAki做什么职业?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 南秋SouthAki是一个前端开发工程师'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们来测试一下是否有成功写入向量数据库\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "r1 = FAISS.load_local(\"test_faiss\", embeddings=HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\"),allow_dangerous_deserialization=True)\n",
    "r2 = r1.as_retriever(\n",
    "    search_kwargs={\"k\": 1}\n",
    ")\n",
    "\n",
    "memory2 = VectorStoreRetrieverMemory(retriever=r2)\n",
    "\n",
    "memory2.load_memory_variables({\"prompt\": \"SouthAki是什么职业\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在链上使用memory\n",
    "- LLMChain\n",
    "- ConversationChain\n",
    "- 自定义\n",
    "- 同一个链合并使用多个记忆\n",
    "- 给多参数链增加记忆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个我们来实现的就是关于LLMChain的memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    你是一个可以和人类对话的AI机器人\n",
      "    \n",
      "    人类:你好\n",
      "    机器人:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    你是一个可以和人类对话的AI机器人\n",
      "    Human: 你好\n",
      "AI: 你好！有什么问题或者需要帮助的吗？\n",
      "    人类:我叫SouthAki,是一个前端工程师,你呢\n",
      "    机器人:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    你是一个可以和人类对话的AI机器人\n",
      "    Human: 你好\n",
      "AI: 你好！有什么问题或者需要帮助的吗？\n",
      "Human: 我叫SouthAki,是一个前端工程师,你呢\n",
      "AI: 你好，SouthAki！我是AI助手，专门设计来帮助解答问题和提供信息。我在乎你的前端开发相关的疑问，或者其他任何话题，我们都可以聊聊。有什么我可以帮助你的吗？\n",
      "    人类:我喜欢玩游戏,最喜欢玩Minecraft,你知道这个游戏吗?\n",
      "    机器人:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    你是一个可以和人类对话的AI机器人\n",
      "    Human: 你好\n",
      "AI: 你好！有什么问题或者需要帮助的吗？\n",
      "Human: 我叫SouthAki,是一个前端工程师,你呢\n",
      "AI: 你好，SouthAki！我是AI助手，专门设计来帮助解答问题和提供信息。我在乎你的前端开发相关的疑问，或者其他任何话题，我们都可以聊聊。有什么我可以帮助你的吗？\n",
      "Human: 我喜欢玩游戏,最喜欢玩Minecraft,你知道这个游戏吗?\n",
      "AI: 当然，Minecraft是一款非常受欢迎的沙盒游戏，由 Mojang Studios 开发。在游戏中，玩家可以在三维空间中建造、探索和生存，使用各种不同的方块和资源。你可以创造自己的建筑、红石电路、甚至是完全自定义的游戏模式。Minecraft有多个版本，包括Java版、Bedrock版（也称为口袋版或Windows 10版）以及主机版，适用于各种平台。你最喜欢Minecraft的哪个方面呢？\n",
      "    人类:你还记得我叫什么名字吗?\n",
      "    机器人:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当然，你是SouthAki，一个喜欢玩Minecraft的前端工程师。有什么关于Minecraft或者其他话题想要讨论的吗？'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先依旧导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 自定义一个模版\n",
    "template = \"\"\"\n",
    "    你是一个可以和人类对话的AI机器人\n",
    "    {chat_history}\n",
    "    人类:{human_input}\n",
    "    机器人:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"],\n",
    "    template=template,\n",
    ")\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key= \"chat_history\"\n",
    ")\n",
    "\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0,\n",
    "    model = \"Qwen-max\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    prompt = prompt,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "chain.predict(human_input= '你好')\n",
    "chain.predict(human_input= '我叫SouthAki,是一个前端工程师,你呢')\n",
    "chain.predict(human_input= '我喜欢玩游戏,最喜欢玩Minecraft,你知道这个游戏吗?')\n",
    "chain.predict(human_input= '你还记得我叫什么名字吗?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来的是对话的chatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 你好,我是一个可以和人类进行自然语言对话的AI机器人\n",
      "Human: 我叫SouthAki,是一个前端工程师,你呢\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 你好,我是一个可以和人类进行自然语言对话的AI机器人\n",
      "Human: 我叫SouthAki,是一个前端工程师,你呢\n",
      "AI: 你好，我是来自阿里云的语言模型，我叫通义千问。作为一个AI助手，我的目标是帮助用户获得准确、有用的信息，解决他们的问题和困惑。我会不断学习和进步，不断提升自己的能力，为用户提供更好的服务。如果您有任何问题或需要帮助，请随时告诉我，我会尽力提供支持。\n",
      "Human: 我喜欢玩游戏,最喜欢玩Minecraft,你知道这个游戏吗?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 你好,我是一个可以和人类进行自然语言对话的AI机器人\n",
      "Human: 我叫SouthAki,是一个前端工程师,你呢\n",
      "AI: 你好，我是来自阿里云的语言模型，我叫通义千问。作为一个AI助手，我的目标是帮助用户获得准确、有用的信息，解决他们的问题和困惑。我会不断学习和进步，不断提升自己的能力，为用户提供更好的服务。如果您有任何问题或需要帮助，请随时告诉我，我会尽力提供支持。\n",
      "Human: 我喜欢玩游戏,最喜欢玩Minecraft,你知道这个游戏吗?\n",
      "AI: Minecraft是一款非常受欢迎的沙盒式建造游戏，由瑞典开发团队Mojang Studios开发。在游戏中，玩家可以在一个三维空间中自由探索、采集资源，并使用这些资源建造建筑物、工具和其他各种物品。游戏提供了生存模式和创造模式等多种玩法，玩家可以根据自己的喜好选择不同的模式来体验游戏的乐趣。Minecraft以其极高的自由度和丰富的可玩性受到了全球众多玩家的喜爱。\n",
      "Human: 你还记得我叫什么名字吗?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你是叫做 SouthAki 的前端工程师，很高兴认识你！'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先依旧是导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "# 定义提问提示模版\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content = \"你好,我是一个可以和人类进行自然语言对话的AI机器人\",\n",
    "            role = \"system\"\n",
    "        ),\n",
    "        # Meassagesplaceholder: 是一个占位符,这里的作用是传入我们的历史聊天信息\n",
    "        MessagesPlaceholder(\n",
    "            variable_name= \"chat_history\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 先来看看格式是否符合我们预期\n",
    "# print(prompt.format_prompt(human_input=\"你好\",chat_history = []))\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key= \"chat_history\",\n",
    "    return_messages = True\n",
    ")\n",
    "\n",
    "llm = ChatTongyi(\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0,\n",
    "    model_name = \"qwen-vl-max\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    prompt = prompt,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# chain.predict(human_input= '你好')\n",
    "chain.predict(human_input= '我叫SouthAki,是一个前端工程师,你呢')\n",
    "chain.predict(human_input= '我喜欢玩游戏,最喜欢玩Minecraft,你知道这个游戏吗?')\n",
    "chain.predict(human_input= '你还记得我叫什么名字吗?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一个的是对话链\n",
    "ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: 你好\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='你好'), AIMessage(content='你好！有什么问题或者需要帮助的吗？我可以提供各种信息，比如天气预报、新闻更新、食谱建议、电影推荐，甚至是帮你解决数学问题。只要你告诉我你想要什么，我都会尽力协助你。')]\n",
      "Human: 我叫SouthAki,你呢\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='你好'), AIMessage(content='你好！有什么问题或者需要帮助的吗？我可以提供各种信息，比如天气预报、新闻更新、食谱建议、电影推荐，甚至是帮你解决数学问题。只要你告诉我你想要什么，我都会尽力协助你。'), HumanMessage(content='我叫SouthAki,你呢'), AIMessage(content='你好，SouthAki！我是来自云端的智能助手，你可以叫我Aki。很高兴与你交流。如果你有任何需要，比如想知道今天的天气、寻找一道美食的做法，或者是想了解最新的科技资讯，尽管告诉我，我会尽我所能帮助你的。')]\n",
      "Human: 今天天气真好,我感觉我想去北京旅游\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='你好'), AIMessage(content='你好！有什么问题或者需要帮助的吗？我可以提供各种信息，比如天气预报、新闻更新、食谱建议、电影推荐，甚至是帮你解决数学问题。只要你告诉我你想要什么，我都会尽力协助你。'), HumanMessage(content='我叫SouthAki,你呢'), AIMessage(content='你好，SouthAki！我是来自云端的智能助手，你可以叫我Aki。很高兴与你交流。如果你有任何需要，比如想知道今天的天气、寻找一道美食的做法，或者是想了解最新的科技资讯，尽管告诉我，我会尽我所能帮助你的。'), HumanMessage(content='今天天气真好,我感觉我想去北京旅游'), AIMessage(content='听起来像是个不错的计划！北京是一个充满历史和文化的城市，有许多值得一看的地方，比如故宫、长城、天坛和颐和园等。如果你打算去北京旅游，我可以为你提供一些旅行建议：\\n\\n1. **时间规划**：确保你有足够的时间来探索这个城市，因为北京的景点众多。一般来说，至少需要3-4天才能游览主要的名胜古迹。\\n\\n2. **交通**：北京的公共交通系统非常发达，地铁网络覆盖全市，方便快捷。你也可以使用共享单车或打车软件，如滴滴出行，来穿梭于各个景点之间。\\n\\n3. **美食**：别忘了品尝北京的特色美食，比如北京烤鸭、炸酱面和豆汁儿炸果。\\n\\n4. **文化体验**：可以考虑观看一场京剧表演，感受中国的传统戏曲艺术。\\n\\n5. **天气**：记得查看一下未来几天北京的天气预报，以便做好准备。如果现在天气好，那真是个出游的好时机。\\n\\n6. **门票和住宿**：提前预订景点门票和酒店，以避免高峰期的人流和价格上涨。\\n\\n7. **防疫措施**：由于当前全球疫情情况，记得检查当地的防疫政策，确保你的行程顺利。\\n\\n如果你需要更具体的信息，比如推荐的酒店、餐厅或者旅行路线，随时告诉我！')]\n",
      "Human: 帮我做个一日游攻略\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当然可以，SouthAki。以下是一份北京一日游的攻略，旨在让你在短时间内尽可能多地体验北京的魅力：\\n\\n**早晨：**\\n1. **天安门广场**（6:30-8:00）：早起参观天安门广场，观看升旗仪式（具体时间可能因日期而异，建议提前查询），然后游览人民英雄纪念碑、毛主席纪念堂。\\n\\n2. **故宫博物院**（8:30-12:00）：从天安门广场步行即可到达故宫，这里是中国明清两代皇家宫殿，有很多历史文物和故事。\\n\\n**午餐：**\\n3. **王府井小吃街**（12:00-13:00）：品尝北京地道小吃，如炸酱面、艾窝窝、糖葫芦等。\\n\\n**下午：**\\n4. **景山公园**（13:30-14:30）：从故宫北门出来，步行到景山公园，登高望远，可以看到故宫全景。\\n\\n5. **北海公园**（15:00-17:00）：接着步行或乘坐公交车到北海公园，划船、欣赏白塔，感受古典园林之美。\\n\\n**傍晚：**\\n6. **南锣鼓巷**（17:30-19:00）：逛一逛这条充满老北京风情的胡同，体验当地文化，还可以购买一些手工艺品或纪念品。\\n\\n**晚餐：**\\n7. **簋街**（19:30-21:00）：前往簋街享用晚餐，这里有很多著名的餐馆，可以尝试正宗的北京烤鸭。\\n\\n**夜生活：**\\n8. **后海酒吧街**（21:00-23:00）：结束晚餐后，可以到后海散步，享受北京的夜生活，这里有各种风格的酒吧和音乐演出。\\n\\n请注意，这是一条紧凑的行程，可能需要根据实际情况进行调整。如果时间允许，你还可以考虑在一天中的某个时段参观附近的恭王府或国家博物馆。希望这个攻略对你有所帮助，祝你在北京度过愉快的一天！'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们来尝试一下,使用预制链提供的东西\n",
    "# 首先导入模块\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# 依旧是定义我们的大模型\n",
    "llm = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0\n",
    ")\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    "    memory_key= \"history\"\n",
    ")\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "chain.predict(input=\"你好\")\n",
    "chain.predict(input=\"我叫SouthAki,你呢\")\n",
    "chain.predict(input=\"今天天气真好,我感觉我想去北京旅游\")\n",
    "chain.predict(input=\"帮我做个一日游攻略\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到使用预制链确实很轻松的实现了\n",
    "但是某些时候,我们需要通过定制,使得llm输出更符合我们的预期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    下面是一段AI与人类的对话,AI会针对人类的问题,提供尽可能详细回答,如果AI不知道答案,会直接回复'Soory, I don't know the answer.'\n",
      "    当前对话:[]\n",
      "    human:你好\n",
      "    AI助手:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    下面是一段AI与人类的对话,AI会针对人类的问题,提供尽可能详细回答,如果AI不知道答案,会直接回复'Soory, I don't know the answer.'\n",
      "    当前对话:[HumanMessage(content='你好'), AIMessage(content='你好！有什么问题或者需要帮助的吗？')]\n",
      "    human:今天天气真好,适合旅游啊\n",
      "    AI助手:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    下面是一段AI与人类的对话,AI会针对人类的问题,提供尽可能详细回答,如果AI不知道答案,会直接回复'Soory, I don't know the answer.'\n",
      "    当前对话:[HumanMessage(content='你好'), AIMessage(content='你好！有什么问题或者需要帮助的吗？'), HumanMessage(content='今天天气真好,适合旅游啊'), AIMessage(content='是的，天气好的时候确实很适合出游。如果你正在计划一次旅行，我可以帮你提供一些旅行建议，比如目的地推荐、行程规划或者天气预报等信息。你有特定的目的地或者需求吗？')]\n",
      "    human:我最近想去南京,你有什么推荐的吗,帮我做个一日游攻略出来\n",
      "    AI助手:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    下面是一段AI与人类的对话,AI会针对人类的问题,提供尽可能详细回答,如果AI不知道答案,会直接回复'Soory, I don't know the answer.'\n",
      "    当前对话:[HumanMessage(content='你好'), AIMessage(content='你好！有什么问题或者需要帮助的吗？'), HumanMessage(content='今天天气真好,适合旅游啊'), AIMessage(content='是的，天气好的时候确实很适合出游。如果你正在计划一次旅行，我可以帮你提供一些旅行建议，比如目的地推荐、行程规划或者天气预报等信息。你有特定的目的地或者需求吗？'), HumanMessage(content='我最近想去南京,你有什么推荐的吗,帮我做个一日游攻略出来'), AIMessage(content='当然可以！南京是一个充满历史文化和美食的城市，以下是我为你准备的一日游攻略：\\n\\n1. **早上**：\\n   - **中山陵**：作为孙中山先生的陵墓，中山陵是南京的标志性景点。你可以先去这里，欣赏宏伟的建筑并了解历史。\\n   - **明孝陵**：位于中山陵附近，是明朝开国皇帝朱元璋和王皇后的陵墓，石像路和神道非常有名。\\n\\n2. **午餐**：\\n   - 在附近的**夫子庙秦淮风光带**品尝南京特色小吃，如鸭血粉丝汤、小笼包、盐水鸭等。\\n\\n3. **下午**：\\n   - **夫子庙**：游览这座历史悠久的孔庙和江南贡院，感受古代科举文化。\\n   - **老门东历史文化街区**：体验南京的传统民俗文化和购物。\\n\\n4. **傍晚**：\\n   - **玄武湖公园**：在湖边散步，欣赏夕阳和湖光山色，也可以租一辆自行车环湖骑行。\\n\\n5. **晚餐**：\\n   - 推荐尝试南京的地道餐馆，比如“绿柳居”或“蒋有记”，享用一顿正宗的南京菜。\\n\\n6. **晚上**：\\n   - **夜游秦淮河**：乘坐游船欣赏两岸的夜景，感受古都的繁华与宁静。\\n   - **新街口**：如果时间允许，还可以去南京的商业中心新街口逛逛，体验现代都市的繁华。\\n\\n记得提前查看天气预报，带上防晒和雨具。祝你在南京度过愉快的一天！')]\n",
      "    human:我叫什么名字?\n",
      "    AI助手:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你的名字我没有记录，因为你之前没有告诉我。如果你愿意告诉我，我很乐意记住它。'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义一下,对其进行覆盖\n",
    "# 首先依旧导入我们的模块\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Tongyi\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# 定义大模型\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0,\n",
    "    model = \"Qwen-max\"\n",
    ")\n",
    "\n",
    "# 设置问题模版\n",
    "template = \"\"\"\n",
    "    下面是一段AI与人类的对话,AI会针对人类的问题,提供尽可能详细回答,如果AI不知道答案,会直接回复'Soory, I don't know the answer.'\n",
    "    当前对话:{history}\n",
    "    human:{input}\n",
    "    AI助手:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = ConversationBufferMemory(\n",
    "        # 这里为什么要有一个ai_prefix呢?\n",
    "        # 因为我们上面修改了那个问题的信息回复人,如果不用占位符,就会导致报错\n",
    "        ai_prefix = \"AI助手\",\n",
    "        return_messages = True\n",
    "    ),\n",
    "    prompt = prompt,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# 测试 提出问题\n",
    "chain.predict(input = \"你好\")\n",
    "chain.predict(input = \"今天天气真好,适合旅游啊\")\n",
    "chain.predict(input = \"我最近想去南京,你有什么推荐的吗,帮我做个一日游攻略出来\")\n",
    "chain.predict(input = \"我叫什么名字?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面,我们展示一下同一条链合并使用多个memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    下面是一段AI与人类的对话,AI会针对人类的问题,尽力的给出详细答案,如果不知道问题的答案,会回答'Sorry, I don't know the answer to that question.'\n",
      "    之前的对话摘要:\n",
      "    当前对话:\n",
      "    Human:你好\n",
      "    AI:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    下面是一段AI与人类的对话,AI会针对人类的问题,尽力的给出详细答案,如果不知道问题的答案,会回答'Sorry, I don't know the answer to that question.'\n",
      "    之前的对话摘要:The human greets the AI in Chinese, and the AI responds with a friendly greeting, offering assistance.\n",
      "    当前对话:Human: 你好\n",
      "AI: 你好！有什么问题我可以帮助你解答吗？\n",
      "    Human:我想知道加密货币的相关知识\n",
      "    AI:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    下面是一段AI与人类的对话,AI会针对人类的问题,尽力的给出详细答案,如果不知道问题的答案,会回答'Sorry, I don't know the answer to that question.'\n",
      "    之前的对话摘要:The human inquires about cryptocurrency, and the AI explains that it is a digital or virtual currency that uses encryption techniques for secure transactions and asset transfers, operating independently of central banks. Bitcoin is the most well-known, but there are others like Ethereum, Litecoin, and Dogecoin. Key features of cryptocurrencies include decentralization through blockchain technology, security from fraud, anonymity for users, transaction transparency, and the irreversibility of confirmed transactions. The AI suggests areas for further study and offers assistance with specific questions on buying cryptocurrency or its underlying mechanisms.\n",
      "    当前对话:Human: 你好\n",
      "AI: 你好！有什么问题我可以帮助你解答吗？\n",
      "Human: 我想知道加密货币的相关知识\n",
      "AI: 加密货币，也称为数字货币或虚拟货币，是一种使用加密技术来保障交易安全并确认资产转移的电子货币。它的设计使得它能够独立于中央银行运作。最知名的加密货币是比特币（Bitcoin），但还有许多其他的选择，如以太坊（Ethereum）、莱特币（Litecoin）和狗狗币（Dogecoin）等。\n",
      "\n",
      "加密货币的核心特性包括：\n",
      "\n",
      "1. **去中心化**：没有中央机构控制交易，而是通过区块链技术进行分布式记账。\n",
      "2. **安全性**：交易通过加密技术保证，防止欺诈和双重支付。\n",
      "3. **匿名性**：虽然交易记录在公共区块链上，但用户可以通过匿名的方式拥有和交易加密货币。\n",
      "4. **透明性**：所有交易历史对公众开放，任何人都可以查看。\n",
      "5. **不可逆转性**：一旦交易被确认，就无法撤销。\n",
      "\n",
      "要了解更多关于加密货币的信息，你可以深入研究区块链技术、挖矿过程、钱包管理以及市场动态等方面。如果你有具体的问题，比如如何购买加密货币，或者加密货币的工作原理，随时告诉我，我会尽我所能提供帮助。\n",
      "    Human:好厉害啊,那你对BTC有认识吗？\n",
      "    AI:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    下面是一段AI与人类的对话,AI会针对人类的问题,尽力的给出详细答案,如果不知道问题的答案,会回答'Sorry, I don't know the answer to that question.'\n",
      "    之前的对话摘要:The human expresses admiration and asks about BTC, to which the AI confirms that BTC stands for Bitcoin, the first and most well-known cryptocurrency created in 2009 by an unknown person or group using the pseudonym Satoshi Nakamoto. Bitcoin was designed to be decentralized and independent of government or financial institution control. Key aspects of Bitcoin include:\n",
      "\n",
      "1. **Limited supply**: There's a maximum limit of 21 million Bitcoins, expected to be reached around 2140, which contributes to its value preservation.\n",
      "2. **Proof-of-Work consensus**: It employs a Proof-of-Work mechanism for transaction validation and block creation, a process known as mining.\n",
      "3. **Transaction confirmation**: Transactions need to be validated by miners and added to the blockchain, typically taking around 10 minutes, depending on network conditions.\n",
      "4. **Decentralization**: The Bitcoin network is maintained by computers worldwide, ensuring decentralization and resistance to censorship.\n",
      "\n",
      "The AI invites the user to inquire further about Bitcoin's technical details, investment considerations, or mining participation.\n",
      "    当前对话:Human: 你好\n",
      "AI: 你好！有什么问题我可以帮助你解答吗？\n",
      "Human: 我想知道加密货币的相关知识\n",
      "AI: 加密货币，也称为数字货币或虚拟货币，是一种使用加密技术来保障交易安全并确认资产转移的电子货币。它的设计使得它能够独立于中央银行运作。最知名的加密货币是比特币（Bitcoin），但还有许多其他的选择，如以太坊（Ethereum）、莱特币（Litecoin）和狗狗币（Dogecoin）等。\n",
      "\n",
      "加密货币的核心特性包括：\n",
      "\n",
      "1. **去中心化**：没有中央机构控制交易，而是通过区块链技术进行分布式记账。\n",
      "2. **安全性**：交易通过加密技术保证，防止欺诈和双重支付。\n",
      "3. **匿名性**：虽然交易记录在公共区块链上，但用户可以通过匿名的方式拥有和交易加密货币。\n",
      "4. **透明性**：所有交易历史对公众开放，任何人都可以查看。\n",
      "5. **不可逆转性**：一旦交易被确认，就无法撤销。\n",
      "\n",
      "要了解更多关于加密货币的信息，你可以深入研究区块链技术、挖矿过程、钱包管理以及市场动态等方面。如果你有具体的问题，比如如何购买加密货币，或者加密货币的工作原理，随时告诉我，我会尽我所能提供帮助。\n",
      "Human: 好厉害啊,那你对BTC有认识吗？\n",
      "AI: 当然，BTC是比特币（Bitcoin）的缩写，它是第一个也是最著名的加密货币。比特币由一个化名为中本聪（Satoshi Nakamoto）的未知个人或团队于2009年创建。它的设计目的是作为一种去中心化的数字货币，不受任何政府或金融机构的控制。\n",
      "\n",
      "比特币的关键特点包括：\n",
      "\n",
      "- **总量上限**：比特币的供应量上限为2100万个，预计将在2140年前达到这个上限，这种有限的供应有助于维持其价值。\n",
      "- **工作量证明**：比特币网络使用工作量证明（Proof-of-Work, PoW）共识机制来验证交易和创建新的区块，这个过程通常被称为“挖矿”。\n",
      "- **交易确认**：每个交易需要被网络中的矿工确认并打包到区块链中，这个过程通常需要大约10分钟，并且随着网络拥堵程度的不同，可能需要更多时间。\n",
      "- **去中心化**：比特币网络由全球各地的计算机节点维护，这些节点存储并同步整个区块链，确保了系统的去中心化和抗审查性。\n",
      "\n",
      "如果你想了解更多关于比特币的细节，如它的技术细节、投资考虑、或者如何参与挖矿，都可以继续提问。\n",
      "    Human:那ETH呢?\n",
      "    AI:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ETH是以太坊（Ethereum）的缩写，是以太坊网络的原生加密货币。以太坊是由 Vitalik Buterin 在2013年提出的，并在2015年正式上线。与比特币主要作为价值存储不同，以太坊是一个开放的、分散的平台，允许开发人员构建和运行去中心化的应用程序（DApps）以及智能合约。\\n\\n以太坊的主要特点包括：\\n\\n1. **智能合约**：以太坊网络支持智能合约，这是一种自动执行的程序，能够在满足预定条件时执行预设的操作，这为创建去中心化的金融产品、游戏、身份验证系统等提供了可能。\\n2. **gas和交易费**：在以太坊上执行操作需要消耗“gas”，这是以太币的小额支付，用于支付执行智能合约所需的计算资源。\\n3. **升级计划**：以太坊正在经历从工作量证明（PoW）到权益证明（Proof-of-Stake, PoS）的过渡，这个过程被称为“以太坊2.0”。PoS将减少能源消耗，提高效率，并增加网络的安全性。\\n4. **可扩展性挑战**：尽管以太坊是目前最广泛使用的智能合约平台，但它面临着交易速度慢和交易费用高的问题。社区正在努力通过分片和其他技术解决方案来解决这些问题。\\n\\n如果你对以太坊的生态系统、智能合约的工作原理、以太坊2.0的进展，或者其他相关话题感兴趣，欢迎继续提问。'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先还是导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    CombinedMemory\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 定义大模型\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key = api_key,\n",
    "    model = \"Qwen-max\",\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# 使用ConversationSummaryMemory对对话进行总结\n",
    "summary = ConversationSummaryMemory(\n",
    "    llm= llm,\n",
    "    input_key= 'input'\n",
    ")\n",
    "\n",
    "# 使用ConversationBufferMemory对对话进行缓存\n",
    "cov_memory = ConversationBufferMemory(\n",
    "    memory_key= 'history_now',\n",
    "    input_key= 'input'\n",
    ")\n",
    "\n",
    "memory = CombinedMemory(\n",
    "    memories=[summary, cov_memory]\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    下面是一段AI与人类的对话,AI会针对人类的问题,尽力的给出详细答案,如果不知道问题的答案,会回答'Sorry, I don't know the answer to that question.'\n",
    "    之前的对话摘要:{history}\n",
    "    当前对话:{history_now}\n",
    "    Human:{input}\n",
    "    AI:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template= template,\n",
    "    input_variables=['history', 'history_now', 'input']\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    prompt = prompt,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "chain.run(\"你好\")\n",
    "chain.run(\"我想知道加密货币的相关知识\")\n",
    "chain.run(\"好厉害啊,那你对BTC有认识吗？\")\n",
    "chain.run(\"那ETH呢?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面来讲一个多参数链增加记忆的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "with open('./letter.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    # 对文本进行切分\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, separator=\"\\n\")\n",
    "    texts = text_splitter.split_text(text)\n",
    "\n",
    "    # 使用HuggingFaceEmbeddings进行向量化\n",
    "    embeedings = HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-small-en\"\n",
    "    )\n",
    "    # 使用chroma去存储向量\n",
    "    docsearch = Chroma.from_texts(\n",
    "        texts,\n",
    "        embeedings\n",
    "    )\n",
    "    query = \"这家公司有什么新策略?\"\n",
    "    docs = docsearch.similarity_search(query = query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当上面成功后,我们需要构建一个问答对话链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    下面是一段AI与人类的对话,AI会针对人类的问题,提供尽可能详细的回答,如果AI不知道这个问题的答案,会返回一段字符串'Sorry, I don't know the answer.',依旧参考一下相关文档以及历史对话信息,AI会据此组织最终回答的内容.[Generated with ChatGPT]\n",
      "Confidential Document - For Internal Use Only\n",
      "Date: July 1, 2023\n",
      "Subject: Updates and Discussions on Various Topics\n",
      "Dear Team,\n",
      "I hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.\n",
      "Security and Privacy Measures\n",
      "As part of our ongoing commitment to ensure the security and privacy of our customers' data, we have implemented robust measures across all our systems. We would like to commend John Doe (email: john.doe@example.com) from the IT department for his diligent work in enhancing our network security. Moving forward, we kindly remind everyone to strictly adhere to our data protection policies and guidelines. Additionally, if you come across any potential security risks or incidents, please report them immediately to our dedicated team at security@example.com.\n",
      "\n",
      "[Generated with ChatGPT]\n",
      "Confidential Document - For Internal Use Only\n",
      "Date: July 1, 2023\n",
      "Subject: Updates and Discussions on Various Topics\n",
      "Dear Team,\n",
      "I hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.\n",
      "Security and Privacy Measures\n",
      "As part of our ongoing commitment to ensure the security and privacy of our customers' data, we have implemented robust measures across all our systems. We would like to commend John Doe (email: john.doe@example.com) from the IT department for his diligent work in enhancing our network security. Moving forward, we kindly remind everyone to strictly adhere to our data protection policies and guidelines. Additionally, if you come across any potential security risks or incidents, please report them immediately to our dedicated team at security@example.com.\n",
      "\n",
      "[Generated with ChatGPT]\n",
      "Confidential Document - For Internal Use Only\n",
      "Date: July 1, 2023\n",
      "Subject: Updates and Discussions on Various Topics\n",
      "Dear Team,\n",
      "I hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.\n",
      "Security and Privacy Measures\n",
      "As part of our ongoing commitment to ensure the security and privacy of our customers' data, we have implemented robust measures across all our systems. We would like to commend John Doe (email: john.doe@example.com) from the IT department for his diligent work in enhancing our network security. Moving forward, we kindly remind everyone to strictly adhere to our data protection policies and guidelines. Additionally, if you come across any potential security risks or incidents, please report them immediately to our dedicated team at security@example.com.\n",
      "\n",
      "[Generated with ChatGPT]\n",
      "Confidential Document - For Internal Use Only\n",
      "Date: July 1, 2023\n",
      "Subject: Updates and Discussions on Various Topics\n",
      "Dear Team,\n",
      "I hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.\n",
      "Security and Privacy Measures\n",
      "As part of our ongoing commitment to ensure the security and privacy of our customers' data, we have implemented robust measures across all our systems. We would like to commend John Doe (email: john.doe@example.com) from the IT department for his diligent work in enhancing our network security. Moving forward, we kindly remind everyone to strictly adhere to our data protection policies and guidelines. Additionally, if you come across any potential security risks or incidents, please report them immediately to our dedicated team at security@example.com.\n",
      "    []\n",
      "    Human: 公司有什么新策略?\n",
      "    AI:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content=\"[Generated with ChatGPT]\\nConfidential Document - For Internal Use Only\\nDate: July 1, 2023\\nSubject: Updates and Discussions on Various Topics\\nDear Team,\\nI hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.\\nSecurity and Privacy Measures\\nAs part of our ongoing commitment to ensure the security and privacy of our customers' data, we have implemented robust measures across all our systems. We would like to commend John Doe (email: john.doe@example.com) from the IT department for his diligent work in enhancing our network security. Moving forward, we kindly remind everyone to strictly adhere to our data protection policies and guidelines. Additionally, if you come across any potential security risks or incidents, please report them immediately to our dedicated team at security@example.com.\"),\n",
       "  Document(page_content=\"[Generated with ChatGPT]\\nConfidential Document - For Internal Use Only\\nDate: July 1, 2023\\nSubject: Updates and Discussions on Various Topics\\nDear Team,\\nI hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.\\nSecurity and Privacy Measures\\nAs part of our ongoing commitment to ensure the security and privacy of our customers' data, we have implemented robust measures across all our systems. We would like to commend John Doe (email: john.doe@example.com) from the IT department for his diligent work in enhancing our network security. Moving forward, we kindly remind everyone to strictly adhere to our data protection policies and guidelines. Additionally, if you come across any potential security risks or incidents, please report them immediately to our dedicated team at security@example.com.\"),\n",
       "  Document(page_content=\"[Generated with ChatGPT]\\nConfidential Document - For Internal Use Only\\nDate: July 1, 2023\\nSubject: Updates and Discussions on Various Topics\\nDear Team,\\nI hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.\\nSecurity and Privacy Measures\\nAs part of our ongoing commitment to ensure the security and privacy of our customers' data, we have implemented robust measures across all our systems. We would like to commend John Doe (email: john.doe@example.com) from the IT department for his diligent work in enhancing our network security. Moving forward, we kindly remind everyone to strictly adhere to our data protection policies and guidelines. Additionally, if you come across any potential security risks or incidents, please report them immediately to our dedicated team at security@example.com.\"),\n",
       "  Document(page_content=\"[Generated with ChatGPT]\\nConfidential Document - For Internal Use Only\\nDate: July 1, 2023\\nSubject: Updates and Discussions on Various Topics\\nDear Team,\\nI hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.\\nSecurity and Privacy Measures\\nAs part of our ongoing commitment to ensure the security and privacy of our customers' data, we have implemented robust measures across all our systems. We would like to commend John Doe (email: john.doe@example.com) from the IT department for his diligent work in enhancing our network security. Moving forward, we kindly remind everyone to strictly adhere to our data protection policies and guidelines. Additionally, if you come across any potential security risks or incidents, please report them immediately to our dedicated team at security@example.com.\")],\n",
       " 'human_input': '公司有什么新策略?',\n",
       " 'chat_history': [HumanMessage(content='公司有什么新策略?'),\n",
       "  AIMessage(content=\"Sorry, I don't know the answer. The provided documents focus on security and privacy measures but do not mention any new company strategies.\")],\n",
       " 'output_text': \"Sorry, I don't know the answer. The provided documents focus on security and privacy measures but do not mention any new company strategies.\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "with open('./letter.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    # 对文本进行切分\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, separator=\"\\n\")\n",
    "    texts = text_splitter.split_text(text)\n",
    "\n",
    "    # 使用HuggingFaceEmbeddings进行向量化\n",
    "    embeedings = HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-small-en\"\n",
    "    )\n",
    "    # 使用chroma去存储向量\n",
    "    docsearch = Chroma.from_texts(\n",
    "        texts,\n",
    "        embeedings\n",
    "    )\n",
    "    query = \"这家公司有什么新策略?\"\n",
    "    docs = docsearch.similarity_search(query = query)\n",
    "\n",
    "# 首先依旧是导入模块\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 定义大模型\n",
    "llm = Tongyi(\n",
    "    model = \"Qwen-max\",\n",
    "    dashscope_api_key = api_key,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    下面是一段AI与人类的对话,AI会针对人类的问题,提供尽可能详细的回答,如果AI不知道这个问题的答案,会返回一段字符串'Sorry, I don't know the answer.',依旧参考一下相关文档以及历史对话信息,AI会据此组织最终回答的内容.{context}\n",
    "    {chat_history}\n",
    "    Human: {human_input}\n",
    "    AI:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template= template,\n",
    "    input_variables = [\"context\",\"chat_history\",\"human_input\"]\n",
    ")\n",
    "\n",
    "# 使用ConversationBufferMemory对对话进行缓存\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key= \"chat_history\",\n",
    "    input_key= \"human_input\",\n",
    "    return_messages= True\n",
    ")\n",
    "\n",
    "# 加载我们的对话链\n",
    "chain = load_qa_chain(\n",
    "    llm= llm,\n",
    "    memory= memory,\n",
    "    prompt= prompt,\n",
    "    verbose= True,\n",
    "    chain_type= \"stuff\"\n",
    ")\n",
    "\n",
    "# 调用一下上面的文档和我们的问答历史\n",
    "chain({\"input_documents\":docs,\"human_input\":\"公司有什么新策略?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
