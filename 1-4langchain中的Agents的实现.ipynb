{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个简单的Agents\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# 首先依旧导入模块\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "import os\n",
    "\n",
    "llm = ChatTongyi(\n",
    "    dashscope_api_key = api_key,\n",
    "    model = \"Qwen-max\",\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# 搭建工具\n",
    "# 我们使用一个工具,serpai是一个聚合搜索引擎,需要安装谷歌搜索包和申请apikey\n",
    "# https://serpapi.com/dashboard\n",
    "# llm-math 是一个封装好的数学计算链\n",
    "# 首先我们需要安装谷歌搜索包\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m我需要知道前任女王的名字和年龄，然后用计算器计算她年龄的平方。\n",
      "Action: Search\n",
      "Action Input: \"英国前任女王名字 年龄\"\n",
      "Observation:\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['伊丽莎白二世（英语：Elizabeth II；1926年4月21日—2022年9月8日），全名伊丽莎白·亚历山德拉·玛丽（英语：Elizabeth Alexandra Mary），是1952年至2022年间英国暨其他英 ...', '到了18岁成年之际，随着英国议会修改法律，伊丽莎白公主跻身五名英国国务顾问之一，可在其父无法履职或身在国外之时，代行王室职责。1945年，她说服乔治六 ...', '伊莉莎白二世（英語：Elizabeth II；1926年4月21日—2022年9月8日），全名伊莉莎白·亞歷山德拉·瑪麗（英語：Elizabeth Alexandra Mary），是1952年至2022年間英國暨其他大 ...', '... ，是日本漫画《名侦探柯南》及其衍生作品中的角色。是一名律师，拥有“律政界女王 ... 生日. 10月10日. 年龄. 37岁（初登场）→38岁. 毕业院校. 帝丹高中 ...', '楼主的话：. 当地时间9月8日，英国女王伊丽莎白二世去世，终年96岁。从1952年登基至今，伊丽莎白二世在位70年，是英国历史上在位时间最长的君主。', '国内时间9月9日，据英国媒体报道，现任英国女王伊丽莎白二世于英国时间9月8日因身体原因不幸逝世，享年96岁，此消息公布后，瞬间引起了全世界网友的 ...', '英国女王即英国女性君主。英国君主是英国及英国海外领地的国家元首，现在的男性君主称为国王（King），女性君主称为女王（Queen）。1953年6月2日，伊丽莎白二世正式 ...', '英国女王伊丽莎白二世9月8日下午6时30分离开了钟爱她的臣民，也为一个时代画上了句号。史上年龄最大的王储查尔斯王子也终于顺位成为国王，他做的第一 ...', 'https://vimeo.com/manage/videos/968601255']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m我现在知道了前任女王的名字是伊丽莎白二世，以及她的年龄是96岁，现在我可以使用计算器来算出她年龄的平方了。\n",
      "Action: Calculator\n",
      "Action Input: 96*96\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 9216\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m我现在知道最终答案了\n",
      "Final Answer: 英国前任女王的名字是伊丽莎白二世，她去世时的年龄为96岁，年龄的平方是9216。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'英国前任女王的名字是伊丽莎白二世，她去世时的年龄为96岁，年龄的平方是9216。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入模块\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# 首先依旧导入模块\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "import os\n",
    "\n",
    "llm = ChatTongyi(\n",
    "    dashscope_api_key = dashscope_api_key,\n",
    "    model = \"qwen-vl-max\",\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "# 搭建工具\n",
    "tools = load_tools(\n",
    "    [\"serpapi\", \"llm-math\"],\n",
    "    llm=llm,\n",
    "    serpapi_api_key=serpapi_api_key\n",
    ")\n",
    "\n",
    "# 定义agents使用小样本增强\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "# 定义工具\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,# 这里有不同的类型\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"英国前任女王的名字叫什么,她的年龄的平方是多少?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几种内置的主要agent类型\n",
    "- openai_functions OpenAI函数调用型\n",
    "- zero_short_react_description 零样本增强生成型\n",
    "- chat_zero_shot_react_description 零样本增强对话型\n",
    "- conversational_react_description 对话增强生成型\n",
    "- chat_conversational_react_description 对话增强结构化\n",
    "- structued_chat_zero_short_react_description 零样本增强结构化对话型\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "零样本实现\n",
    "零样本增强生成,既在没有示例的情况下,可以自主进行对话的类型[零样本,单样本,少样本]\n",
    "https://blog.csdn.net/zcyzcyjava/article/details/127006287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m 新西兰是君主立宪制国家，其国家元首是英国女王。因此，上一任新西兰女王就是指的是英国的前一位君主。目前英国的君主是伊丽莎白二世，但她在2022年9月8日去世，随后由她的儿子查尔斯三世继位。我会计算查尔斯三世现在的年龄除以2。\n",
      "Action: Calculator\n",
      "Action Input: 74 / 2\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 37.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m我找到了答案。\n",
      "Final Answer: 上一任新西兰女王是查尔斯三世，他的年龄除以2是37。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'上一任新西兰女王是查尔斯三世，他的年龄除以2是37。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import Tongyi\n",
    "from langchain.agents import (\n",
    "    load_tools,\n",
    "    initialize_agent,\n",
    "    AgentType\n",
    ")\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key = dashscope_api_key,\n",
    "    model = 'Qwen-max',\n",
    "    temperature =0\n",
    ")\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, serpapi_api_key=serpapi_api_key)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run('上一任新西兰女王是谁?她年龄除以2是多少?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chat_zero_short_react_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The third pole of the world refers to the Qinghai-Tibet Plateau. It's also known as the \"Third Pole of the Earth\", which means it is the third highest elevation region on earth after the South Pole and the North Pole. The average altitude of the Qinghai-Tibet Plateau is more than 4000 meters, so its height multiplied by 2 will be greater than 8000 meters. I need to use the calculator to perform this calculation.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"4000 * 2\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 8000\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: The height of the Qinghai-Tibet Plateau multiplied by 2 is 8000 meters.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The height of the Qinghai-Tibet Plateau multiplied by 2 is 8000 meters.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain.llms import Tongyi\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.agents import (\n",
    "    load_tools,\n",
    "    initialize_agent,\n",
    "    AgentType\n",
    ")\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "llm = ChatTongyi(\n",
    "    dashscope_api_key = dashscope_api_key,\n",
    "    model_name = 'qwen-vl-max',\n",
    "    temperature =0\n",
    ")\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, serpapi_api_key=serpapi_api_key)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run('世界第三极是什么?它的高度乘2是多少?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conversational_react_description 一个对话型的agent,这个agent要求与memory一起使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=ConversationBufferMemory(memory_key='chat_history') verbose=True tags=['conversational-react-description'] agent=ConversationalAgent(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input'], template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\n\\nTOOLS:\\n------\\n\\nAssistant has access to the following tools:\\n\\n> Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\n> Calculator: Useful for when you need to answer questions about math.\\n\\nTo use a tool, please use the following format:\\n\\n```\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [Search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n```\\n\\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\\n\\n```\\nThought: Do I need to use a tool? No\\nAI: [your response here]\\n```\\n\\nBegin!\\n\\nPrevious conversation history:\\n{chat_history}\\n\\nNew input: {input}\\n{agent_scratchpad}'), llm=Tongyi(client=<class 'dashscope.aigc.generation.Generation'>, dashscope_api_key='sk-2a9e44d9621f4fd1b5f9f2c081779215')), output_parser=ConvoOutputParser(), allowed_tools=['Search', 'Calculator'], ai_prefix='AI') tools=[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='ebcc8130fff3a121b7029d6db2bd880991535ffa5ce8df722ba3d4aa1a892f73', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='ebcc8130fff3a121b7029d6db2bd880991535ffa5ce8df722ba3d4aa1a892f73', aiosession=None)>), Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=Tongyi(client=<class 'dashscope.aigc.generation.Generation'>, dashscope_api_key='sk-2a9e44d9621f4fd1b5f9f2c081779215')))>, coroutine=<bound method Chain.arun of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=Tongyi(client=<class 'dashscope.aigc.generation.Generation'>, dashscope_api_key='sk-2a9e44d9621f4fd1b5f9f2c081779215')))>)]\n",
      "Assistant is a large language model trained by OpenAI.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
      "\n",
      "TOOLS:\n",
      "------\n",
      "\n",
      "Assistant has access to the following tools:\n",
      "\n",
      "> Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "> Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "To use a tool, please use the following format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: the action to take, should be one of [Search, Calculator]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "{chat_history}\n",
      "\n",
      "New input: {input}\n",
      "{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Tongyi\n",
    "from langchain.agents import(\n",
    "    load_tools,\n",
    "    initialize_agent,\n",
    "    AgentType\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# 记忆组件\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key = dashscope_api_key,\n",
    "    model = \"Qwen-max\",\n",
    "    temperature = 0,\n",
    ")\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, serpapi_api_key=serpapi_api_key)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    memory=memory # 记忆组件\n",
    ")\n",
    "\n",
    "print(agent)\n",
    "# 看下模版\n",
    "print(agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Hello SouthAki, nice to meet you! I'm Assistant, a language model here to help with any questions or conversations you have. How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello SouthAki, nice to meet you! I'm Assistant, a language model here to help with any questions or conversations you have. How can I assist you today?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"hi, i am SouthAki,what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Search\n",
      "Action Input: 香港 旅游景点\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[{'position': 1, 'block_position': 'top', 'title': '4-Hour Private Hong Kong Layover Tour', 'price': '$527', 'extracted_price': 527, 'link': 'https://www.viator.com/en-us/hong-kong-sar', 'source': 'Viator', 'rating': 5.0, 'reviews': 24, 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT151NAaULmr_pNlSKOzEFK9MOPPSeAej8NskqIVEhZS8Im&s=4', 'extensions': ['Free cancellation']}, {'position': 2, 'block_position': 'top', 'title': '6-Hour Private Hong Kong Layover Tour', 'price': '$792', 'extracted_price': 792, 'link': 'https://www.viator.com/en-us/hong-kong-sar', 'source': 'Viator', 'rating': 5.0, 'reviews': 14, 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRQ1yvAujHf4ttk8ZACiZ72lC8dxkaPZqPMEiTC93gouGpG&s=4', 'extensions': ['Free cancellation']}, {'position': 3, 'block_position': 'top', 'title': 'Victoria Harbour Day or Sunset Cruise', 'price': '$20', 'extracted_price': 20, 'link': 'https://www.getyourguide.com/', 'source': 'GetYourGuide', 'rating': 4.4, 'reviews': 90, 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ3FnrvqM7sLsolWBPOluVaJhybQ7M3BSe9-piPOAoTqRc9&s=4', 'extensions': ['Kid-friendly']}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: 香港有很多好玩的地方！例如，你可以考虑参加一个私人香港过境游，比如4小时或6小时的行程，这将带你游览城市的著名景点。此外，维多利亚港的日间或日落巡游也是一个不错的选择，你可以在游船上欣赏到香港壮观的天际线。这些都是游客们非常喜欢的活动。如果你对具体的景点或者信息有兴趣，可以告诉我，我会提供更多的帮助。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'香港有很多好玩的地方！例如，你可以考虑参加一个私人香港过境游，比如4小时或6小时的行程，这将带你游览城市的著名景点。此外，维多利亚港的日间或日落巡游也是一个不错的选择，你可以在游船上欣赏到香港壮观的天际线。这些都是游客们非常喜欢的活动。如果你对具体的景点或者信息有兴趣，可以告诉我，我会提供更多的帮助。'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"香港有哪里比较好玩的吗?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: 当然，你的名字是SouthAki，所以你的名字的最后三个字母是\"ki\"。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当然，你的名字是SouthAki，所以你的名字的最后三个字母是\"ki\"。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"我还没玩过,你知道我名字后三个字母是什么吗?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Search\n",
      "Action Input: Current temperature in Guangzhou, Guangdong Province, China\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'weather_result', 'temperature': '91', 'unit': 'Fahrenheit', 'precipitation': '7%', 'humidity': '63%', 'wind': '13 mph', 'location': 'Guangzhou, Guangdong Province, China', 'date': 'Sunday 5:00 PM', 'weather': 'Partly cloudy'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: 目前，中国广东省广州市的气温是91华氏度（约32.8摄氏度），天气部分多云，有7%的降水概率，湿度为63%，风速为13英里/小时。请注意，天气情况可能会变化，请随时查看最新的预报。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'目前，中国广东省广州市的气温是91华氏度（约32.8摄氏度），天气部分多云，有7%的降水概率，湿度为63%，风速为13英里/小时。请注意，天气情况可能会变化，请随时查看最新的预报。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"中国广东省广州市现在的气温是多少来着?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: So far, we have asked five questions in this conversation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'So far, we have asked five questions in this conversation.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"截止到现在我们问了多少问题?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chat_convasational_react_description 使用了chatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.agents import(\n",
    "    load_tools,\n",
    "    initialize_agent,\n",
    "    AgentType\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# 记忆组件\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages= True\n",
    ")\n",
    "\n",
    "llm = ChatTongyi(\n",
    "    dashscope_api_key = dashscope_api_key,\n",
    "    model_name = \"qwen-vl-max\",\n",
    "    temperature = 0,\n",
    ")\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, serpapi_api_key=serpapi_api_key)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    memory=memory # 记忆组件\n",
    ")\n",
    "\n",
    "print(agent)\n",
    "# 看下模版\n",
    "print(agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Hi 南秋SouthAki, nice to meet you! I'm Assistant, a language model here to assist you with any questions or discussions. How can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi 南秋SouthAki, nice to meet you! I'm Assistant, a language model here to assist you with any questions or discussions. How can I help you today?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"hi, i am 南秋SouthAki,what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Search\n",
      "Action Input: \"Hong Kong famous foods\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m19 Must-eat Foods in Hong Kong\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: 香港的美食非常丰富多样！以下是一些香港著名的美食，你可以尝试一下：\n",
      "\n",
      "1. 港式早茶：包括点心如虾饺、肠粉和烧卖。\n",
      "2. 烧鹅：香脆的外皮和嫩滑的肉质非常美味。\n",
      "3. 云吞面：鲜美的馄饨搭配弹牙的面条。\n",
      "4. 寿司和刺身：香港有众多优质的日本餐厅。\n",
      "5. 蛇羹：一种传统的汤，由蛇肉和其他配料制成。\n",
      "6. 柠檬茶和鸳鸯：经典的港式饮品。\n",
      "7. 奶茶和菠萝油：奶茶醇厚，菠萝油酥软可口。\n",
      "8. 煲仔饭：在小砂锅中烹煮的米饭，通常配有肉类和蔬菜。\n",
      "9. 西多士：法式吐司蘸蛋液炸制，常配以炼乳或花生酱。\n",
      "10. 冰室的甜品：如芒果糯米糍、杨枝甘露等。\n",
      "\n",
      "这些都是香港非常受欢迎的美食，你一定不能错过。如果你有特别的口味或者想知道更多，也可以告诉我哦！\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'香港的美食非常丰富多样！以下是一些香港著名的美食，你可以尝试一下：\\n\\n1. 港式早茶：包括点心如虾饺、肠粉和烧卖。\\n2. 烧鹅：香脆的外皮和嫩滑的肉质非常美味。\\n3. 云吞面：鲜美的馄饨搭配弹牙的面条。\\n4. 寿司和刺身：香港有众多优质的日本餐厅。\\n5. 蛇羹：一种传统的汤，由蛇肉和其他配料制成。\\n6. 柠檬茶和鸳鸯：经典的港式饮品。\\n7. 奶茶和菠萝油：奶茶醇厚，菠萝油酥软可口。\\n8. 煲仔饭：在小砂锅中烹煮的米饭，通常配有肉类和蔬菜。\\n9. 西多士：法式吐司蘸蛋液炸制，常配以炼乳或花生酱。\\n10. 冰室的甜品：如芒果糯米糍、杨枝甘露等。\\n\\n这些都是香港非常受欢迎的美食，你一定不能错过。如果你有特别的口味或者想知道更多，也可以告诉我哦！'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"香港有什么比较好吃的美食?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Calculator\n",
      "Action Input: Not needed, as the task is not related to calculations.\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Search\n",
      "Action Input: \"current weather Macau\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer:  The current weather in Macau is [insert current weather details here, such as sunny, cloudy, temperature, humidity, etc.]. Please note that this information may not be real-time and it's recommended to check a weather service for the most up-to-date conditions.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: 对不起，我无法提供实时的天气信息，但通常来说，你可以检查天气预报服务来获取澳门当前的准确天气。不过，如果你想知道我是否还记得你名字的最后一个字母，那当然是\"A\"。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'对不起，我无法提供实时的天气信息，但通常来说，你可以检查天气预报服务来获取澳门当前的准确天气。不过，如果你想知道我是否还记得你名字的最后一个字母，那当然是\"A\"。'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"我还没吃过,不过你还知道我名字的最后一个字母是什么吗?中国澳门现在的气温是多少来着?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: In this conversation, we have now asked seven questions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In this conversation, we have now asked seven questions.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"截止到现在我们问了多少问题?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来,我们来讲一下openai_functions 使用到了openai的函数调用来实现的,只支持OpenAI模型\n",
    "所以下面的演示,我直接以代码的形式讲解,不会去运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import(\n",
    "    load_tools,\n",
    "    initialize_agent,\n",
    "    AgentType\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"\"\n",
    "\n",
    "# 记忆组件\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature = 0,\n",
    "    model = 'gpt-4-0613'\n",
    ")\n",
    "tools = load_tools(\n",
    "    [\"serpapi\", \"llm-math\"],\n",
    "    llm = llm\n",
    ")\n",
    "agent = initialize_agent(\n",
    "    tools = tools,\n",
    "    llm = llm,\n",
    "    agent = AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose = True,\n",
    "    memory = memory\n",
    ")\n",
    "print(agent)\n",
    "print(agent.agent.prompt.messages)\n",
    "\n",
    "# 然后后续只需要运行agent.run(input = \"\"),就行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structured_chat_zero_shot_reace_description 对输出的结构化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=ConversationBufferMemory(return_messages=True, memory_key='chat_history') verbose=True tags=['structured-chat-zero-shot-react-description'] agent=StructuredChatAgent(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Respond to the human as helpfully and accurately as possible. You have access to the following tools:\\n\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query., args: {{\\'tool_input\\': {{\\'type\\': \\'string\\'}}}}\\nCalculator: Useful for when you need to answer questions about math., args: {{\\'tool_input\\': {{\\'type\\': \\'string\\'}}}}\\n\\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\n\\nValid \"action\" values: \"Final Answer\" or Search, Calculator\\n\\nProvide only ONE action per $JSON_BLOB, as shown:\\n\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\nFollow this format:\\n\\nQuestion: input question to answer\\nThought: consider previous and subsequent steps\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: action result\\n... (repeat Thought/Action/Observation N times)\\nThought: I know what to respond\\nAction:\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\n```\\n\\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\\nThought:')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template='{input}\\n\\n{agent_scratchpad}'))]), llm=ChatTongyi(client=<class 'dashscope.aigc.multimodal_conversation.MultiModalConversation'>, model_name='qwen-vl-max', dashscope_api_key=SecretStr('**********'))), output_parser=StructuredChatOutputParserWithRetries(output_fixing_parser=OutputFixingParser(parser=StructuredChatOutputParser(), retry_chain=PromptTemplate(input_variables=['completion', 'error', 'instructions'], template='Instructions:\\n--------------\\n{instructions}\\n--------------\\nCompletion:\\n--------------\\n{completion}\\n--------------\\n\\nAbove, the Completion did not satisfy the constraints given in the Instructions.\\nError:\\n--------------\\n{error}\\n--------------\\n\\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:')\n",
      "| ChatTongyi(client=<class 'dashscope.aigc.multimodal_conversation.MultiModalConversation'>, model_name='qwen-vl-max', dashscope_api_key=SecretStr('**********')))), allowed_tools=['Search', 'Calculator']) tools=[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='ebcc8130fff3a121b7029d6db2bd880991535ffa5ce8df722ba3d4aa1a892f73', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='ebcc8130fff3a121b7029d6db2bd880991535ffa5ce8df722ba3d4aa1a892f73', aiosession=None)>), Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatTongyi(client=<class 'dashscope.aigc.multimodal_conversation.MultiModalConversation'>, model_name='qwen-vl-max', dashscope_api_key=SecretStr('**********'))))>, coroutine=<bound method Chain.arun of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatTongyi(client=<class 'dashscope.aigc.multimodal_conversation.MultiModalConversation'>, model_name='qwen-vl-max', dashscope_api_key=SecretStr('**********'))))>)]\n",
      "Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n",
      "\n",
      "Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query., args: {{'tool_input': {{'type': 'string'}}}}\n",
      "Calculator: Useful for when you need to answer questions about math., args: {{'tool_input': {{'type': 'string'}}}}\n",
      "\n",
      "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
      "\n",
      "Valid \"action\" values: \"Final Answer\" or Search, Calculator\n",
      "\n",
      "Provide only ONE action per $JSON_BLOB, as shown:\n",
      "\n",
      "```\n",
      "{{\n",
      "  \"action\": $TOOL_NAME,\n",
      "  \"action_input\": $INPUT\n",
      "}}\n",
      "```\n",
      "\n",
      "Follow this format:\n",
      "\n",
      "Question: input question to answer\n",
      "Thought: consider previous and subsequent steps\n",
      "Action:\n",
      "```\n",
      "$JSON_BLOB\n",
      "```\n",
      "Observation: action result\n",
      "... (repeat Thought/Action/Observation N times)\n",
      "Thought: I know what to respond\n",
      "Action:\n",
      "```\n",
      "{{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Final response to human\"\n",
      "}}\n",
      "```\n",
      "\n",
      "Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\n",
      "Thought:\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.agents import(\n",
    "    load_tools,\n",
    "    initialize_agent,\n",
    "    AgentType\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# 记忆组件\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages= True\n",
    ")\n",
    "\n",
    "llm = ChatTongyi(\n",
    "    dashscope_api_key = dashscope_api_key,\n",
    "    model_name = \"qwen-vl-max\",\n",
    "    temperature = 0,\n",
    ")\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, serpapi_api_key=serpapi_api_key)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    memory=memory # 记忆组件\n",
    ")\n",
    "\n",
    "print(agent)\n",
    "# 看下模版\n",
    "print(agent.agent.llm_chain.prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看上面的提示词模版基本上是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I can use the search engine to find information on LangChain.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"What is LangChain?\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[\"LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\", 'LangChain entity_type: kp3_verticals.', 'LangChain kgmid: /g/11kjpl7_60.', 'LangChain initial_release_date: October 2022.', 'LangChain developer_s: Harrison Chase.', 'LangChain license: MIT License.', 'LangChain repository: github.com/langchain-ai/langchain.', 'LangChain stable_release: 0.1.16 / 11 April 2024; 2 months ago.', 'LangChain written_in: Python and JavaScript.', 'LangChain provides AI developers with tools to connect language models with external data sources. It is open-source and supported by an active community.', 'LangChain is a framework to build with LLMs by chaining interoperable components. LangGraph is the framework for building controllable agentic workflows.', \"LangChain uses the power of AI large language models combined with data sources to create quite powerful apps. Read how it works and how it's used.\", 'LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models.', 'LangChain is a framework for developing applications powered by large language models (LLMs). LangChain simplifies every stage of the LLM application lifecycle:.', 'LangChain is a sophisticated framework comprising several key components that work in synergy to enhance natural language processing tasks.', 'Langchain makes creating agents using large language models simple through their agents API. Developers can use OpenAI functions or other means ...', 'LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs).', 'LangChain is a framework for developing applications powered by large language models (LLMs). For these applications, LangChain simplifies the entire ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI know what to respond\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It simplifies every stage of the LLM application lifecycle and includes a library of abstractions for Python and JavaScript, representing common steps and concepts necessary to work with language models.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is langchain?',\n",
       " 'chat_history': [HumanMessage(content='what is langchain?'),\n",
       "  AIMessage(content='LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It simplifies every stage of the LLM application lifecycle and includes a library of abstractions for Python and JavaScript, representing common steps and concepts necessary to work with language models.')],\n",
       " 'output': 'LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It simplifies every stage of the LLM application lifecycle and includes a library of abstractions for Python and JavaScript, representing common steps and concepts necessary to work with language models.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"input\":\"what is langchain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将memory插入提示词模版里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='ebcc8130fff3a121b7029d6db2bd880991535ffa5ce8df722ba3d4aa1a892f73', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='ebcc8130fff3a121b7029d6db2bd880991535ffa5ce8df722ba3d4aa1a892f73', aiosession=None)>), Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatTongyi(client=<class 'dashscope.aigc.multimodal_conversation.MultiModalConversation'>, model_name='qwen-vl-max', dashscope_api_key=SecretStr('**********'))))>, coroutine=<bound method Chain.arun of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatTongyi(client=<class 'dashscope.aigc.multimodal_conversation.MultiModalConversation'>, model_name='qwen-vl-max', dashscope_api_key=SecretStr('**********'))))>)]\n"
     ]
    }
   ],
   "source": [
    "# 导入模块\n",
    "from langchain.agents import (\n",
    "    load_agent,\n",
    "    initialize_agent,\n",
    "    AgentType,\n",
    "    Tool,\n",
    "    load_tools\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.chains import LLMMathChain\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# 定义大模型\n",
    "llm = ChatTongyi(\n",
    "    model_name= 'qwen-vl-max',\n",
    "    temperature= 0,\n",
    "    dashscope_api_key = dashscope_api_key\n",
    ")\n",
    "# 加载工具\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, serpapi_api_key=serpapi_api_key)\n",
    "print(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加memory组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent = AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True,\n",
    "    memory = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印一下这个agent_chain是什么内容看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=ConversationBufferMemory(return_messages=True, memory_key='chat_history') verbose=True tags=['chat-conversational-react-description'] agent=ConversationalChatAgent(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\n> Calculator: Useful for when you need to answer questions about math.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of Search, Calculator\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]), llm=ChatTongyi(client=<class 'dashscope.aigc.multimodal_conversation.MultiModalConversation'>, model_name='qwen-vl-max', dashscope_api_key=SecretStr('**********'))), output_parser=ConvoOutputParser(), allowed_tools=['Search', 'Calculator'], template_tool_response=\"TOOL RESPONSE: \\n---------------------\\n{observation}\\n\\nUSER'S INPUT\\n--------------------\\n\\nOkay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else.\") tools=[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='ebcc8130fff3a121b7029d6db2bd880991535ffa5ce8df722ba3d4aa1a892f73', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='ebcc8130fff3a121b7029d6db2bd880991535ffa5ce8df722ba3d4aa1a892f73', aiosession=None)>), Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatTongyi(client=<class 'dashscope.aigc.multimodal_conversation.MultiModalConversation'>, model_name='qwen-vl-max', dashscope_api_key=SecretStr('**********'))))>, coroutine=<bound method Chain.arun of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatTongyi(client=<class 'dashscope.aigc.multimodal_conversation.MultiModalConversation'>, model_name='qwen-vl-max', dashscope_api_key=SecretStr('**********'))))>)]\n"
     ]
    }
   ],
   "source": [
    "print(agent_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如我们想看到模版的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Respond to the human as helpfully and accurately as possible. You have access to the following tools:\\n\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query., args: {{\\'tool_input\\': {{\\'type\\': \\'string\\'}}}}\\nCalculator: Useful for when you need to answer questions about math., args: {{\\'tool_input\\': {{\\'type\\': \\'string\\'}}}}\\n\\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\n\\nValid \"action\" values: \"Final Answer\" or Search, Calculator\\n\\nProvide only ONE action per $JSON_BLOB, as shown:\\n\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\nFollow this format:\\n\\nQuestion: input question to answer\\nThought: consider previous and subsequent steps\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: action result\\n... (repeat Thought/Action/Observation N times)\\nThought: I know what to respond\\nAction:\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\n```\\n\\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\\nThought:')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template='{input}\\n\\n{agent_scratchpad}'))]\n"
     ]
    }
   ],
   "source": [
    "print(agent_chain.agent.llm_chain.prompt.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I know what to respond\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Hello SouthAki, how can I assist you today?\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: 我需要使用搜索工具来了解广播剧的相关信息。\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"What is a radio drama?\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Radio drama is a dramatized, purely acoustic performance. With no visual component, radio drama depends on dialogue, music and sound effects to help the listener imagine the characters and story: \"It is auditory in the physical dimension but equally powerful as a visual force in the psychological dimension.\"', 'Radio drama entity_type: kp3_verticals.', 'Radio drama kgmid: /m/0f9vg.', 'Radio drama is a dramatized, purely acoustic performance. With no visual component, radio drama depends on dialogue, music and sound effects to help the ...', 'Radio drama (or audio drama, audio play, radio play, radio theatre) is a dramatised, purely acoustic performance, broadcast on radio or published on audio ...', 'The basic premise is that the actors you see on stage are “radio actors” who portray the characters in each story using their voice and facial ...', \"Radio plays are dramatized, purely auditory performances that are specifically designed to focus on painting a picture in the listeners mind's ...\", 'Radio plays, or radio dramas, are dramatized, acoustic-only performances. They are typically listened to in the ear, rather than watched upon a stage, as they ...', 'In 1921, the KDKA radio station in Pittsburgh aired a brief sketch called A Rural Line on Education, becoming the first “play” written specifically for radio ...', 'An audio drama is a fictional story presented through an audio-only medium, performed for the listener. An audio book is generally a written ...', 'Radio drama (or audio drama, audio play, radio play, radio theatre, or audio theatre) is a dramatized, purely acoustic performance without visual elements.', 'Audio Theater – Term used for multi-cast narrative stories that follow a dramatic script, uses narration, sound FX libraries, and music.']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI can now understand what a radio drama is.\n",
      "\n",
      "Thought: I can give my response.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Yes, I enjoy listening to radio dramas too! They're a great way to get lost in a story without needing to rely on visuals.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: 我知道答案。\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"你的名字是无法确定的，因为我是作为一个人工智能助手被调用的。你可能需要询问该用户的姓名。\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你的名字是无法确定的，因为我是作为一个人工智能助手被调用的。你可能需要询问该用户的姓名。'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\"我叫SouthAki\")\n",
    "agent_chain.run(\"我喜欢听广播剧,你喜欢听吗?\")\n",
    "agent_chain.run(\"我叫什么名字?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到我们的memory没有起作用,所以这里要加入一个参数\n",
    "agent_kwargs传递参数,把memory key传入到提示词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent = AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True,\n",
    "    agent_kwargs={\n",
    "        \"extra_prom_messages\":[\n",
    "            MessagesPlaceholder(variable_name= \"chat_history\"),\n",
    "            MessagesPlaceholder(variable_name= \"agent_scratchpad\")\n",
    "        ]\n",
    "    },\n",
    "    memory = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I know what to respond\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Hello SouthAki, how can I assist you today?\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: 我需要使用搜索工具来了解广播剧的相关信息。\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"What is a radio drama?\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Radio drama is a dramatized, purely acoustic performance. With no visual component, radio drama depends on dialogue, music and sound effects to help the listener imagine the characters and story: \"It is auditory in the physical dimension but equally powerful as a visual force in the psychological dimension.\"', 'Radio drama entity_type: kp3_verticals.', 'Radio drama kgmid: /m/0f9vg.', 'Radio drama is a dramatized, purely acoustic performance. With no visual component, radio drama depends on dialogue, music and sound effects to help the ...', 'Radio drama (or audio drama, audio play, radio play, radio theatre) is a dramatised, purely acoustic performance, broadcast on radio or published on audio ...', 'The basic premise is that the actors you see on stage are “radio actors” who portray the characters in each story using their voice and facial ...', \"Radio plays are dramatized, purely auditory performances that are specifically designed to focus on painting a picture in the listeners mind's ...\", 'Radio plays, or radio dramas, are dramatized, acoustic-only performances. They are typically listened to in the ear, rather than watched upon a stage, as they ...', 'In 1921, the KDKA radio station in Pittsburgh aired a brief sketch called A Rural Line on Education, becoming the first “play” written specifically for radio ...', 'An audio drama is a fictional story presented through an audio-only medium, performed for the listener. An audio book is generally a written ...', 'Radio drama (or audio drama, audio play, radio play, radio theatre, or audio theatre) is a dramatized, purely acoustic performance without visual elements.', 'Audio Theater – Term used for multi-cast narrative stories that follow a dramatic script, uses narration, sound FX libraries, and music.']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI can now understand what a radio drama is.\n",
      "\n",
      "Thought: I can give my response.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Yes, I enjoy listening to radio dramas too! They're a great way to get lost in a story without needing to rely on visuals.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: 我知道答案。\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"你的名字是无法确定的，因为我是作为一个人工智能助手被调用的。你可能需要询问该用户的姓名。\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你的名字是无法确定的，因为我是作为一个人工智能助手被调用的。你可能需要询问该用户的姓名。'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\"我叫SouthAki\")\n",
    "agent_chain.run(\"我喜欢听广播剧,你喜欢听吗?\")\n",
    "agent_chain.run(\"我叫什么名字?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在agent和tools之间共享记忆\n",
    "- 自定义一个工具用来LLMChain来总结内容\n",
    "- 使用readonlymemory来共享记忆\n",
    "- 观察共享和不共享的区别\n",
    "我们来实现一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# 导入模块\n",
    "from langchain.agents import (\n",
    "    load_agent,\n",
    "    initialize_agent,\n",
    "    AgentType,\n",
    "    Tool\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory,ReadOnlySharedMemory\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.chains import LLMChain\n",
    "from  langchain.prompts import PromptTemplate,MessagesPlaceholder\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "# 创建llm\n",
    "llm= Tongyi(\n",
    "    dashscope_api_key= dashscope_api_key,\n",
    "    model= \"Qwen-max\",\n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一条链来总结对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# 问题\n",
    "template= \"\"\"\n",
    "    以下是一段AI机器人和人类的对话:\n",
    "    {chat_history}\n",
    "    根据输入和上面的对话记录写一份对话总结.\n",
    "    输入:{input}\n",
    "\"\"\"\n",
    "# 提示词模版\n",
    "prompt = PromptTemplate(\n",
    "    # 输入验证器\n",
    "    input_variables=[\"chat_history\", \"input\"],\n",
    "    template=template,\n",
    ")\n",
    "# 创建记忆\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages= True    \n",
    ")\n",
    "\n",
    "# 关键一步,如何让我们的Agent和Tool共享记忆\n",
    "readonlymemory = ReadOnlySharedMemory(memory= memory)\n",
    "\n",
    "# 构建chain\n",
    "summary_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt= prompt,\n",
    "    verbose= True,\n",
    "    memory= readonlymemory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来构建工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool(name='Search', description='当需要了解实时信息或者你不知道的事情的时候才可以使用搜索工具', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google'}, serpapi_api_key='ebcc8130fff3a121b7029d6db2bd880991535ffa5ce8df722ba3d4aa1a892f73', aiosession=None)>), Tool(name='Summary', description='当你被要求总结一段对话的时候才可以使用这个工具,工具输入必须为字符串,只在必要的时候使用', func=<function SummaryChainFun at 0x0000020DF5BFE3E0>)]\n"
     ]
    }
   ],
   "source": [
    "# 搜索工具\n",
    "search = SerpAPIWrapper(\n",
    "    serpapi_api_key= serpapi_api_key,\n",
    "    params={\n",
    "        \"engine\": \"google\"\n",
    "    }\n",
    ")\n",
    "# 总结工具(自定义)\n",
    "def SummaryChainFun(history):\n",
    "    print(\"\\n====================总结chain开始运行==========================\")\n",
    "    print(\"输入历史:\",history)\n",
    "    summary_chain.run(history)\n",
    "\n",
    "tools= [\n",
    "    Tool(\n",
    "        name= \"Search\",\n",
    "        func= search.run,\n",
    "        description= \"当需要了解实时信息或者你不知道的事情的时候才可以使用搜索工具\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name= \"Summary\",\n",
    "        func= SummaryChainFun,\n",
    "        description= \"当你被要求总结一段对话的时候才可以使用这个工具,工具输入必须为字符串,只在必要的时候使用\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 我们看下内容\n",
    "print(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来,我们创建Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Search(query: str, **kwargs: Any) -> str - 当需要了解实时信息或者你不知道的事情的时候才可以使用搜索工具\n",
      "Summary(history) - 当你被要求总结一段对话的时候才可以使用这个工具,工具输入必须为字符串,只在必要的时候使用\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Search, Summary]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    memory = memory\n",
    ")\n",
    "# 验证一下\n",
    "print(agent_chain.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok,这里可以看到我们的模版,但是不知道大家有没有发现,就是上面的内容中,并没有我们关于记忆的部分,也就是我们定义的chat_history\n",
    "那么怎么办呢\n",
    "这里可以看一下上面的部分,就是上面部分我们可以分成三个部分\n",
    "分别是: 前缀,内容,后缀\n",
    "其中前缀后缀都是可以由我们自定义,那么我们可以修改上面的前后缀,加入我们的记忆chat_history,来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\n",
      "    Search: Use this tool to search for information.\n",
      "    Summary: Use this tool to summarize conversations.\n",
      "\n",
      "\n",
      "Search(query: str, **kwargs: Any) -> str - 当需要了解实时信息或者你不知道的事情的时候才可以使用搜索工具\n",
      "Summary(history) - 当你被要求总结一段对话的时候才可以使用这个工具,工具输入必须为字符串,只在必要的时候使用\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Search, Summary]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "\n",
      "    Begin!\"\n",
      "    {chat_history}\n",
      "    Question: {input}\n",
      "    {agent_scratchpad}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 修改代码\n",
    "prefix = \"\"\"\n",
    "    Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\n",
    "    Search: Use this tool to search for information.\n",
    "    Summary: Use this tool to summarize conversations.\n",
    "\"\"\"\n",
    "suffix = \"\"\"\n",
    "    Begin!\"\n",
    "    {chat_history}\n",
    "    Question: {input}\n",
    "    {agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors= True,\n",
    "    agent_kwargs={\n",
    "        \"prefix\": prefix,\n",
    "        \"suffix\": suffix,\n",
    "        \"agent_scratchpad\": MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "        \"chat_history\": MessagesPlaceholder(\"chat_history\"),\n",
    "        \"input\": MessagesPlaceholder(\"input\")\n",
    "    },\n",
    "    memory = memory\n",
    ")\n",
    "# 来看下我们修改后的结果\n",
    "print(agent_chain.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok,我们看到我们修改了前后缀已经成功了,来试试看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: 这是一个关于历史知识的问题，不需要使用搜索工具。\n",
      "Final Answer: 美国的第45任总统是唐纳德·特朗普（Donald Trump）。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'美国的第45任总统是唐纳德·特朗普（Donald Trump）。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input = \"美国第45任总统是谁?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: 我知道答案，不需要使用工具。\n",
      "Final Answer: 唐纳德·特朗普的夫人叫梅拉尼娅·特朗普（Melania Trump）。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'唐纳德·特朗普的夫人叫梅拉尼娅·特朗普（Melania Trump）。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input = \"他的夫人叫什么名字?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='美国第45任总统是谁?'), AIMessage(content='美国的第45任总统是唐纳德·特朗普（Donald Trump）。'), HumanMessage(content='他的夫人叫什么名字?'), AIMessage(content='唐纳德·特朗普的夫人叫梅拉尼娅·特朗普（Melania Trump）。')]\n"
     ]
    }
   ],
   "source": [
    "# 我们看下对话的记忆的\n",
    "print(agent_chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: 我应该使用Summary工具来总结我们的对话内容。\n",
      "Action: Summary\n",
      "Action Input: \"[HumanMessage(content='美国第45任总统是谁?'), AIMessage(content='美国的第45任总统是唐纳德·特朗普（Donald Trump）。'), HumanMessage(content='他的夫人叫什么名字?'), AIMessage(content='唐纳德·特朗普的夫人叫梅拉尼娅·特朗普（Melania Trump）。')]\"\n",
      "\u001b[0m\n",
      "====================总结chain开始运行==========================\n",
      "输入历史: [HumanMessage(content='美国第45任总统是谁?'), AIMessage(content='美国的第45任总统是唐纳德·特朗普（Donald Trump）。'), HumanMessage(content='他的夫人叫什么名字?'), AIMessage(content='唐纳德·特朗普的夫人叫梅拉尼娅·特朗普（Melania Trump）。')]\"\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    以下是一段AI机器人和人类的对话:\n",
      "    [HumanMessage(content='美国第45任总统是谁?'), AIMessage(content='美国的第45任总统是唐纳德·特朗普（Donald Trump）。'), HumanMessage(content='他的夫人叫什么名字?'), AIMessage(content='唐纳德·特朗普的夫人叫梅拉尼娅·特朗普（Melania Trump）。')]\n",
      "    根据输入和上面的对话记录写一份对话总结.\n",
      "    输入:[HumanMessage(content='美国第45任总统是谁?'), AIMessage(content='美国的第45任总统是唐纳德·特朗普（Donald Trump）。'), HumanMessage(content='他的夫人叫什么名字?'), AIMessage(content='唐纳德·特朗普的夫人叫梅拉尼娅·特朗普（Melania Trump）。')]\"\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mNone\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m 我现在可以作总结了。\n",
      "Final Answer: 我们聊到了美国的第45任总统是唐纳德·特朗普，以及他的夫人名叫梅拉尼娅·特朗普。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我们聊到了美国的第45任总统是唐纳德·特朗普，以及他的夫人名叫梅拉尼娅·特朗普。'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input= \"我们都聊了什么?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: 需要搜索相关信息来获取答案。\n",
      "Action: Search\n",
      "Action Input: 中国最新南极科考站 开站时间\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['【环球时报报道记者樊巍刘扬】2月7日，中国南极秦岭站正式开站，这是继长城站、中山站、昆仑站、泰山站之后，我国建设的第五座南极考察站，也是第三座常年 ...', '中国在南极的科考站从无到有，长城站、中山站、昆仑站、泰山站，已先后相继矗立在南极陆地或南极冰盖上，而极地科学家期盼已久的秦岭站也终于在第40次科考 ...', '自然资源部今天宣布我国南极秦岭站正式开站这是我国在南极建设的第五个科学考察站我国第五座南极科考站今天正式开站秦岭站是我国在南极继长城站、中山 ...', '【侨报网综合讯】中国第五个南极考察站秦岭站于7日正式开站。中国国家海洋局极地考察办公室副主任龙威介绍，秦岭是横贯中国中部的古老山脉，是地理上 ...', '央视网消息：今天（1月31日），正在建设中的我国第五座南极科考站——罗斯海新站的停机坪正式开工建设。来看总台央视记者从罗斯海发回的报道。', '世界尽头南极，又添了一个新地标。2月7日，位于西南极罗斯海恩克斯堡岛的中国秦岭站开站。这是中国第五个南极科考站，第三个南极常年考察站，也是我国首个 ...', '作为新时代我国建立的第1个常年科考站，中国南极秦岭站填补了我国在南极罗斯海区域的考察空白。作为在秦岭站里中国科考队的“得力助手”，联想ThinkPad将 ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m 我现在知道中国最新的南极科考站是秦岭站，它在2023年2月7日正式开站。\n",
      "Final Answer: 中国最新的南极科考站是秦岭站，该站在2023年2月7日开站并展开科学考察活动。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'中国最新的南极科考站是秦岭站，该站在2023年2月7日开站并展开科学考察活动。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input= \"中国最新的南极科考站是什么?哪一年开站展开科学考察?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: 我需要总结当前的对话内容，使其适合一个5岁孩子理解。\n",
      "Action: Summary\n",
      "Action Input: 我们谈论了美国的第45任总统是唐纳德·特朗普，他的妻子叫梅拉尼娅·特朗普。还有，中国有一个新的南极科考站叫秦岭站，它在2023年开始工作。\n",
      "\u001b[0m\n",
      "====================总结chain开始运行==========================\n",
      "输入历史: 我们谈论了美国的第45任总统是唐纳德·特朗普，他的妻子叫梅拉尼娅·特朗普。还有，中国有一个新的南极科考站叫秦岭站，它在2023年开始工作。\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    以下是一段AI机器人和人类的对话:\n",
      "    [HumanMessage(content='美国第45任总统是谁?'), AIMessage(content='美国的第45任总统是唐纳德·特朗普（Donald Trump）。'), HumanMessage(content='他的夫人叫什么名字?'), AIMessage(content='唐纳德·特朗普的夫人叫梅拉尼娅·特朗普（Melania Trump）。'), HumanMessage(content='我们都聊了什么?'), AIMessage(content='我们聊到了美国的第45任总统是唐纳德·特朗普，以及他的夫人名叫梅拉尼娅·特朗普。'), HumanMessage(content='中国最新的南极科考站是什么?哪一年开站展开科学考察?'), AIMessage(content='中国最新的南极科考站是秦岭站，该站在2023年2月7日开站并展开科学考察活动。')]\n",
      "    根据输入和上面的对话记录写一份对话总结.\n",
      "    输入:我们谈论了美国的第45任总统是唐纳德·特朗普，他的妻子叫梅拉尼娅·特朗普。还有，中国有一个新的南极科考站叫秦岭站，它在2023年开始工作。\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mNone\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m 我现在可以给出总结了，内容已经简化适合5岁的孩子理解\n",
      "Final Answer: 我们聊到美国有一位叫唐纳德·特朗普的先生，他是第45任总统，他的太太叫梅拉尼娅。还有一个叫秦岭的新冰冻土地站，它在中国的南极，2023年开始科学家在那里做研究哦。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我们聊到美国有一位叫唐纳德·特朗普的先生，他是第45任总统，他的太太叫梅拉尼娅。还有一个叫秦岭的新冰冻土地站，它在中国的南极，2023年开始科学家在那里做研究哦。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input=\"帮我总结下目前的对话内容，给我5岁的儿子看看\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tool与toolKit介绍\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何使用我们的Tool\n",
    "下面是我们的模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加预制工具的方法很简单\n",
    "from langchain.agents import load_tools\n",
    "tool_names = [...]\n",
    "tools = load_tools(tool_names) #使用load方法\n",
    "#有些tool需要单独设置llm\n",
    "from langchain.agents import load_tools\n",
    "tool_names = [...]\n",
    "llm = ...\n",
    "tools = load_tools(tool_names, llm=llm) #在load的时候指定llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来会讲几个常用的tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个是serp Api\n",
    "最常见的聚合搜索引擎 https://serper.dev/dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Charles III'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入模块\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "import os\n",
    "search = SerpAPIWrapper(\n",
    "    serpapi_api_key=serpapi_api_key\n",
    ")\n",
    "search.run(\"加拿大女王是谁?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持自定义参数，比如将引擎切换到bing，设置搜索语言等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"engine\": \"bing\",\n",
    "    \"gl\": \"us\",\n",
    "    \"hl\": \"en\",\n",
    "}\n",
    "search = SerpAPIWrapper(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\'Barack Hussein Obama II is an American politician who served as the 44th president of the United States from 2009 to 2017. As a member of the Democratic Party, he was the first African-American presi…New content will be added above the current area of focus upon selectionBarack Hussein Obama II is an American politician who served as the 44th president of the United States from 2009 to 2017. As a member of the Democratic Party, he was the first African-American president in U.S. history. Obama previously served as a U.S. senator representing Illinois from 2005 to 2008, and as an Illinois state senator from 1997 to 2004.Wikipediabarackobama.com\\', \\'Barack Obama type: 44th President of the USA.\\', \\'In 1975, when Obama started high school in Hawaii, teacher Eric Kusunoki read the roll call and stumbled on Obama\\\\\\'s first name. \"Is Barack here?\" he asked, pronouncing it BAR-rack .\\', \"Barack Obama, the 44th president of the United States, was born on August 4, 1961, in Honolulu, Hawaii to Barack Obama, Sr. (1936–1982) (born in Oriang\\' Kogelo of Rachuonyo North District, Kenya) and Stanley Ann Dunham, known as Ann (1942–1995) (born in Wichita, Kansas, United States). Obama spent most of his childhood years in Honolulu, where his mother atten…\", \\'The president of the United States is the head of state and …\\', \\'Barack Obama (born August 4, 1961, Honolulu, Hawaii, U.S.) is the 44th president of the United States (2009–17) and the first African American to hold the office. …\\', \\'US President Barack Obama\\\\\\'s birth certificate, published on Wednesday, names him as \"Barack Hussein Obama, II\". Why does he qualify for the Roman numerals?\\', \\'Barack Obama was the 44th president of the United States (2009-2017) and the first African American to be elected to that office. Obama was born in Hawaii, studied …\\', \\'Obama’s father, Barack Sr., a Kenyan economist, met his mother, Stanley Ann Dunham, when both were students in Hawaii, where Barack was born on August 4, 1961.\\', \\'On November 4, 2008, Obama became the first African-American to be elected President. He resigned his seat in the U.S. Senate on November 16, 2008. Barack Obama was inaugurated as the 44th President of the …\\', \\'Barack Hussein Obama II (born August 4, 1961) is an American politician who served as the 44th president of the United States from 2009 to 2017. As a member of the …\\', \"But it also shone a light on a rarely used part of his name, the II at the end. The reason for it is, at root, quite simple - the US president has been given his father\\'s name, Barack Hussein...\"]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.run(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Dall-E\n",
    "Dall-E是OpenAI出品的文到图AI大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 来实现看看\n",
    "# 首先需要安装包\n",
    "! pip install opencv-python scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里可能需要用到OpenAI的,但是后面我会尝试使用通义\n",
    "# 先展示一下就是OpenAI的写法\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    # 这里要注意,就是使用的模型一定要是最新的\n",
    "    model= \"gpt-4\"\n",
    ")\n",
    "\n",
    "from langchain.agents import(\n",
    "    initialize_agent,\n",
    "    load_tools\n",
    ")\n",
    "\n",
    "tools = load_tools([\"dalle-image-generator\"])\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")\n",
    "output = agent.run(\"Generate an image of a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for DallEAPIWrapper\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m\n\u001b[0;32m      9\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatTongyi(\n\u001b[0;32m     10\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen-vl-max\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     12\u001b[0m     dashscope_api_key \u001b[38;5;241m=\u001b[39m dashscope_api_key\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_agent, load_tools\n\u001b[1;32m---> 16\u001b[0m tools \u001b[38;5;241m=\u001b[39m \u001b[43mload_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdalle-image-generator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m agent \u001b[38;5;241m=\u001b[39m initialize_agent(\n\u001b[0;32m     18\u001b[0m     tools,\n\u001b[0;32m     19\u001b[0m     llm,\n\u001b[0;32m     20\u001b[0m     agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero-shot-react-description\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m output \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate an image of a halloween night at a haunted museum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\agent_toolkits\\load_tools.py:726\u001b[0m, in \u001b[0;36mload_tools\u001b[1;34m(tool_names, llm, callbacks, allow_dangerous_tools, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m     _get_tool_func, extra_keys \u001b[38;5;241m=\u001b[39m _EXTRA_OPTIONAL_TOOLS[name]\n\u001b[0;32m    725\u001b[0m     sub_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m extra_keys \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs}\n\u001b[1;32m--> 726\u001b[0m     tool \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tool_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msub_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    727\u001b[0m     tools\u001b[38;5;241m.\u001b[39mappend(tool)\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\agent_toolkits\\load_tools.py:392\u001b[0m, in \u001b[0;36m_get_dalle_image_generator\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_dalle_image_generator\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tool:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tool(\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDall-E-Image-Generator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 392\u001b[0m         \u001b[43mDallEAPIWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrun,\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA wrapper around OpenAI DALL-E API. Useful for when you need to generate images from a text description. Input should be an image description.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    394\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for DallEAPIWrapper\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "# 转译Tongyi\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "llm = ChatTongyi(\n",
    "    model_name = \"qwen-vl-max\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key = dashscope_api_key\n",
    ")\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "\n",
    "tools = load_tools([\"dalle-image-generator\"])\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")\n",
    "output = agent.run(\"Create an image of a halloween night at a haunted museum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看来是不可以的,没关系,跳过就行,后续补充 to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eleven Labs Text2Speech\n",
    "ElevenLabs 是非常优秀的TTS合成API\n",
    "下面需要单独安装一些包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install elevenlabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eleven_labs_text2speech'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "eleven_api_key = os.getenv(\"ELEVEN_API_KEY\")\n",
    "\n",
    "# 工具使用\n",
    "from langchain.tools import ElevenLabsText2SpeechTool\n",
    "import elevenlabs\n",
    "elevenlabs.api_key = eleven_api_key\n",
    "text_to_speak = \"Hello! 你好! Hola! नमस्ते! Bonjour! こんにちは! مرحبا! 안녕하세요! Ciao! Cześć! Привіт! வணக்கம்!\"\n",
    "tts = ElevenLabsText2SpeechTool(\n",
    "    voice = \"Bella\",\n",
    "    text_to_speak = text_to_speak,\n",
    "    verbose = True\n",
    ")\n",
    "# 测试是否可用\n",
    "tts.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试播放,先缓存在我们本地上吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error while running ElevenLabsText2SpeechTool: Unusual activity detected. Free Tier usage disabled. If you are using a proxy/VPN you might need to purchase a Paid Plan to not trigger our abuse detectors. Free Tier only works if users do not abuse it, for example by creating multiple free accounts. If we notice that many people try to abuse it, we will need to reconsider Free Tier altogether. \nPlease play fair and purchase any Paid Subscription to continue.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\tools\\eleven_labs\\text2speech.py:58\u001b[0m, in \u001b[0;36mElevenLabsText2SpeechTool._run\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     speech \u001b[38;5;241m=\u001b[39m \u001b[43melevenlabs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(\n\u001b[0;32m     60\u001b[0m         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbx\u001b[39m\u001b[38;5;124m\"\u001b[39m, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, delete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\elevenlabs\\simple.py:73\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(text, api_key, voice, model, stream, latency, stream_chunk_size)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\elevenlabs\\api\\tts.py:45\u001b[0m, in \u001b[0;36mTTS.generate\u001b[1;34m(text, voice, model, api_key)\u001b[0m\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     41\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m     42\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[0;32m     43\u001b[0m     voice_settings\u001b[38;5;241m=\u001b[39mvoice\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mmodel_dump() \u001b[38;5;28;01mif\u001b[39;00m voice\u001b[38;5;241m.\u001b[39msettings \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\elevenlabs\\api\\base.py:75\u001b[0m, in \u001b[0;36mAPI.post\u001b[1;34m(url, *args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\elevenlabs\\api\\base.py:67\u001b[0m, in \u001b[0;36mAPI.request\u001b[1;34m(url, method, api_key, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AuthorizationError(message)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIError(message, status)\n",
      "\u001b[1;31mAPIError\u001b[0m: Unusual activity detected. Free Tier usage disabled. If you are using a proxy/VPN you might need to purchase a Paid Plan to not trigger our abuse detectors. Free Tier only works if users do not abuse it, for example by creating multiple free accounts. If we notice that many people try to abuse it, we will need to reconsider Free Tier altogether. \nPlease play fair and purchase any Paid Subscription to continue.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# speech_file = tts.run(text_to_speak)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tts\u001b[38;5;241m.\u001b[39mplay(\u001b[43mtts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_to_speak\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\tools.py:452\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(observation, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\tools.py:409\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[0;32m    407\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    408\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 409\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run, \u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[0;32m    414\u001b[0m     )\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\tools\\eleven_labs\\text2speech.py:65\u001b[0m, in \u001b[0;36mElevenLabsText2SpeechTool._run\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while running ElevenLabsText2SpeechTool: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error while running ElevenLabsText2SpeechTool: Unusual activity detected. Free Tier usage disabled. If you are using a proxy/VPN you might need to purchase a Paid Plan to not trigger our abuse detectors. Free Tier only works if users do not abuse it, for example by creating multiple free accounts. If we notice that many people try to abuse it, we will need to reconsider Free Tier altogether. \nPlease play fair and purchase any Paid Subscription to continue."
     ]
    }
   ],
   "source": [
    "# 缓存到本地\n",
    "speech_file = tts.run(text_to_speak)\n",
    "# 等到加载完再执行播放\n",
    "tts.play(tts.run(text_to_speak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 边生成边播放\n",
    "tts.stream_speech(text_to_speak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来,我们来实现新的一个\n",
    "GraphQL\n",
    "一种appi查询语言,类似SQL,用来查询奈飞的数据库,查找一下和星球大战相关的电影,API地址\n",
    "首先要install几个包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install httpx gql > /dev/null\n",
    "! pip install gql\n",
    "! pip install requests_toolbelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m我会回答。\n",
      "Final Answer: 以下是所有星球大战电影的标题：\n",
      "\n",
      "1.《星球大战：新希望》（Star Wars: A New Hope）\n",
      "\n",
      "2.《星球大战：帝国反击战》（Star Wars: The Empire Strikes Back）\n",
      "\n",
      "3.《星球大战：绝地归来》（Star Wars: Return of the Jedi）\n",
      "\n",
      "4.《星球大战前传一：魅影危机》（Star Wars: The Phantom Menace）\n",
      "\n",
      "5.《星球大战前传二：克隆人的进攻》（Star Wars: Attack of the Clones）\n",
      "\n",
      "6.《星球大战前传三：西斯的复仇》（Star Wars: Revenge of the Sith）\n",
      "\n",
      "7.《星球大战续集一：原力觉醒》（Star Wars: The Force Awakens）\n",
      "\n",
      "8.《星球大战续集二：最后的绝地武士》（Star Wars: The Last Jedi）\n",
      "\n",
      "9.《星球大战续集三：天行者崛起》（Star Wars: Rise of Skywalker）\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "以下是所有星球大战电影的标题：\n",
      "\n",
      "1.《星球大战：新希望》（Star Wars: A New Hope）\n",
      "\n",
      "2.《星球大战：帝国反击战》（Star Wars: The Empire Strikes Back）\n",
      "\n",
      "3.《星球大战：绝地归来》（Star Wars: Return of the Jedi）\n",
      "\n",
      "4.《星球大战前传一：魅影危机》（Star Wars: The Phantom Menace）\n",
      "\n",
      "5.《星球大战前传二：克隆人的进攻》（Star Wars: Attack of the Clones）\n",
      "\n",
      "6.《星球大战前传三：西斯的复仇》（Star Wars: Revenge of the Sith）\n",
      "\n",
      "7.《星球大战续集一：原力觉醒》（Star Wars: The Force Awakens）\n",
      "\n",
      "8.《星球大战续集二：最后的绝地武士》（Star Wars: The Last Jedi）\n",
      "\n",
      "9.《星球大战续集三：天行者崛起》（Star Wars: Rise of Skywalker）\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.utilities import GraphQLAPIWrapper\n",
    "\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "eleven_api_key = os.getenv(\"ELEVEN_API_KEY\")\n",
    "\n",
    "llm = ChatTongyi(\n",
    "    model_name = \"qwen-vl-max\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key = dashscope_api_key\n",
    ")\n",
    "tools = load_tools(\n",
    "    [\"graphql\"],\n",
    "    graphql_endpoint=\"https://swapi-graphql.netlify.app/.netlify/functions/index\",\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True\n",
    ")\n",
    "\n",
    "graphql_fields = \"\"\"\n",
    "allFilms {\n",
    "    films {\n",
    "        title\n",
    "        director\n",
    "        releaseDate\n",
    "        speciesConnection {\n",
    "            species {\n",
    "                name\n",
    "                classification\n",
    "                homeworld {\n",
    "                    name\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"Search for the titles of all the stawars films stored in the graphql database that has this schema,and answer in chinese:\"\n",
    "\n",
    "\n",
    "result = agent({\"input\": suffix + graphql_fields})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面,我们来讲ToolKit的使用\n",
    "tookit是langchain已经封装好的一系列工具，一个工具包是一组工具来组合完成特定的任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个是\n",
    "Azure认知服务 https://portal.azure.com/#allservices\n",
    "\n",
    "- AzureCogsFormRecognizerTool：从文档里提取文本\n",
    "- AzureCogsSpeech2TextTool：语音到文本\n",
    "- AzureCogsText2SpeechTool：文本到语音"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们需要安装几个包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-ai-formrecognizer\n",
    "! pip install azure-cognitiveservices-speech\n",
    "! pip install azure-ai-textanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 这里的部分需要你去申请微软Azure服务,需要信用卡\n",
    "os.environ[\"AZURE_COGS_KEY\"] = \"c10\"\n",
    "os.environ[\"AZURE_COGS_ENDPOINT\"] = \"https://eastus.api.cognitive.microsoft.com/\"\n",
    "os.environ[\"AZURE_COGS_REGION\"] = \"eastus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "azure-ai-vision is not installed. Run `pip install azure-ai-vision` to install.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\tools\\azure_cognitive_services\\image_analysis.py:49\u001b[0m, in \u001b[0;36mAzureCogsImageAnalysisTool.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msdk\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision_service\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sdk\u001b[38;5;241m.\u001b[39mVisionServiceOptions(\n\u001b[0;32m     52\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mazure_cogs_endpoint, key\u001b[38;5;241m=\u001b[39mazure_cogs_key\n\u001b[0;32m     53\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'azure.ai.vision'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureCognitiveServicesToolkit\n\u001b[0;32m      3\u001b[0m toolkit \u001b[38;5;241m=\u001b[39m AzureCognitiveServicesToolkit()\n\u001b[1;32m----> 4\u001b[0m [tool\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtoolkit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\agent_toolkits\\azure_cognitive_services.py:32\u001b[0m, in \u001b[0;36mAzureCognitiveServicesToolkit.get_tools\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# TODO: Remove check once azure-ai-vision supports MacOS.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinux\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 32\u001b[0m     tools\u001b[38;5;241m.\u001b[39mappend(\u001b[43mAzureCogsImageAnalysisTool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tools\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\v1\\main.py:1048\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m validator \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39m__pre_root_validators__:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1048\u001b[0m         input_data \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {}, \u001b[38;5;28mset\u001b[39m(), ValidationError([ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY)], cls_)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\tools\\azure_cognitive_services\\image_analysis.py:63\u001b[0m, in \u001b[0;36mAzureCogsImageAnalysisTool.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m     56\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis_options\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     57\u001b[0m         sdk\u001b[38;5;241m.\u001b[39mImageAnalysisFeature\u001b[38;5;241m.\u001b[39mCAPTION\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;241m|\u001b[39m sdk\u001b[38;5;241m.\u001b[39mImageAnalysisFeature\u001b[38;5;241m.\u001b[39mOBJECTS\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;241m|\u001b[39m sdk\u001b[38;5;241m.\u001b[39mImageAnalysisFeature\u001b[38;5;241m.\u001b[39mTAGS\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;241m|\u001b[39m sdk\u001b[38;5;241m.\u001b[39mImageAnalysisFeature\u001b[38;5;241m.\u001b[39mTEXT\n\u001b[0;32m     61\u001b[0m     )\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure-ai-vision is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun `pip install azure-ai-vision` to install.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m     )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[1;31mImportError\u001b[0m: azure-ai-vision is not installed. Run `pip install azure-ai-vision` to install."
     ]
    }
   ],
   "source": [
    "# 创建toolkit\n",
    "from langchain.agents.agent_toolkits import AzureCognitiveServicesToolkit\n",
    "toolkit = AzureCognitiveServicesToolkit()\n",
    "[tool.name for tool in toolkit.get_tools()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent使用\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "# 定义llm\n",
    "llm = ChatTongyi(\n",
    "    dashscope_api_key = dashscope_api_key,\n",
    "    model_name = \"qwen-vl-max\",\n",
    "    temperature = 0\n",
    ")\n",
    "agent = initialize_agent(\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    tools= toolkit.get_tools()\n",
    ")\n",
    "audio_file = agent.run(\"Tell me a joke and read it out for me.\")\n",
    "print(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来实现的是一个Python代码机器人\n",
    "也是属于langchain定义toolkit之中\n",
    "来用下看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_experimental\n",
      "  Downloading langchain_experimental-0.0.62-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-community<0.3.0,>=0.2.6 (from langchain_experimental)\n",
      "  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_experimental) (0.2.10)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (0.6.7)\n",
      "Collecting langchain<0.3.0,>=0.2.7 (from langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
      "  Downloading langchain-0.2.7-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain_experimental)\n",
      "  Downloading langchain_core-0.2.12-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (0.1.75)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (8.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_experimental) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_experimental) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_experimental) (2.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_experimental) (2.4)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<0.3.0,>=0.2.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (0.2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.10->langchain_experimental) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.10->langchain_experimental) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.10->langchain_experimental) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\xiele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.0.0)\n",
      "Downloading langchain_experimental-0.0.62-py3-none-any.whl (202 kB)\n",
      "   ---------------------------------------- 0.0/202.7 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/202.7 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 30.7/202.7 kB 330.3 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 41.0/202.7 kB 326.8 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 41.0/202.7 kB 326.8 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 122.9/202.7 kB 554.9 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 122.9/202.7 kB 554.9 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 143.4/202.7 kB 448.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 202.7/202.7 kB 559.6 kB/s eta 0:00:00\n",
      "Downloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.2 MB 2.0 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.1/2.2 MB 2.3 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.2/2.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.4/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.2.12-py3-none-any.whl (355 kB)\n",
      "   ---------------------------------------- 0.0/355.8 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 102.4/355.8 kB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 245.8/355.8 kB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 245.8/355.8 kB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 245.8/355.8 kB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  348.2/355.8 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 355.8/355.8 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading langchain-0.2.7-py3-none-any.whl (983 kB)\n",
      "   ---------------------------------------- 0.0/983.6 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 225.3/983.6 kB 6.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 471.0/983.6 kB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 655.4/983.6 kB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 880.6/983.6 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 983.6/983.6 kB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: langchain-core, langchain, langchain-community, langchain_experimental\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.10\n",
      "    Uninstalling langchain-core-0.2.10:\n",
      "      Successfully uninstalled langchain-core-0.2.10\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.6\n",
      "    Uninstalling langchain-0.2.6:\n",
      "      Successfully uninstalled langchain-0.2.6\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.2.4\n",
      "    Uninstalling langchain-community-0.2.4:\n",
      "      Successfully uninstalled langchain-community-0.2.4\n",
      "Successfully installed langchain-0.2.7 langchain-community-0.2.7 langchain-core-0.2.12 langchain_experimental-0.0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 首先需要安装一个包\n",
    "! pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to calculate the 10th fibonacci number\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "Observation:\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSyntaxError('invalid syntax', ('<string>', 8, 13, 'Observation:\\n', 8, 14))\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `I made a mistake in my previous code. I forgot to include the closing parenthesis on line 8. Let me correct that and try again.\n\n```\ndef fibonacci(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n\nfibonacci(9)\n```\n\nObservation:`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1370\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1370\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:810\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    809\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\mrkl\\output_parser.py:79\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*?)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL):\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m         observation\u001b[38;5;241m=\u001b[39mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001b[0;32m     82\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m     83\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     84\u001b[0m     )\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[0;32m     87\u001b[0m ):\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Could not parse LLM output: `I made a mistake in my previous code. I forgot to include the closing parenthesis on line 8. Let me correct that and try again.\n\n```\ndef fibonacci(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n\nfibonacci(9)\n```\n\nObservation:`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 30\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatTongyi\n\u001b[0;32m     18\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m create_python_agent(\n\u001b[0;32m     19\u001b[0m     llm\u001b[38;5;241m=\u001b[39m ChatTongyi(\n\u001b[0;32m     20\u001b[0m         dashscope_api_key \u001b[38;5;241m=\u001b[39m dashscope_api_key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# agent_executor_kwargs= {\"handle_parsing_errors\": True}\u001b[39;00m\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the 10th fibonacci number?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:600\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    601\u001b[0m         _output_key\n\u001b[0;32m    602\u001b[0m     ]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    606\u001b[0m         _output_key\n\u001b[0;32m    607\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1636\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1636\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1639\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1645\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1646\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1342\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1335\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1340\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1342\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1344\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1352\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1381\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[1;32m-> 1381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn output parsing error occurred. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m     )\n\u001b[0;32m   1387\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `I made a mistake in my previous code. I forgot to include the closing parenthesis on line 8. Let me correct that and try again.\n\n```\ndef fibonacci(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n\nfibonacci(9)\n```\n\nObservation:`"
     ]
    }
   ],
   "source": [
    "# OK,我们接着实现\n",
    "# 首先导入模块\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "eleven_api_key = os.getenv(\"ELEVEN_API_KEY\")\n",
    "\n",
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm= ChatTongyi(\n",
    "        dashscope_api_key = dashscope_api_key,\n",
    "        temperature = 0,\n",
    "        model_name = \"qwen-vl-max\"\n",
    "    ),\n",
    "    tool= PythonREPLTool(),\n",
    "    agent_type= AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose= True,\n",
    "    # agent_executor_kwargs= {\"handle_parsing_errors\": True}\n",
    ")\n",
    "\n",
    "agent_executor.run(\"What is the 10th fibonacci number?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来的是SQL Toolkit\n",
    "SQL Database\n",
    "使用SQLDatabaseChain构建的agent，用来根据数据库回答一般行动饿问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_community.chat_models import ChatTongyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `I`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1370\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1370\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:463\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2877\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   2872\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2873\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   2874\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2875\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2876\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2877\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2864\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   2859\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2860\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   2861\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2862\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2863\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2864\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   2865\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2866\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   2867\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   2868\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2869\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1862\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1862\u001b[0m     chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2826\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   2824\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 2826\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_pipeline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1175\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1175\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:812\u001b[0m, in \u001b[0;36mRunnable.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;124;03mDefault implementation of stream, which calls invoke.\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03mSubclasses should override this method if they support streaming output.\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 812\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:183\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1593\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1590\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1591\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1592\u001b[0m         Output,\n\u001b[1;32m-> 1593\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1595\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1596\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1597\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1598\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1599\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1601\u001b[0m     )\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:184\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m--> 184\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    188\u001b[0m         config,\n\u001b[0;32m    189\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    190\u001b[0m     )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:237\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mThe return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    Structured output.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\output_parsers\\react_single_input.py:75\u001b[0m, in \u001b[0;36mReActSingleInputOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*?)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m         observation\u001b[38;5;241m=\u001b[39mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001b[0;32m     78\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m     79\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m     )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[0;32m     83\u001b[0m ):\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Could not parse LLM output: `I`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m\n\u001b[0;32m      9\u001b[0m toolkit \u001b[38;5;241m=\u001b[39m SQLDatabaseToolkit(db\u001b[38;5;241m=\u001b[39mdb, llm\u001b[38;5;241m=\u001b[39m Tongyi(temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, model\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen-max\u001b[39m\u001b[38;5;124m\"\u001b[39m, dashscope_api_key\u001b[38;5;241m=\u001b[39m dashscope_api_key))\n\u001b[0;32m     11\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m create_sql_agent(\n\u001b[0;32m     12\u001b[0m     llm\u001b[38;5;241m=\u001b[39m ChatTongyi(\n\u001b[0;32m     13\u001b[0m         temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     handle_parsing_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDescribe the playlisttrack table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:600\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    601\u001b[0m         _output_key\n\u001b[0;32m    602\u001b[0m     ]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    606\u001b[0m         _output_key\n\u001b[0;32m    607\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1636\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1636\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1639\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1645\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1646\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1342\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1335\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1340\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1342\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1344\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1352\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1381\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[1;32m-> 1381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn output parsing error occurred. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m     )\n\u001b[0;32m   1387\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `I`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "eleven_api_key = os.getenv(\"ELEVEN_API_KEY\")\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm= Tongyi(temperature = 0, model= \"Qwen-max\", dashscope_api_key= dashscope_api_key))\n",
    "\n",
    "agent_executor = create_sql_agent(\n",
    "    llm= ChatTongyi(\n",
    "        temperature = 0,\n",
    "        model_name= \"qwen-vl-max\",\n",
    "        dashscope_api_key= dashscope_api_key\n",
    "    ),\n",
    "    toolkit= toolkit,\n",
    "    verbose= True,\n",
    "    agent_type= AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "agent_executor.run(\"Describe the playlisttrack table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `I`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1370\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1370\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:463\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2877\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   2872\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2873\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   2874\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2875\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2876\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2877\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2864\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   2859\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2860\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   2861\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2862\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2863\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2864\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   2865\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2866\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   2867\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   2868\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2869\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1862\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1862\u001b[0m     chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2826\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   2824\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 2826\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_pipeline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1175\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1175\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:812\u001b[0m, in \u001b[0;36mRunnable.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;124;03mDefault implementation of stream, which calls invoke.\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03mSubclasses should override this method if they support streaming output.\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 812\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:183\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1593\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1590\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1591\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1592\u001b[0m         Output,\n\u001b[1;32m-> 1593\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1595\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1596\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1597\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1598\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1599\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1601\u001b[0m     )\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:184\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m--> 184\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    188\u001b[0m         config,\n\u001b[0;32m    189\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    190\u001b[0m     )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:237\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mThe return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    Structured output.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\output_parsers\\react_single_input.py:75\u001b[0m, in \u001b[0;36mReActSingleInputOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*?)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m         observation\u001b[38;5;241m=\u001b[39mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001b[0;32m     78\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m     79\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m     )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[0;32m     83\u001b[0m ):\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Could not parse LLM output: `I`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 我们也可以用自然语言描述一个数据库表\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDescribe the playlist table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:600\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    601\u001b[0m         _output_key\n\u001b[0;32m    602\u001b[0m     ]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    606\u001b[0m         _output_key\n\u001b[0;32m    607\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1636\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1636\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1639\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1645\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1646\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1342\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1335\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1340\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1342\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1344\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1352\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\agents\\agent.py:1381\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[1;32m-> 1381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn output parsing error occurred. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m     )\n\u001b[0;32m   1387\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `I`"
     ]
    }
   ],
   "source": [
    "# 我们也可以用自然语言描述一个数据库表\n",
    "agent_executor.run(\"Describe the playlist table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用自然语言跑SQL查询语句\n",
    "agent_executor.run(\n",
    "    \"Show the total number of tranks in each playlist name should be included in the result.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent可以自动修复不存在的键值\n",
    "agent_executor.run(\"Who are the top 3 best selling artists?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LECL : langchain表达式语言\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种在langchain之上封装的高级解释语言\n",
    "简化链条开发,支持真实生产环境而发明\n",
    "\n",
    "更好的流式支持\n",
    "更好的异步支持\n",
    "优化执行时间\n",
    "支持重试和反馈\n",
    "轻松获取中间步骤\n",
    "输入输出强验证\n",
    "无缝追踪集成\n",
    "无缝部署集成\n",
    "\n",
    "LangChain表达式语言，或者LCEL，是一种声明式的方式，可以轻松地将链条组合在一起。 LCEL从第一天开始就被设计为支持将原型放入生产中，不需要改变任何代码，从最简单的“提示+LLM”链到最复杂的链(我们已经看到人们成功地在生产中运行了包含数百步的LCEL链)。\n",
    "最新版的langchain的特性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看下一个最简单的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "eleven_api_key = os.getenv(\"ELEVEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'问：你最喜欢哪款的冰淇淋？\\n答：最喜欢不让我胖的那款。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先导入我们的模块,这里需要注意的是,最新版的langchain中,其实已经将一些东西拆分出来了\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"给我讲一个关于{topic}的笑话\")\n",
    "model = ChatTongyi(temperature=0,model_name = \"qwen-vl-max\",dashscope_api_key = dashscope_api_key)\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"topic\": \"冰淇淋\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以看下就是prompt在新版的LCEL中的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='给我讲一个关于长颈鹿的笑话')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"长颈鹿\"})\n",
    "prompt_value\n",
    "# 这里可以看到我们使用的是chatModel的方式,于是就是一个人类问答的messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='给我讲一个关于长颈鹿的笑话')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们也可以单独的打印出来我们的message\n",
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 给我讲一个关于长颈鹿的笑话'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以把里面的内容打印成字符串\n",
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是关于Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'text': '当我说我要和长颈鹿去约会时，大家都说我太高了。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '83ebb76f-0231-9bca-b084-1f24dd8e0673', 'token_usage': {'input_tokens': 28, 'output_tokens': 17}}, id='run-f9dcc0e7-d4ea-4558-805f-21dd235f4817-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke(prompt_value)\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用llm区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'好的，这是一个关于长颈鹿的笑话：\\n\\n为什么长颈鹿总是那么冷静？\\n\\n因为它们总是高瞻远瞩，不容易被小事困扰呢！'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms.tongyi import Tongyi\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key= dashscope_api_key,\n",
    "    model= \"Qwen-max\",\n",
    "    temperature= 0\n",
    ")\n",
    "llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'当我说我要和长颈鹿去约会时，大家都说我太高了。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Search Exampl\n",
    "\n",
    "建立向量数据\n",
    "使用RAG增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 对一段文本进行向量化\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], embedding=HuggingFaceEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 设置提示词模版\n",
    "template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "    Question:{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatTongyi(\n",
    "    model_name= \"qwen-vl-max\",\n",
    "    dashscope_api_key= dashscope_api_key,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# 构造我们的链\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 展开提问\n",
    "chain.invoke(\"Where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义也非常简单,比如我们来实现一个让它回答的时候使用中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依旧是上面的模版\n",
    "template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "    Question:{question}\n",
    "    Answer in the following language:{language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # 这里第一步就是先把问题拿去向量化\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    } \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'哈里森在哪里工作？\\n答案：哈里森在 Kensho 工作。'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们尝试一下是否生效\n",
    "chain.invoke({\"question\": \"where did harrison work?\", \"language\": \"chinese\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL接口\n",
    "<hr>\n",
    "- 输入格式\n",
    "- 输出格式\n",
    "- 8种不同的接口方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 加载 API key\n",
    "load_dotenv(find_dotenv())\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "eleven_api_key = os.getenv(\"ELEVEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打造一个模版\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "model = ChatTongyi(\n",
    "    dashscope_api_key = dashscope_api_key,\n",
    "    model_name = \"qwen-vl-max\",\n",
    "    temperature = 0\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_template(\"给我讲一个关于{topic}的笑话\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看下输入输出的格式\n",
    "- input schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'topic': {'title': 'Topic', 'type': 'string'}},\n",
       " 'required': ['topic']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt\n",
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'topic': {'title': 'Topic', 'type': 'string'}},\n",
       " 'required': ['topic']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ChatTongyiInput',\n",
       " 'anyOf': [{'type': 'string'},\n",
       "  {'$ref': '#/definitions/StringPromptValue'},\n",
       "  {'$ref': '#/definitions/ChatPromptValueConcrete'},\n",
       "  {'type': 'array',\n",
       "   'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "     {'$ref': '#/definitions/HumanMessage'},\n",
       "     {'$ref': '#/definitions/ChatMessage'},\n",
       "     {'$ref': '#/definitions/SystemMessage'},\n",
       "     {'$ref': '#/definitions/FunctionMessage'},\n",
       "     {'$ref': '#/definitions/ToolMessage'}]}}],\n",
       " 'definitions': {'StringPromptValue': {'title': 'StringPromptValue',\n",
       "   'description': 'String prompt value.',\n",
       "   'type': 'object',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'StringPromptValue',\n",
       "     'enum': ['StringPromptValue'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['text']},\n",
       "  'ToolCall': {'title': 'ToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id']},\n",
       "  'InvalidToolCall': {'title': 'InvalidToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'error': {'title': 'Error', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error']},\n",
       "  'UsageMetadata': {'title': 'UsageMetadata',\n",
       "   'type': 'object',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens']},\n",
       "  'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'},\n",
       "    'tool_calls': {'title': 'Tool Calls',\n",
       "     'default': [],\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/definitions/ToolCall'}},\n",
       "    'invalid_tool_calls': {'title': 'Invalid Tool Calls',\n",
       "     'default': [],\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/definitions/InvalidToolCall'}},\n",
       "    'usage_metadata': {'$ref': '#/definitions/UsageMetadata'}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content', 'name']},\n",
       "  'ToolMessage': {'title': 'ToolMessage',\n",
       "   'description': \"Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A TooMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.\",\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id']},\n",
       "  'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',\n",
       "   'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n",
       "   'type': 'object',\n",
       "   'properties': {'messages': {'title': 'Messages',\n",
       "     'type': 'array',\n",
       "     'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "       {'$ref': '#/definitions/HumanMessage'},\n",
       "       {'$ref': '#/definitions/ChatMessage'},\n",
       "       {'$ref': '#/definitions/SystemMessage'},\n",
       "       {'$ref': '#/definitions/FunctionMessage'},\n",
       "       {'$ref': '#/definitions/ToolMessage'}]}},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ChatPromptValueConcrete',\n",
       "     'enum': ['ChatPromptValueConcrete'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['messages']}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输出格式\n",
    "output schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ChatTongyiOutput',\n",
       " 'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "  {'$ref': '#/definitions/HumanMessage'},\n",
       "  {'$ref': '#/definitions/ChatMessage'},\n",
       "  {'$ref': '#/definitions/SystemMessage'},\n",
       "  {'$ref': '#/definitions/FunctionMessage'},\n",
       "  {'$ref': '#/definitions/ToolMessage'}],\n",
       " 'definitions': {'ToolCall': {'title': 'ToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id']},\n",
       "  'InvalidToolCall': {'title': 'InvalidToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'error': {'title': 'Error', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error']},\n",
       "  'UsageMetadata': {'title': 'UsageMetadata',\n",
       "   'type': 'object',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens']},\n",
       "  'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'},\n",
       "    'tool_calls': {'title': 'Tool Calls',\n",
       "     'default': [],\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/definitions/ToolCall'}},\n",
       "    'invalid_tool_calls': {'title': 'Invalid Tool Calls',\n",
       "     'default': [],\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/definitions/InvalidToolCall'}},\n",
       "    'usage_metadata': {'$ref': '#/definitions/UsageMetadata'}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content', 'name']},\n",
       "  'ToolMessage': {'title': 'ToolMessage',\n",
       "   'description': \"Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A TooMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.\",\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id']}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream(流式)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '为什么'}][{'text': '狗'}][{'text': '可以从'}][{'text': '鼻子里发出警报'}][{'text': '声？\\n\\n因为它会“汪”响'}][{'text': '警笛！（音响警笛）'}][]"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"topic\": \"狗\"}):\n",
    "    print(s.content, end='',flush= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'text': '好的，这里有一个狗的笑话：\\n\\n为什么狗不能当裁判？\\n\\n因为他们总是“犯规”（汪犯规）。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '7bc66bc1-deae-9567-bcfc-7dd68135c0d7', 'token_usage': {'input_tokens': 26, 'output_tokens': 24}}, id='run-4bb64a3b-0b70-40c3-bd2e-89fcf2bead58-0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"狗\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch(并行)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=[{'text': '为什么狗可以从鼻子里发出警报声？\\n\\n因为它会“汪”响警笛！（音响警笛）'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '0bdee063-6334-9103-9fdd-c8434e52cb8e', 'token_usage': {'input_tokens': 26, 'output_tokens': 25}}, id='run-161838ce-d215-409a-99f7-ab45e789559d-0'),\n",
       " AIMessage(content=[{'text': '为什么猫会成为网络明星？\\n\\n因为它们在网上被点击最多！'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': 'babe21df-5d93-9570-bb4a-703f44e00e1d', 'token_usage': {'input_tokens': 26, 'output_tokens': 15}}, id='run-84ee2fea-4a32-4af0-9a87-d473f5538da2-0')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"狗\"},{\"topic\": \"猫\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_concurrency 可以控制并发数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=[{'text': '为什么狗可以从鼻子里发出警报声？\\n\\n因为它会“汪”响警笛！（音响警笛）'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '7e6ebbee-2c4d-91f4-9bcd-ba61a285805a', 'token_usage': {'input_tokens': 26, 'output_tokens': 25}}, id='run-d2c24467-97ba-42fa-9eb1-d4fc361e2e3e-0'),\n",
       " AIMessage(content=[{'text': '为什么猫会成为网络明星？\\n\\n因为它们在网上被点击最多！'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '2960b7c0-15a2-9e01-8bf5-50ecc66cfd99', 'token_usage': {'input_tokens': 26, 'output_tokens': 15}}, id='run-7939cd95-105f-4c9c-9881-30dc9d2a2a0d-0'),\n",
       " AIMessage(content=[{'text': '当鸭变成鸡，会发生什么？\\n它们会“嘎嘎”叫而不是“呱呱”叫。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '10274710-a9f7-9cdb-9953-c77ed663ba25', 'token_usage': {'input_tokens': 26, 'output_tokens': 23}}, id='run-5c785eaf-c9ef-4464-8152-d7b56e1b9e30-0')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"狗\"}, {\"topic\": \"猫\"}, {\"topic\": \"鸭\"}], config={\"max_concurrency\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async Steam异步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '对不起'}][{'text': '，'}][{'text': '我'}][{'text': '并不知道通义是什么'}][{'text': '意思。'}]"
     ]
    }
   ],
   "source": [
    "async for s in chain.astream({\"topic\": \"通义\"}):\n",
    "    print(s.content, end= '',flush= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'text': 'OpenAI走进一家酒吧，酒保说：“欢迎光临！你们想喝点什么？” OpenAI想了想，然后说：“来一杯Python吧。” 于是他们一起享用了一杯美味的Python鸡尾酒。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '0e553375-6662-90ad-8f54-83fc83b7db95', 'token_usage': {'input_tokens': 27, 'output_tokens': 47}}, id='run-6fcd2764-99b8-485d-9314-b280aa7f49d1-0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.ainvoke({\"topic\": \"OpenAI\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=[{'text': '好的，来个鸡的笑话。\\n\\n为什么鸡要过马路？\\n\\n为了到对面去啊！'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': 'f28d5c69-05cc-9e6d-856b-181da4f62371', 'token_usage': {'input_tokens': 26, 'output_tokens': 21}}, id='run-32e20508-3cfb-40b7-94ed-8f02d32a2c3b-0'),\n",
       " AIMessage(content=[{'text': '当鸭变成鸡，会发生什么？\\n它们会“嘎嘎”叫而不是“呱呱”叫。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '0db960d3-deb4-9193-a033-104245593e87', 'token_usage': {'input_tokens': 26, 'output_tokens': 23}}, id='run-349963d7-f5ad-485f-9b14-e4642ebc22f0-0'),\n",
       " AIMessage(content=[{'text': '好的，来一个鹅的笑话：\\n\\n为什么鹅总是追人？\\n\\n因为它想告诉你它不想成为烤鹅。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': 'a8605907-d58d-9288-bc03-07f0eba12dcd', 'token_usage': {'input_tokens': 26, 'output_tokens': 24}}, id='run-c0064617-a5f5-44fc-ba6b-75cffe9ac804-0')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.abatch([{\"topic\": \"鸡\"},{\"topic\": \"鸭\"},{\"topic\": \"鹅\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "异步获取中间的步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': '3885c1b0-dcdb-4046-a402-32e1621f82b8',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '096bd683-987e-4368-a570-78bf2c3347d8',\n",
      "            'metadata': {},\n",
      "            'name': 'Docs',\n",
      "            'start_time': '2024-07-09T12:11:35.857+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['map:key:context', 'FAISS', 'HuggingFaceEmbeddings'],\n",
      "            'type': 'retriever'}})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs/final_output',\n",
      "  'value': {'documents': [Document(page_content='德牧是我最喜欢的犬种,是牧羊犬')]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/Docs/end_time',\n",
      "  'value': '2024-07-09T12:11:35.941+00:00'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '文档'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '文档'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '中'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '文档中'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '提到'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '文档中提到'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '，德牧是一种牧'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '文档中提到，德牧是一种牧'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '羊犬。'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '文档中提到，德牧是一种牧羊犬。'})\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 首先导入模块\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "template = \"\"\"\n",
    "    基于下面的上下文来回答问题:\n",
    "    {context}\n",
    "\n",
    "    Question:{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"德牧是我最喜欢的犬种,是牧羊犬\"], embedding=HuggingFaceEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name = \"Docs\"),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async for chunk in retriever_chain.astream_log(\n",
    "    \"德牧是什么?\", include_names=[\"Docs\"]\n",
    "):\n",
    "    print(\"-\"*40)\n",
    "    print(chunk)\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RunLog({'final_output': None,\n",
      " 'id': '2e076ead-c41f-444f-972c-b74c4230d467',\n",
      " 'logs': {},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': [],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLog({'final_output': None,\n",
      " 'id': '2e076ead-c41f-444f-972c-b74c4230d467',\n",
      " 'logs': {'Docs': {'end_time': None,\n",
      "                   'final_output': None,\n",
      "                   'id': '4ac1fd4b-e8d2-4d96-8686-c91a295787bc',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-07-09T12:14:09.029+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context',\n",
      "                            'FAISS',\n",
      "                            'HuggingFaceEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': [],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLog({'final_output': None,\n",
      " 'id': '2e076ead-c41f-444f-972c-b74c4230d467',\n",
      " 'logs': {'Docs': {'end_time': '2024-07-09T12:14:09.109+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='德牧是我最喜欢的犬种,是牧羊犬')]},\n",
      "                   'id': '4ac1fd4b-e8d2-4d96-8686-c91a295787bc',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-07-09T12:14:09.029+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context',\n",
      "                            'FAISS',\n",
      "                            'HuggingFaceEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': [],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLog({'final_output': '文档',\n",
      " 'id': '2e076ead-c41f-444f-972c-b74c4230d467',\n",
      " 'logs': {'Docs': {'end_time': '2024-07-09T12:14:09.109+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='德牧是我最喜欢的犬种,是牧羊犬')]},\n",
      "                   'id': '4ac1fd4b-e8d2-4d96-8686-c91a295787bc',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-07-09T12:14:09.029+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context',\n",
      "                            'FAISS',\n",
      "                            'HuggingFaceEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['文档'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLog({'final_output': '文档中',\n",
      " 'id': '2e076ead-c41f-444f-972c-b74c4230d467',\n",
      " 'logs': {'Docs': {'end_time': '2024-07-09T12:14:09.109+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='德牧是我最喜欢的犬种,是牧羊犬')]},\n",
      "                   'id': '4ac1fd4b-e8d2-4d96-8686-c91a295787bc',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-07-09T12:14:09.029+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context',\n",
      "                            'FAISS',\n",
      "                            'HuggingFaceEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['文档', '中'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLog({'final_output': '文档中提到',\n",
      " 'id': '2e076ead-c41f-444f-972c-b74c4230d467',\n",
      " 'logs': {'Docs': {'end_time': '2024-07-09T12:14:09.109+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='德牧是我最喜欢的犬种,是牧羊犬')]},\n",
      "                   'id': '4ac1fd4b-e8d2-4d96-8686-c91a295787bc',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-07-09T12:14:09.029+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context',\n",
      "                            'FAISS',\n",
      "                            'HuggingFaceEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['文档', '中', '提到'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLog({'final_output': '文档中提到，德牧是一种牧',\n",
      " 'id': '2e076ead-c41f-444f-972c-b74c4230d467',\n",
      " 'logs': {'Docs': {'end_time': '2024-07-09T12:14:09.109+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='德牧是我最喜欢的犬种,是牧羊犬')]},\n",
      "                   'id': '4ac1fd4b-e8d2-4d96-8686-c91a295787bc',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-07-09T12:14:09.029+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context',\n",
      "                            'FAISS',\n",
      "                            'HuggingFaceEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['文档', '中', '提到', '，德牧是一种牧'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "RunLog({'final_output': '文档中提到，德牧是一种牧羊犬。',\n",
      " 'id': '2e076ead-c41f-444f-972c-b74c4230d467',\n",
      " 'logs': {'Docs': {'end_time': '2024-07-09T12:14:09.109+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='德牧是我最喜欢的犬种,是牧羊犬')]},\n",
      "                   'id': '4ac1fd4b-e8d2-4d96-8686-c91a295787bc',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-07-09T12:14:09.029+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context',\n",
      "                            'FAISS',\n",
      "                            'HuggingFaceEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['文档', '中', '提到', '，德牧是一种牧', '羊犬。'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 只看状态值的话是这样的\n",
    "async for chunk in retriever_chain.astream_log(\n",
    "    \"德牧是什么?\", include_names=[\"Docs\"], diff= False\n",
    "):\n",
    "    print(\"-\"*40)\n",
    "    print(chunk)\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并行支持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain1 = ChatPromptTemplate.from_template(\"给我讲一个关于{topic}的唐诗\") | model\n",
    "chain2 = (\n",
    "    ChatPromptTemplate.from_template(\"写两行关于{topic}的宋词\")\n",
    "    | model\n",
    ")\n",
    "combined = RunnableParallel(joke=chain1, poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '以下'}][{'text': '是一'}][{'text': '首'}][{'text': '李白的著名唐诗'}][{'text': '《将进酒》：\\n\\n君不见'}][{'text': '黄河之水天上来，奔流'}][{'text': '到海不复回。\\n君不见'}][{'text': '高堂明镜悲白发，'}][{'text': '朝如青丝暮成雪。'}][{'text': '\\n人生得意须尽欢，莫使'}][{'text': '金樽空对月。\\n天生我'}][{'text': '材必有用，千金散尽'}][{'text': '还复来。\\n烹羊宰牛'}][{'text': '且为乐，会须一饮'}][{'text': '三百杯。\\n\\n这首诗是李白的经典'}][{'text': '之作，通过描述黄河的壮丽'}][{'text': '景色和人们对生活的感慨，表达了诗人'}][{'text': '豪放、旷达的人生观和'}][{'text': '乐观积极的生活态度。整首诗'}][{'text': '语言华丽，意境深远，展现了李白'}][{'text': '独特的艺术风格和文学才华。'}]"
     ]
    }
   ],
   "source": [
    "# 先来看单独运行的时间\n",
    "# %%time\n",
    "for s in chain1.stream({\"topic\": \"李白\"}):\n",
    "    print(s.content, end='',flush= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '《'}][{'text': '江'}][{'text': '城'}][{'text': '子·乙卯正'}][{'text': '月二十日夜记梦》\\n\\n十年'}][{'text': '生死两茫茫，不思量，'}][{'text': '自难忘。千里孤坟，无'}][{'text': '处话凄凉。纵使相'}][{'text': '逢应不识，尘满面'}][{'text': '，鬓如霜。\\n\\n夜来幽'}][{'text': '梦忽还乡，小轩窗'}][{'text': '，正梳妆。相顾无'}][{'text': '言，惟有泪千行。'}][{'text': '料得年年肠断处，'}][{'text': '明月夜，短松冈。'}][{'text': '\\n\\n这首词是苏轼为悼念'}][{'text': '原配妻子王弗而作。'}][{'text': '十年生死两茫茫，不思量'}][{'text': '，自难忘”，人鬼殊途'}][{'text': '，恩爱永绝，“不思'}][{'text': '量”已然是刻骨铭心'}][{'text': '，“自难忘”。'}]"
     ]
    }
   ],
   "source": [
    "for s in chain2.stream({\"topic\": \"苏轼\"}):\n",
    "    print(s.content, end= '', flush= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': AIMessage(content=[{'text': '《将进酒》\\n君不见黄河之水天上来，奔流到海不复回。\\n君不见高堂明镜悲白发，朝如青丝暮成雪。\\n\\n人生得意须尽欢，莫使金樽空对月。\\n天生我材必有用，千金散尽还复来。\\n烹羊宰牛且为乐，会须一饮三百杯。\\n\\n岑夫子，丹丘生，将进酒，杯莫停。\\n与君歌一曲，请君为我倾耳听。\\n钟鼓馔玉不足贵，但愿长醉不愿醒。\\n\\n古来圣贤皆寂寞，惟有饮者留其名。\\n陈王昔时宴平乐，斗酒十千恣欢谑。\\n主人何为言少钱，径须沽取对君酌。\\n五花马、千金裘，\\n呼儿将出换美酒，与尔同销万古愁。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '118b755a-833e-9998-bcdd-dfdee1fe63b4', 'token_usage': {'input_tokens': 30, 'output_tokens': 197}}, id='run-8ccfefec-296b-4f2f-b231-d9e18958e0cc-0'),\n",
       " 'poem': AIMessage(content=[{'text': '\"举杯邀明月，对影成三人。\" - 李白《月下独酌》\\n\\n\"人有悲欢离合，月有阴晴圆缺。此事古难全，但愿人长久，千里共婵娟。\" - 苏轼《水调歌头·明月几时有》'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '91681194-90cd-914b-adb9-0a262e8523a1', 'token_usage': {'input_tokens': 30, 'output_tokens': 71}}, id='run-8a95cbdb-3b5a-44ef-ba1e-82d25542331c-0')}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 并行执行\n",
    "combined.invoke({\"topic\": \"李白或者苏轼\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并行批处理,适用大量生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=[{'text': '以下是一首李白的著名唐诗《将进酒》：\\n\\n君不见黄河之水天上来，奔流到海不复回。\\n君不见高堂明镜悲白发，朝如青丝暮成雪。\\n人生得意须尽欢，莫使金樽空对月。\\n天生我材必有用，千金散尽还复来。\\n烹羊宰牛且为乐，会须一饮三百杯。\\n\\n这首诗是李白的经典之作，通过描述黄河的壮丽景色和人们对生活的感慨，表达了诗人豪放、旷达的人生观和乐观积极的生活态度。整首诗语言华丽，意境深远，展现了李白独特的艺术风格和文学才华。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': 'e661ee8a-625a-97e3-a1fb-d3044d1c3f66', 'token_usage': {'input_tokens': 27, 'output_tokens': 144}}, id='run-ec859de7-1b68-4f1d-95a1-81b54f31cdbf-0'),\n",
       " AIMessage(content=[{'text': '清明时节雨纷纷，路上行人欲断魂。\\n\\n借问酒家何处有，牧童遥指杏花村。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': 'd832b31b-7fa9-98e6-92af-f3512768eebd', 'token_usage': {'input_tokens': 28, 'output_tokens': 27}}, id='run-6548a268-9068-46ca-ba37-e612aa98df03-0')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1.batch([{\"topic\": \"李白\"}, {\"topic\": \"杜牧\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2.batch([{\"topic\": \"李白\"}, {\"topic\": \"杜牧\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'joke': AIMessage(content=[{'text': '以下是一首李白的著名唐诗《将进酒》：\\n\\n君不见黄河之水天上来，奔流到海不复回。\\n君不见高堂明镜悲白发，朝如青丝暮成雪。\\n人生得意须尽欢，莫使金樽空对月。\\n天生我材必有用，千金散尽还复来。\\n烹羊宰牛且为乐，会须一饮三百杯。\\n\\n这首诗是李白的经典之作，通过描述黄河的壮丽景色和人们对生活的感慨，表达了诗人豪放、旷达的人生观和乐观积极的生活态度。整首诗语言华丽，意境深远，展现了李白独特的艺术风格和文学才华。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': 'a50af0ec-3a8a-943c-bc1c-d4639d67962e', 'token_usage': {'input_tokens': 27, 'output_tokens': 144}}, id='run-8a2f9e2a-a00f-4d59-80e5-f7724e804f8e-0'),\n",
       "  'poem': AIMessage(content=[{'text': '请注意，宋词是宋代的一种文学形式，而李白是唐代诗人。虽然我们不能用宋词来描述李白，但可以使用唐诗或其他方式来赞美他。例如：\\n\\n古人云：李白斗酒诗百篇，\\n他的才华如同璀璨的星辰照亮了唐朝的历史篇章。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '84f71bca-fd00-9301-816d-42b23cc9dff9', 'token_usage': {'input_tokens': 27, 'output_tokens': 62}}, id='run-3b9f26be-3c7d-4898-a8de-745155af4f3d-0')},\n",
       " {'joke': AIMessage(content=[{'text': '清明时节雨纷纷，路上行人欲断魂。\\n\\n借问酒家何处有，牧童遥指杏花村。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': 'be4ce63c-1d76-9e32-b8d5-aa3a9886d514', 'token_usage': {'input_tokens': 28, 'output_tokens': 27}}, id='run-b9709557-2ced-48fb-a57c-c8cb96dad74d-0'),\n",
       "  'poem': AIMessage(content=[{'text': '请注意，杜牧是唐代诗人，并非宋代。因此无法提供杜牧的宋词。但我可以为您创作一首以杜牧为主题，风格类似于唐诗的作品：\\n\\n杜牧才情高，文采如繁星。\\n笔下生花舞，诗篇传古今。\\n\\n这只是个小例子，如果您有更具体的需求或想要探讨更多关于杜牧的话题，请随时告诉我。我很乐意帮助您。'}], response_metadata={'model_name': 'qwen-vl-max', 'finish_reason': 'stop', 'request_id': '61a27667-d717-9e3a-9901-9dd938a5adb0', 'token_usage': {'input_tokens': 28, 'output_tokens': 86}}, id='run-21b20e10-198d-4809-afc5-8c5abe7e3c94-0')}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 并行处理\n",
    "combined.batch([{\"topic\": \"李白\"}, {\"topic\": \"杜牧\"}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
