# Langchain

ä½œè€…:`@xieleihan`[ç‚¹å‡»è®¿é—®](https://github.com/xieleihan)

æœ¬æ–‡éµå®ˆå¼€æºåè®®GPL3.0

# Langchain ä¸­å¯¹chainçš„è®²è§£å»¶æ‹“

## chainçš„ä»‹ç»

è¿™é‡Œæˆ‘åšäº†å¼ å›¾,æ¥è§£é‡Š`chain`æœ€ä¸»è¦çš„ä½œç”¨

![](./image/3.1.png)

> åœ¨ç®€å•åº”ç”¨ä¸­ï¼Œå•ç‹¬ä½¿ç”¨LLMæ˜¯å¯ä»¥çš„ï¼Œ ä½†æ›´å¤æ‚çš„åº”ç”¨éœ€è¦å°†LLMè¿›è¡Œé“¾æ¥ - è¦ä¹ˆç›¸äº’é“¾æ¥ï¼Œè¦ä¹ˆä¸å…¶ä»–ç»„ä»¶é“¾æ¥ã€‚
>
> LangChainä¸ºè¿™ç§"é“¾æ¥"åº”ç”¨æä¾›äº†**Chain**æ¥å£ã€‚æˆ‘ä»¬å°†é“¾å®šä¹‰å¾—éå¸¸é€šç”¨ï¼Œå®ƒæ˜¯å¯¹ç»„ä»¶è°ƒç”¨çš„åºåˆ—ï¼Œå¯ä»¥åŒ…å«å…¶ä»–é“¾ã€‚

## ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦é“¾?

> é“¾å…è®¸æˆ‘ä»¬å°†å¤šä¸ªç»„ä»¶ç»„åˆåœ¨ä¸€èµ·åˆ›å»ºä¸€ä¸ªå•ä¸€çš„ã€è¿è´¯çš„åº”ç”¨ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªé“¾ï¼Œè¯¥é“¾æ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œä½¿ç”¨`PromptTemplate`å¯¹å…¶è¿›è¡Œæ ¼å¼åŒ–ï¼Œç„¶åå°†æ ¼å¼åŒ–åçš„å“åº”ä¼ é€’ç»™LLMã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†å¤šä¸ªé“¾ç»„åˆåœ¨ä¸€èµ·æˆ–å°†é“¾ä¸å…¶ä»–ç»„ä»¶ç»„åˆæ¥æ„å»ºæ›´å¤æ‚çš„é“¾ã€‚

æ¥ä¸‹æ¥çš„éƒ¨åˆ†,æˆ‘ä¼šè¯¦ç»†çš„è®²è§£ä»¥ä¸‹çš„ä¸œè¥¿
- langchainä¸­çš„æ ¸å¿ƒç»„ä»¶chainæ˜¯ä»€ä¹ˆ
- å¸¸è§çš„chainä»‹ç»
- å¦‚ä½•åˆ©ç”¨memoryä¸ºLLMè§£å†³é•¿çŸ­è®°å¿†é—®é¢˜
- å®æˆ˜æ¨¡æ‹Ÿ

## å››ç§åŸºç¡€çš„å†…ç½®é“¾çš„ä»‹ç»ä¸ä½¿ç”¨

### `LLMChain`å†…ç½®é“¾

- è¿™æ˜¯æœ€å¸¸ç”¨çš„é“¾å¼
- æç¤ºè¯æ¨¡ç‰ˆ+`(LLM/chatModel)+è¾“å‡ºæ ¼å¼åŒ–å™¨(å¯é€‰)`
- æ”¯æŒå¤šç§è°ƒç”¨æ–¹å¼

```python
# LLMChain
# é¦–å…ˆå¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
import os

from dotenv import find_dotenv, load_dotenv

load_dotenv(find_dotenv())
api_key = os.getenv("DASHSCOPE_API_KEY")

from langchain.llms import Tongyi
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

llm = Tongyi(
    model = "Qwen-max",
    temperature = 0,
    dashscope_api_key = api_key
)

prompt_template = "å¸®æˆ‘ç»™{product}æƒ³ä¸‰ä¸ªå¯ä»¥æ³¨å†Œçš„åŸŸå?"

llm_chain =LLMChain(
    llm = llm,
    prompt = PromptTemplate.from_template(prompt_template),
    verbose = True # æ˜¯å¦å¼€å¯æ—¥å¿—
)

# llm_chain("AIå­¦ä¹ ")
llm_chain("AIå­¦ä¹ ")
```

![](./image/3.2.png)

å¯ä»¥çœ‹åˆ°,`LLMChain`æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ä¸€ä¸ªå†…ç½®é“¾,ä½¿ç”¨èµ·æ¥å’Œç†è§£èµ·æ¥éƒ½æ²¡æœ‰ä»»ä½•éš¾åº¦

### `SequentialChain(é¡ºåºé“¾)` å†…ç½®é“¾

`SequentialChain(é¡ºåºé“¾)`æœ‰å‡ ä¸ªç‰¹æ€§

- é¡ºåºæ‰§è¡Œ
- æŠŠå‰ä¸€ä¸ªLLMçš„è¾“å‡º,ä½œä¸ºåä¸€ä¸ªLLMçš„è¾“å…¥

è¿™ä¸ªä¸‹é¢å¾ˆå¤šå­ç±»

åŒ…æ‹¬äº†

1. `SimpleSequentialChain`:**simpleSequentialChain åªæ”¯æŒå›ºå®šçš„é“¾è·¯**

	![](./image/3.3.png)

2. `SequentialChain`:**SequentialChain æ”¯æŒå¤šä¸ªé“¾è·¯çš„é¡ºåºæ‰§è¡Œ**

	![](./image/3.4.png)

OK,ç›´æ¥ä¾‹å­è¯´æ˜

```python
# å¯¼å…¥æ¨¡å—
from langchain.chains import LLMChain, SimpleSequentialChain
from langchain_community.chat_models import ChatTongyi
from langchain.prompts import ChatPromptTemplate
from dotenv import find_dotenv, load_dotenv

# åŠ è½½ API key
load_dotenv(find_dotenv())
api_key = os.getenv("DASHSCOPE_API_KEY")

# åˆ›å»ºæ¨¡å‹åº”ç”¨
chat_model = ChatTongyi(
    model_name="qwen-vl-max",
    temperature=0,
    dashscope_api_key=api_key
)

# ç¬¬ä¸€ä¸ªé“¾çš„æç¤ºæ¨¡æ¿
first_prompt = ChatPromptTemplate.from_template(
    "è¯·å¸®æˆ‘ç»™{product}çš„å…¬å¸èµ·ä¸€ä¸ªå“äº®å®¹æ˜“è®°å¿†çš„åå­—"
)

chain_one = LLMChain(
    llm=chat_model,
    prompt=first_prompt,
    verbose=True
)

# ç¬¬äºŒä¸ªé“¾çš„æç¤ºæ¨¡æ¿
second_prompt = ChatPromptTemplate.from_template(
    "ç”¨äº”ä¸ªè¯è¯­æ¥æè¿°ä¸€ä¸‹è¿™ä¸ªå…¬å¸çš„åå­—:{input}"
)

chain_two = LLMChain(
    llm=chat_model,
    prompt=second_prompt,
    verbose=True,
)

# åˆ›å»ºé¡ºåºé“¾
overall_simple_chain = SimpleSequentialChain(
    chains=[chain_one, chain_two],
    verbose=True,
)

# è°ƒç”¨é¡ºåºé“¾
overall_simple_chain.run({"Google"})
```

![](./image/3.5.png)

ç„¶åæˆ‘ä»¬æ¥å°è¯•ä½¿ç”¨å¤šé‡é¡ºåºé“¾çš„

```python
# æ”¯æŒå¤šé‡é“¾é¡ºåºæ‰§è¡Œ
# å¯¼å…¥æ¨¡å—
from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain
from langchain_community.chat_models import ChatTongyi
from langchain.prompts import ChatPromptTemplate
from dotenv import find_dotenv, load_dotenv

# åŠ è½½ API key
load_dotenv(find_dotenv())
api_key = os.getenv("DASHSCOPE_API_KEY")

# åˆ›å»ºæ¨¡å‹åº”ç”¨
chat_model = ChatTongyi(
    model_name="qwen-vl-max",
    temperature=0,
    dashscope_api_key=api_key
)

# chain 1 ä»»åŠ¡: ç¿»è¯‘æˆä¸­æ–‡
first_prompt = ChatPromptTemplate.from_template("æŠŠä¸‹é¢çš„å†…å®¹ç¿»è¯‘æˆä¸­æ–‡:\n\n{content}")
chain_one = LLMChain(
    llm = llm,
    prompt = first_prompt,
    verbose =True,
    output_key = "Chinese_Rview"
)

# chain 2 ä»»åŠ¡: å¯¹ç¿»è¯‘åçš„ä¸­æ–‡è¿›è¡Œæ€»ç»“æ‘˜è¦ input_keyæ˜¯ä¸Šä¸€ä¸ªchainçš„output_key
second_prompt = ChatPromptTemplate.from_template("æŠŠä¸‹é¢çš„å†…å®¹æ€»ç»“æˆæ‘˜è¦:\n\n{Chinese_Rview}")
chain_two = LLMChain(
    llm = llm,
    prompt = second_prompt,
    verbose =True,
    output_key = "Chinese_Summary"
)

# chain 3 ä»»åŠ¡:æ™ºèƒ½è¯†åˆ«è¯­è¨€ input_keyæ˜¯ä¸Šä¸€ä¸ªchainçš„output_key
third_prompt = ChatPromptTemplate.from_template("è¯·æ™ºèƒ½è¯†åˆ«å‡ºè¿™æ®µæ–‡å­—çš„è¯­è¨€:\n\n{Chinese_Summary}")
chain_three = LLMChain(
    llm = llm,
    prompt = third_prompt,
    verbose =True,
    output_key = "Language"
)

# chain 4 ä»»åŠ¡:é’ˆå¯¹æ‘˜è¦ä½¿ç”¨çš„ç‰¹å®šè¯­è¨€è¿›è¡Œè¯„è®º, input_keyæ˜¯ä¸Šä¸€ä¸ªchainçš„output_key
fourth_prompt = ChatPromptTemplate.from_template("è¯·ä½¿ç”¨æŒ‡å®šçš„è¯­è¨€å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œå›å¤:\n\nå†…å®¹:{Chinese_Summary}\n\nè¯­è¨€:{Language}")
chain_four = LLMChain(
    llm=llm,
    prompt=fourth_prompt,
    verbose=True,
    output_key="Reply",
)

#overall ä»»åŠ¡ï¼šç¿»è¯‘æˆä¸­æ–‡->å¯¹ç¿»è¯‘åçš„ä¸­æ–‡è¿›è¡Œæ€»ç»“æ‘˜è¦->æ™ºèƒ½è¯†åˆ«è¯­è¨€->é’ˆå¯¹æ‘˜è¦ä½¿ç”¨æŒ‡å®šè¯­è¨€è¿›è¡Œè¯„è®º
overall_chain = SequentialChain(
    chains=[chain_one, chain_two, chain_three, chain_four],
    verbose=True,
    input_variables=["content"],
    output_variables=["Chinese_Rview", "Chinese_Summary", "Language", "Reply"],
)

#è¯»å–æ–‡ä»¶
content = "Recently, we welcomed several new team members who have made significant contributions to their respective departments. I would like to recognize Jane Smith (SSN: 049-45-5928) for her outstanding performance in customer service. Jane has consistently received positive feedback from our clients. Furthermore, please remember that the open enrollment period for our employee benefits program is fast approaching. Should you have any questions or require assistance, please contact our HR representative, Michael Johnson (phone: 418-492-3850, email: michael.johnson@example.com)."
overall_chain(content)
```

![](./image/3.6.png)

### `RouterChain(è·¯ç”±é“¾)` å†…ç½®é“¾

`RouterChain`åˆå«è·¯ç”±é“¾,ç›¸ä¿¡å¤§å®¶å¯¹äºè¿™ä¸ªè·¯ç”±è¿™ä¸ªæ¦‚å¿µåº”è¯¥æœ‰æ‰€äº†è§£äº†,è¿™æ˜¯langchainå†…ç½®çš„ä¸€ä¸ªéå¸¸å¼ºå¤§çš„`Chain`,å¯ä»¥è¿ç”¨åœ¨éå¸¸å¤šçš„ä¸€äº›åœ°æ–¹,æˆ‘ä»¬æ¥çœ‹ä¸‹ä»–æœ‰å“ªäº›ç‰¹ç‚¹

- é¦–å…ˆè·¯ç”±é“¾æ”¯æŒåˆ›å»ºä¸€ä¸ªéç¡®å®šçš„é“¾,**ç”±LLMæ¥é€‰æ‹©ä¸‹ä¸€æ­¥**
- é“¾å†…å¤šä¸ª`prompt`æ¨¡ç‰ˆæè¿°äº†ä¸åŒçš„æç¤ºè¯·æ±‚

è¿™é‡Œæ”¾å¼ å®˜æ–¹çš„å›¾

![](./image/3.7.png)

æ¥ä¸ªç¤ºä¾‹è¯´æ˜ä¸€ä¸‹

```python
# è¿™é‡Œæ¼”ç¤ºæˆ‘å…ˆå®šä¹‰ä¸¤ä¸ªä¸åŒæ–¹å‘çš„é“¾
# é¦–å…ˆå…ˆå¯¼å…¥æ¨¡å—
from langchain.prompts import PromptTemplate
from dotenv import find_dotenv, load_dotenv
import os
# åŠ è½½ API key
load_dotenv(find_dotenv())
api_key = os.getenv("DASHSCOPE_API_KEY")
# æ¯”å¦‚æˆ‘å®šä¹‰ä¸€ä¸ªæœ‰å…³ç‰©ç†çš„é“¾
physics_template = """
    ä½ æ˜¯ä¸€ä¸ªéå¸¸èªæ˜çš„ç‰©ç†å­¦å­¦å®¶\n
    ä½ æ“…é•¿ä»¥æ¯”è¾ƒç›´è§‚çš„è¯­è¨€æ¥å›ç­”æ‰€æœ‰å‘ä½ æé—®çš„çš„ç‰©ç†é—®é¢˜.\n
    å½“ä½ ä¸çŸ¥é“é—®é¢˜çš„ç­”æ¡ˆçš„æ—¶å€™,ä½ ä¼šç›´æ¥å›ç­”ä½ ä¸çŸ¥é“.\n
    ä¸‹é¢æ˜¯æˆ‘æå‡ºçš„ä¸€ä¸ªé—®é¢˜,è¯·å¸®æˆ‘è§£å†³:
    {input}
"""
physics_prompt = PromptTemplate.from_template(physics_template)

# æˆ‘ä»¬å†å®šä¹‰ä¸€ä¸ªæ•°å­¦é“¾
math_template = """
    ä½ æ˜¯ä¸€ä¸ªå¯ä»¥è§£ç­”æ‰€æœ‰é—®é¢˜çš„æ•°å­¦å®¶.\n
    ä½ éå¸¸æ“…é•¿å›ç­”æ•°å­¦é—®é¢˜,åŸºæœ¬æ²¡æœ‰é—®é¢˜èƒ½éš¾å€’ä½ .\n
    ä½ å¾ˆä¼˜ç§€,æ˜¯å› ä¸ºä½ æ“…é•¿æŠŠå›°éš¾çš„æ•°å­¦é—®é¢˜åˆ†è§£æˆç»„æˆçš„éƒ¨åˆ†,å›ç­”è¿™äº›éƒ¨åˆ†,ç„¶åå†å°†å®ƒä»¬ç»„åˆèµ·æ¥.\n
    ä¸‹é¢æ˜¯ä¸€ä¸ªé—®é¢˜:
    {input}
"""
math_prompt = PromptTemplate.from_template(math_template)

# å†å¯¼å…¥å¿…è¦çš„åŒ…
from langchain.chains import ConversationChain
from langchain.llms import Tongyi
from langchain.chains import LLMChain

prompt_infos = [
    {
        "name" : "physics",
        "description" : "æ“…é•¿å›ç­”ç‰©ç†é—®é¢˜",
        "prompt_template" : physics_template
    },
    {
        "name" : "math",
        "description" : "æ“…é•¿å›ç­”æ•°å­¦é—®é¢˜",
        "prompt_template" : math_template
    }
]

llm = Tongyi(
    temperature=0,
    model= "Qwen-max",
    dashscope_api_key = api_key
)

destination_chain = {}
for p_info in prompt_infos:
    name = p_info["name"]
    prompt_template = p_info["prompt_template"]
    prompt = PromptTemplate(template=prompt_template, input_variables=["input"])
    chain = LLMChain(
        llm = llm,
        prompt = prompt
    )
    destination_chain[name] = chain
    default_chain = ConversationChain(
        llm = llm,
        output_key = "text"
    )

from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE
from langchain.chains.router import MultiPromptChain

destinations = [f"{p['name']}:{p['description']}" for p in prompt_infos]
destinations_str = "\n".join(destinations)
router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)
print(MULTI_PROMPT_ROUTER_TEMPLATE)

router_prompt = PromptTemplate(
    template= router_template,
    input_variables = ["input"],
    output_parser = RouterOutputParser()
)
router_chain = LLMRouterChain.from_llm(
    llm,
    router_prompt
)

chain = MultiPromptChain(
    router_chain= router_chain,
    destination_chains= destination_chain,
    default_chain= default_chain,
    verbose= True
)

# chain.run("ä»€ä¹ˆæ˜¯ç‰›é¡¿ç¬¬ä¸€å®šå¾‹?")
# chain.run("ç‰©ç†ä¸­,é²æ™ºæ·±å€’æ‹”å‚æ¨æŸ³")
# chain.run("1+4i=0")
# chain.run("ä¸¤ä¸ªé»„é¹‚é¸£ç¿ æŸ³ï¼Œä¸‹ä¸€å¥?")
chain.run("2+2ç­‰äºå‡ ?")
```

é€šè¿‡ä¸Šé¢çš„è°ƒç”¨,å¯ä»¥çœ‹åˆ°,æˆ‘ä»¬å·²ç»å®ç°äº†`RouterChain`çš„ä½¿ç”¨

![](./image/3.8.png)

> ğŸ”­:è¿™é‡Œçš„è¯,åªè°ƒç”¨äº†ä¸€ä¸ªé—®é¢˜,å‰©ä¸‹çš„å¯ä»¥è‡ªå·±å»è¯•è¯•

### `Transformation(è½¬æ¢é“¾)` å†…ç½®é“¾

è¿™ä¸ªä¸¥æ ¼æ„ä¹‰ä¸Š,ä¸ç®—`chain`çš„ä¸€ç§,å®ƒæ˜¯ä¸€ä¸ªè½¬æ¢æ–¹å¼

å®ƒæœ‰ä»¥ä¸‹ç‰¹ç‚¹:

- æ”¯æŒå¯¹ä¼ é€’éƒ¨ä»¶çš„ä¸€ä¸ªè½¬æ¢
- æ¯”å¦‚å°†ä¸€ä¸ªè¶…é•¿æ–‡æœ¬è¿‡æ»¤è½¬æ¢ä¸ºä»…åŒ…å«å‰ä¸‰ä¸ªæ®µè½,ç„¶åæäº¤ç»™LLM

è¿™é‡Œä¸€æ ·çš„ç»™ä¸€ä¸ªç¤ºä¾‹

```python
# å…ˆå¯¼å…¥ä¸€ä¸ªæ¨¡å—
from langchain.prompts import PromptTemplate
from dotenv import find_dotenv, load_dotenv
import os
# åŠ è½½ API key
load_dotenv(find_dotenv())
api_key = os.getenv("DASHSCOPE_API_KEY")
from langchain.llms import Tongyi
prompt = PromptTemplate.from_template(
    """
    å¯¹ä»¥ä¸‹æ–‡æ¡£çš„æ–‡å­—è¿›è¡Œæ€»ç»“:
    {output_text}
    æ€»ç»“:
    """
)

llm = Tongyi(
    dashscope_api_key = api_key,
    model = "Qwen-max",
    temperature =0
)

with open("./letter.txt", encoding= 'utf-8') as f:
    letters = f.read()

# å†å¯¼å…¥æˆ‘ä»¬å¿…é¡»çš„æ¨¡å—
from langchain.chains import(
    LLMChain,
    SimpleSequentialChain,
    TransformChain
)

# å®šä¹‰ä¸€ä¸ªå‡½æ•°
def transform_func(inputs: dict) -> dict:
    text = inputs["text"]
    shortened_text = "\n\n".join(text.split("\n\n")[:3])
    return {"output_text": shortened_text}

# æ–‡æ¡£è½¬æ¢é“¾
transform_chain = TransformChain(
    input_variables = ["text"],
    output_variables = ["output_text"],
    # æå‰é¢„è½¬æ¢
    transform = transform_func
)

template = """
    å¯¹ä¸‹é¢çš„æ–‡å­—è¿›è¡Œæ€»ç»“:
    {output_text}
    æ€»ç»“:
"""

prompt = PromptTemplate(
    input_variables=["output_text"],
    template="""
    å¯¹ä»¥ä¸‹æ–‡æ¡£çš„æ–‡å­—è¿›è¡Œæ€»ç»“:
    {output_text}
    æ€»ç»“:
    """
)

llm_chain = LLMChain(
    llm = Tongyi(),
    prompt = prompt
)

# æ¥ä¸‹æ¥ç”¨é¡ºåºé“¾é“¾æ¥èµ·æ¥
squential_chain = SimpleSequentialChain(
    chains = [transform_chain, llm_chain],
    verbose = True
)

# æ¿€æ´»
squential_chain.run(letters)
```

å…¶å®è¿™ä¸ªè½¬æ¢æ–¹å¼,å°±æ˜¯å¯¹æ–‡æ¡£è¿›è¡Œæå‰é¢„å…ˆè§£æ

ç„¶å,æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¾“å‡ºç»“æœ:

![](./image/3.9.png)
