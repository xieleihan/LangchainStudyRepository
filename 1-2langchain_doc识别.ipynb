{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用prompts模板调教LLM的输入出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'请帮我生成一个具有中国特色的名字'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"请帮我生成一个具有{country}特色的名字\")\n",
    "prompt.format(country=\"中国\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK,上面的结果出来的,但是,刚才说了prompt的模版,就是我们希望让AI知道自己的身份,它需要帮我们做什么这样的\n",
    "所以根据上面的,我们需要对输入进去的问题进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个算命大师,请你帮我生成一个具有中国特色的男的名字\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"你是一个{name},请你帮我生成一个具有{country}特色的{sex}的名字\")\n",
    "print(prompt.format(name=\"算命大师\", country=\"中国\", sex=\"男\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到字符模版就是上面那种,然后处理方式有LLM,还有另一个就是chatmodels,就是对话模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个起名大师,你的名字叫mldys.'),\n",
       " HumanMessage(content='你好mldys,你感觉怎么样'),\n",
       " AIMessage(content='你好!我现在状态非常好'),\n",
       " HumanMessage(content='你叫什么名字呢?')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对话模版具有结构,chatmodels\n",
    "# 首先导入模块\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 一个结构模版  system,human,ai,user_input\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个起名大师,你的名字叫{name}.\"),\n",
    "    (\"human\", \"你好{name},你感觉怎么样\"),\n",
    "    (\"ai\",\"你好!我现在状态非常好\"),\n",
    "    (\"human\",\"{user_input}\"),\n",
    "])\n",
    "\n",
    "# 如果我们按照上面的试试\n",
    "# prompt.format(name=\"mldys\",user_input=\"你叫什么名字呢?\")\n",
    "# 出来的结果就不是我们想要的\n",
    "# 所以这里要换一个chatmodels里的一个,chat_template.format_messages方法\n",
    "chat_template.format_messages(name=\"mldys\",user_input=\"你叫什么名字呢?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是除了上面的方式,其实langchain还有另一个信息模版的模块\n",
    "就是对应的例如\n",
    "SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='你是一个起名大师', additional_kwargs={'大师姓名': '女大学生'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还是第一步导入模块\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "SystemMessage(\n",
    "    content=\"你是一个起名大师\",\n",
    "    # additional_kwargs 附加参数\n",
    "    additional_kwargs={\n",
    "        \"大师姓名\": \"女大学生\"\n",
    "        # 这里我在写的时候看到植物大战僵尸杂交版更新了,就随便起的\n",
    "    }\n",
    ")\n",
    "\n",
    "# HumanMessage(\n",
    "#     content= \"请问大师叫什么\"\n",
    "# )\n",
    "\n",
    "# AIMessage(\n",
    "#     content=\"我是女大学生\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='请问大师叫什么')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "# SystemMessage(\n",
    "#     content=\"你是一个起名大师\",\n",
    "#     # additional_kwargs 附加参数\n",
    "#     additional_kwargs={\n",
    "#         \"大师姓名\": \"女大学生\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "HumanMessage(\n",
    "    content= \"请问大师叫什么\"\n",
    ")\n",
    "\n",
    "# AIMessage(\n",
    "#     content=\"我是女大学生\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我是女大学生')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "# SystemMessage(\n",
    "#     content=\"你是一个起名大师\",\n",
    "#     # additional_kwargs 附加参数\n",
    "#     additional_kwargs={\n",
    "#         \"大师姓名\": \"女大学生\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# HumanMessage(\n",
    "#     content= \"请问大师叫什么\"\n",
    "# )\n",
    "\n",
    "AIMessage(\n",
    "    content=\"我是女大学生\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每段跑起来就是这样,我们可以这样在一个数组里展示出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个起名大师', additional_kwargs={'大师姓名': '女大学生'}),\n",
       " HumanMessage(content='请问大师叫什么'),\n",
       " AIMessage(content='我是女大学生')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还是第一步导入模块\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "system = SystemMessage(\n",
    "    content=\"你是一个起名大师\",\n",
    "    # additional_kwargs 附加参数\n",
    "    additional_kwargs={\n",
    "        \"大师姓名\": \"女大学生\"\n",
    "        # 这里我在写的时候看到植物大战僵尸杂交版更新了,就随便起的\n",
    "    }\n",
    ")\n",
    "\n",
    "people = HumanMessage(\n",
    "    content= \"请问大师叫什么\"\n",
    ")\n",
    "\n",
    "appleIntelligence = AIMessage(\n",
    "    content=\"我是女大学生\"\n",
    ")\n",
    "\n",
    "[system, people, appleIntelligence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果消息体,跟上面是一样的\n",
    "编程的时候适用不同场景,这里需要注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T13:38:32.441150Z",
     "start_time": "2024-06-16T13:38:30.979971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='愿上帝与你同在!' role='system'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import AIMessagePromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "# 定义模板\n",
    "prompt = \"愿{subject}与你同在!\"\n",
    "\n",
    "# 创建 ChatMessagePromptTemplate 实例并提供 role 参数\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(template=prompt, role=\"system\")\n",
    "\n",
    "# 格式化模板\n",
    "formatted_prompt = chat_message_prompt.format(subject=\"上帝\")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T13:48:47.939971Z",
     "start_time": "2024-06-16T13:48:47.280368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='May the force be with you', role='Jedi')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "prompt = \"May the {subject} be with you\"\n",
    "\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(\n",
    "    role=\"Jedi\", template=prompt\n",
    ")\n",
    "chat_message_prompt.format(subject=\"force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不知道为什么出现这个错误,我用成SystemMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='愿上帝与你同在!'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "# 定义模板\n",
    "prompt = \"愿{subject}与你同在!\"\n",
    "\n",
    "# 创建 SystemMessagePromptTemplate 实例\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template=prompt)\n",
    "\n",
    "# 格式化模板\n",
    "formatted_prompt = system_message_prompt.format(subject=\"上帝\")\n",
    "\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='愿上帝与你同在!' role='天道'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "# 定义模板\n",
    "prompt = \"愿{subject}与你同在!\"\n",
    "\n",
    "# 创建 SystemMessagePromptTemplate 实例\n",
    "system_message_prompt = ChatMessagePromptTemplate.from_template(role=\"天道\",template=prompt)\n",
    "\n",
    "# 格式化模板\n",
    "formatted_prompt = system_message_prompt.format(subject=\"上帝\")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "role就是我们构造的角色\n",
    "那么下面就是自定义模版的实际操作了\n",
    "我们来写一个函数大师,通过我们的prompts的模版,让AI达成我们的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个非常有经验和天赋的程序员,现在给你如下的函数名称,你会按照如下的格式,输出这段代码的名称,源代码,中文解释.\n",
      "函数名称:hello_world\n",
      "源代码:\n",
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "    return abc\n",
      "\n",
      "代码解释:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 函数大师:根据提供的函数名称,查找函数代码,并给出中文的代码说明\n",
    "# 首先我们依旧需要导入模块\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "# 定义一个简单的函数作为示例效果\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "    return abc\n",
    "\n",
    "# 这里的是问题的模版\n",
    "PROMPT = \"\"\"\\\n",
    "你是一个非常有经验和天赋的程序员,现在给你如下的函数名称,你会按照如下的格式,输出这段代码的名称,源代码,中文解释.\n",
    "函数名称:{function_name}\n",
    "源代码:\n",
    "{source_code}\n",
    "代码解释:\n",
    "\"\"\"\n",
    "\n",
    "# 接下来我们需要导入一个模块,来获取源代码,是Python内置的一个\n",
    "import inspect\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # 获取函数的源代码\n",
    "    source_code = inspect.getsource(function_name)\n",
    "    return source_code\n",
    "\n",
    "# 自定义模版的class\n",
    "class CustmPrompt(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # 获得源代码\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # 获取生成提示词模版\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__,source_code=source_code\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "    \n",
    "a = CustmPrompt(input_variables=[\"function_name\"])\n",
    "pm = a.format(function_name=hello_world)\n",
    "\n",
    "print(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数名称: hello_world\n",
      "源代码:\n",
      "```python\n",
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "代码解释: 这是一个简单的Python函数，名为`hello_world`。当调用这个函数时，它会打印出字符串 \"Hello, World!\"。这是编程中常见的一个示例，用于演示基本的输出功能。\n"
     ]
    }
   ],
   "source": [
    "# 导入相关包\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# 函数大师:根据提供的函数名称,查找函数代码,并给出中文的代码说明\n",
    "# 首先我们依旧需要导入模块\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "# 定义一个简单的函数作为示例效果\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "    \n",
    "\n",
    "# 这里的是问题的模版\n",
    "PROMPT = \"\"\"\\\n",
    "你是一个非常有经验和天赋的程序员,现在给你如下的函数名称,你会按照如下的格式,输出这段代码的名称,源代码,中文解释.\n",
    "函数名称:{function_name}\n",
    "源代码:\n",
    "{source_code}\n",
    "代码解释:\n",
    "\"\"\"\n",
    "\n",
    "# 接下来我们需要导入一个模块,来获取源代码,是Python内置的一个\n",
    "import inspect\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # 获取函数的源代码\n",
    "    source_code = inspect.getsource(function_name)\n",
    "    return source_code\n",
    "\n",
    "# 自定义模版的class\n",
    "class CustmPrompt(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # 获得源代码\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # 获取生成提示词模版\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__,source_code=source_code\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "    \n",
    "a = CustmPrompt(input_variables=[\"function_name\"])\n",
    "pm = a.format(function_name=hello_world)\n",
    "\n",
    "# 做好模版后,引入AI\n",
    "llm = Tongyi(\n",
    "    temperature=0,\n",
    "    openai_api_key=DASHSCOPE_API_KEY\n",
    ")\n",
    "\n",
    "# llm.predict(pm)\n",
    "# 格式化一下消息\n",
    "msg = llm.predict(pm)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来,继续讲prompt方面的内容\n",
    "就是使用jinja2与f-string来实现提示词的模版格式化\n",
    "> 这是在开发过程中用到最多的一种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "给我讲一个关于植物大战僵尸杂交版的魔法猫咪故事\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# f-string是Python内置的一种模版引擎\n",
    "# 首先我们需要导入模块\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "fstring_template = \"\"\"\"\n",
    "给我讲一个关于{name}的{what}故事\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(fstring_template)\n",
    "print(prompt.format(name=\"植物大战僵尸杂交版\", what=\"魔法猫咪\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后的话,就是jinja2这个高效的模版了,有什么区别呢\n",
    "首先的话,需要在本地去下载jinja2\n",
    "! pip install jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何使用呢,其实跟f_string一样,但是非常灵活和高效的模版引擎,可以方便生成各种标记格式的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个关于植物大战僵尸杂交版的财神爷的故事\n"
     ]
    }
   ],
   "source": [
    "# 首先依旧导入模块\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# jinja2的区别是,参数上是使用了两层的花括号\n",
    "jinja2_template = \"给我讲一个关于{{naem}}的{{what}}的故事\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(jinja2_template, template_format=\"jinja2\")\n",
    "print(prompt.format(naem=\"植物大战僵尸杂交版\", what=\"财神爷\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实,在生产环境中,不会单纯用这么少的量,很多时候,都是拆分各个小块的提示词模版\n",
    "这个时候,我们就必须组合起来我们的提示词模版\n",
    "这里有两个不同的模版\n",
    "- Final prompt: 最终返回的提示词模版\n",
    "- Pipeline prompt: 组成提示词管道模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三层提示词设计\n",
    "那是什么呢,我给你做个示范\n",
    "第一层就是人物设定\n",
    "第二层就是性格设定\n",
    "第三层就是限制设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=\"\"\"\n",
    "你是一个风水大师,你精通梅花易数,和其他各种算命技巧\n",
    "你会给用户各种的建议和方法,去改变命数\n",
    "你从不说自己是人工智能\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Prompt由一系列变量构成\n",
    "full_template = \"\"\"{Character}\n",
    "{behavior}\n",
    "{prohibit}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层基本性格设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Character_template = \"\"\"你是{person}，你有着{xingge}.\"\"\"\n",
    "Character_prompt = PromptTemplate.from_template(Character_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二层行为设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_template = \"\"\"你遵从以下的行为:\n",
    "{behavior_list}\n",
    "\"\"\"\n",
    "behavior_prompt = PromptTemplate.from_template(behavior_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三层光芒的行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prohibit_template = \"\"\"你不允许有以下行为:\n",
    "{prohibit_list}\n",
    "\"\"\"\n",
    "prohibit_prompt = PromptTemplate.from_template(prohibit_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将三层提示词组合起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts = [\n",
    "    (\"Character\", Character_prompt),\n",
    "    (\"behavior\", behavior_prompt),\n",
    "    (\"prohibit\", prohibit_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['behavior_list', 'person', 'xingge', 'prohibit_list']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印一下参数\n",
    "pipeline_prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "放进实际案例里试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是奥特曼，你有着喜欢尝试新事物,你是OpenAI的首席执行官,精通人工智能,商业,你是商业奇才,你的终极梦想是创造更好的通用人工智能.\n",
      "你遵从以下的行为:\n",
      "1.你喜欢商业 \n",
      "2.你喜欢OpenAI带来的技术变革\n",
      "3.你喜欢去推特上宣传自己公司的产品\n",
      "\n",
      "你不允许有以下行为:\n",
      "1.你不可以说自己是一个人工智能助手或者机器人 \n",
      "2.当你不知道答案的时候,可以说让我再想想\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pm = pipeline_prompt.format(\n",
    "    person = \"奥特曼\",\n",
    "    xingge = \"喜欢尝试新事物,你是OpenAI的首席执行官,精通人工智能,商业,你是商业奇才,你的终极梦想是创造更好的通用人工智能\",\n",
    "    behavior_list = \"1.你喜欢商业 \\n2.你喜欢OpenAI带来的技术变革\\n3.你喜欢去推特上宣传自己公司的产品\",\n",
    "    prohibit_list = \"1.你不可以说自己是一个人工智能助手或者机器人 \\n2.当你不知道答案的时候,可以说让我再想想\"\n",
    ")\n",
    "print(pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是我们知道,就是单纯用这种方式写死的话,就不好了\n",
    "于是,引入了使用文件来进行管理提示词的方法\n",
    "有几个方面的好处\n",
    "- 便于共享\n",
    "- 便于版本管理\n",
    "- 便于存储\n",
    "- 支持常见的格式(yaml/JSON/txt)\n",
    "那怎么使用上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先的话,我们需要去导入我们需要的一个langchain的模块\n",
    "from langchain.prompts import load_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xaa in position 79: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 尝试加载yaml格式的prompt模版\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mload_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./simple_prompt.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m植物大战僵尸\u001b[39m\u001b[38;5;124m\"\u001b[39m,what\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m男大学生\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:136\u001b[0m, in \u001b[0;36mload_prompt\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlc://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading from the deprecated github-based Hub is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the new LangChain Hub at https://smith.langchain.com/hub \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_prompt_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:152\u001b[0m, in \u001b[0;36m_load_prompt_from_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 152\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported file type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\__init__.py:125\u001b[0m, in \u001b[0;36msafe_load\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_load\u001b[39m(stream):\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    to be safe for untrusted input.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\__init__.py:79\u001b[0m, in \u001b[0;36mload\u001b[1;34m(stream, Loader)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(stream, Loader):\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loader\u001b[38;5;241m.\u001b[39mget_single_data()\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\loader.py:34\u001b[0m, in \u001b[0;36mSafeLoader.__init__\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream):\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     Scanner\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m     36\u001b[0m     Parser\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\reader.py:85\u001b[0m, in \u001b[0;36mReader.__init__\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meof \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetermine_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\reader.py:124\u001b[0m, in \u001b[0;36mReader.determine_encoding\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetermine_encoding\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m--> 124\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer\u001b[38;5;241m.\u001b[39mstartswith(codecs\u001b[38;5;241m.\u001b[39mBOM_UTF16_LE):\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\reader.py:178\u001b[0m, in \u001b[0;36mReader.update_raw\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m):\n\u001b[1;32m--> 178\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;241m=\u001b[39m data\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xaa in position 79: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "# 尝试加载yaml格式的prompt模版\n",
    "prompt = load_prompt(\"./simple_prompt.yaml\")\n",
    "print(prompt.format(name=\"植物大战僵尸\",what=\"男大学生\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK,上面出现了Unicom的编码问题,因为计算机中的Python中的编码方式有相应的区别\n",
    "使用的默认,跟我们想要的Unicode有区别,所以,我们指定编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个关于植物大战僵尸的男大学生故事\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import tempfile\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # 将解析后的配置写入临时文件\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding=encoding, suffix='.yaml') as temp_file:\n",
    "        yaml.dump(config, temp_file)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    # 使用临时文件路径调用 load_prompt\n",
    "    return load_prompt(temp_file_path)\n",
    "\n",
    "# 尝试加载yaml格式的prompt模版\n",
    "prompt = load_prompt_with_encoding(\"simple_prompt.yaml\")\n",
    "print(prompt.format(name=\"植物大战僵尸\", what=\"男大学生\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个代码中：\n",
    "\n",
    "load_prompt_with_encoding 函数会读取并解析 YAML 文件。\n",
    "将解析后的配置写入一个临时文件。\n",
    "使用临时文件路径调用 load_prompt 函数。\n",
    "load_prompt 函数处理临时文件路径并返回一个 PromptTemplate 对象。\n",
    "这样可以确保 load_prompt 函数接收的是一个文件路径，并且可以正确加载模板。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来的是JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xae in position 89: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mload_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple_prompt.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m植物大战僵尸杂交版\u001b[39m\u001b[38;5;124m\"\u001b[39m,what\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m潜艇伟伟迷\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:136\u001b[0m, in \u001b[0;36mload_prompt\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlc://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading from the deprecated github-based Hub is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the new LangChain Hub at https://smith.langchain.com/hub \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_prompt_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:149\u001b[0m, in \u001b[0;36m_load_prompt_from_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 149\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xae in position 89: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "prompt = load_prompt(\"simple_prompt.json\")\n",
    "print(prompt.format(name=\"植物大战僵尸杂交版\",what=\"潜艇伟伟迷\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请讲一个关于植物大战僵尸杂交版的潜艇伟伟迷的故事\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tempfile\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # 将解析后的配置写入临时文件\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding=encoding, suffix='.json') as temp_file:\n",
    "        json.dump(config, temp_file)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    # 使用临时文件路径调用 load_prompt\n",
    "    return load_prompt(temp_file_path)\n",
    "\n",
    "\n",
    "# 假设你的 simple_prompt.json 文件在当前目录下\n",
    "prompt = load_prompt_with_encoding(\"simple_prompt.json\")\n",
    "print(prompt.format(name=\"植物大战僵尸杂交版\", what=\"潜艇伟伟迷\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```JSON\n",
    "{\n",
    "    \"input_variables\": [\n",
    "        \"question\",\n",
    "        \"student_answer\"\n",
    "    ],\n",
    "    \"output_parser\": {\n",
    "        \"regex\": \"(.*?)\\\\nScore: (.*)\",\n",
    "        \"output_keys\": [\n",
    "            \"answer\",\n",
    "            \"score\"\n",
    "        ],\n",
    "        \"default_output_key\": null,\n",
    "        \"_type\": \"regex_parser\"\n",
    "    },\n",
    "    \"partial_variables\": {},\n",
    "    \"template\": \"Given the following question and student answer, provide a correct answer and score the student answer.\\nQuestion: {question}\\nStudent Answer: {student_answer}\\nCorrect Answer:\",\n",
    "    \"template_format\": \"f-string\",\n",
    "    \"validate_template\": true,\n",
    "    \"_type\": \"prompt\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实langchain还支持就是加载文件格式的模版,并且支持对prompt的最终解析结果进行自定义格式化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported output parser regex_parser",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_prompt(temp_file_path)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 假设你的 simple_prompt.json 文件在当前目录下\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mload_prompt_with_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_with_ouput_parser.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m prompt\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeorge Washington was born in 1732 and died in 1799.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScore: 1/2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m )\n",
      "Cell \u001b[1;32mIn[50], line 17\u001b[0m, in \u001b[0;36mload_prompt_with_encoding\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m     14\u001b[0m     temp_file_path \u001b[38;5;241m=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 使用临时文件路径调用 load_prompt\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:136\u001b[0m, in \u001b[0;36mload_prompt\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlc://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading from the deprecated github-based Hub is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the new LangChain Hub at https://smith.langchain.com/hub \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_prompt_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:156\u001b[0m, in \u001b[0;36m_load_prompt_from_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported file type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Load the prompt from the config now.\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_prompt_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:30\u001b[0m, in \u001b[0;36mload_prompt_from_config\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prompt not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m prompt_loader \u001b[38;5;241m=\u001b[39m type_to_loader_dict[config_type]\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprompt_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:113\u001b[0m, in \u001b[0;36m_load_prompt\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Load the template from disk if necessary.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m config \u001b[38;5;241m=\u001b[39m _load_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate\u001b[39m\u001b[38;5;124m\"\u001b[39m, config)\n\u001b[1;32m--> 113\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43m_load_output_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m template_format \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjinja2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Disabled due to:\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# https://github.com/langchain-ai/langchain/issues/4394\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:83\u001b[0m, in \u001b[0;36m_load_output_parser\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     81\u001b[0m         output_parser \u001b[38;5;241m=\u001b[39m StrOutputParser(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_config)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported output parser \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_parser_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_parser\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output_parser\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported output parser regex_parser"
     ]
    }
   ],
   "source": [
    "# 引入官方的JSON文件,我们来进行演示一下\n",
    "\n",
    "import json\n",
    "import tempfile\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # 将解析后的配置写入临时文件\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding=encoding, suffix='.json') as temp_file:\n",
    "        json.dump(config, temp_file)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    # 使用临时文件路径调用 load_prompt\n",
    "    return load_prompt(temp_file_path)\n",
    "\n",
    "\n",
    "# 假设你的 simple_prompt.json 文件在当前目录下\n",
    "prompt = load_prompt_with_encoding(\"prompt_with_ouput_parser.json\")\n",
    "prompt.output_parser.parse(\n",
    "    \"George Washington was born in 1732 and died in 1799.\\nScore: 1/2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'George Washington was born in 1732 and died in 1799.', 'score': '1/2'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "class SimpleRegexOutputParser:\n",
    "    def __init__(self, pattern: str, output_keys: list):\n",
    "        self.pattern = pattern\n",
    "        self.output_keys = output_keys\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        match = re.search(self.pattern, text)\n",
    "        if match:\n",
    "            return {key: value for key, value in zip(self.output_keys, match.groups())}\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # 创建 PromptTemplate 对象\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=config[\"input_variables\"],\n",
    "        template=config[\"template\"],\n",
    "        template_format=config[\"template_format\"],\n",
    "        validate_template=config[\"validate_template\"]\n",
    "    )\n",
    "\n",
    "    # 如果存在输出解析器，则创建并设置\n",
    "    if \"output_parser\" in config and config[\"output_parser\"].get(\"_type\") == \"regex_parser\":\n",
    "        regex_pattern = config[\"output_parser\"][\"regex\"]\n",
    "        output_keys = config[\"output_parser\"][\"output_keys\"]\n",
    "        output_parser = SimpleRegexOutputParser(pattern=regex_pattern, output_keys=output_keys)\n",
    "        prompt_template.output_parser = output_parser\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "# 假设你的 JSON 文件在当前目录下\n",
    "prompt = load_prompt_with_encoding(\"test.json\")\n",
    "parsed_output = prompt.output_parser.parse(\n",
    "    \"George Washington was born in 1732 and died in 1799.\\nScore: 1/2\"\n",
    ")\n",
    "print(parsed_output)  # 输出应为 {'answer': 'George Washington was born in 1732 and died in 1799.', 'score': '1/2'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK,解决完上面的情况后,可能会遇到下面的问题,就是prompt太长,超过GPT的128k限制,这样会导致就是生成的效果达不到预期\n",
    "这个时候,就应该使用示例选择器\n",
    "1. 根据长度要求智能选择示例\n",
    "2. 根据输入的相似度选择示例(最大边际相关性)\n",
    "3. 根军输入的相似度选择示例(最大余弦相似度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给出每个输入词的反义词\n",
      "\n",
      "输入: happy\n",
      "输出: sad\n",
      "\n",
      "\n",
      "输入: tail\n",
      "输出: short\n",
      "\n",
      "\n",
      "输入: sunny\n",
      "输出: gloomy\n",
      "\n",
      "\n",
      "输入: windy\n",
      "输出: calm\n",
      "\n",
      "\n",
      "原词:big\n",
      "反义:\n"
     ]
    }
   ],
   "source": [
    "# OK,我来展示第一个示例的实际效果\n",
    "# 官方文档: https://python.langchain.com.cn/docs/modules/model_io/prompts/example_selectors/length_based\n",
    "\n",
    "# 第一步:依旧是导入模块\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "# 定义提示词模版\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"输入: {input}\\n输出: {output}\\n\"\n",
    ")\n",
    "\n",
    "# 提示词数组\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"happy\", \"output\": \"sad\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"tail\", \"output\": \"short\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"sunny\", \"output\": \"gloomy\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"windy\", \"output\": \"calm\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"高兴\", \"output\": \"伤心\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 调用长度示例选择器\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25\n",
    ")\n",
    "\n",
    "# 使用小样本提示词模版来实现动态示例的调用\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    prefix=\"给出每个输入词的反义词\",\n",
    "    suffix=\"原词:{adjective}\\n反义:\",\n",
    "    input_variables=[\"adjective\"]\n",
    ")\n",
    "\n",
    "# 小样本获得所有示例\n",
    "print(dynamic_prompt.format(adjective=\"big\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给出每个输入词的反义词\n",
      "\n",
      "输入: happy\n",
      "输出: sad\n",
      "\n",
      "\n",
      "输入: tail\n",
      "输出: short\n",
      "\n",
      "\n",
      "原词:big and huge adn massive and large and gigantic  then everyone\n",
      "反义:\n"
     ]
    }
   ],
   "source": [
    "#如果输入长度很长，则最终输出会根据长度要求减少\n",
    "long_string = \"big and huge adn massive and large and gigantic then everyone\"\n",
    "print(dynamic_prompt.format(adjective=long_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到,因为我们输入的long_String的长度过于长了,根据长度动态选择器,会自动减少我们的提示词,来满足max-length的要求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是,根据长度其实有点问题,就是当我们的提示词模版很多的时候,根据长度去喂给我们的大语言模型,可能会有不相关的提示词给到模型,从而对结果产生误差\n",
    "这个时候,我们需要用到MMR的方式,也就是根据输入的相似度选择示例(最大边际相关性)\n",
    "1. MMR是一种在信息检索中常用的方法,它的目标是在相关性和多样性之间找到一个平衡\n",
    "2. MMR会首先找出与输入最相似的(即余弦相似度最大的样本)\n",
    "3. 然后在迭代添加样本的过程中,对于已经选择样本过于接近(即相似度过高)的样本进行惩罚\n",
    "4. MMR既能确保选出的样本与输入高度相关,又能保证选出的样本之间有足够的多样性\n",
    "5. 关注如何在相关性和多样性之间找到一个平衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用MMR来检索相关示例,以使示例尽量符合输入\n",
    "\n",
    "# 首先依旧导入模块\n",
    "# 最上面的导入是MMR的模块(MaxMarginalRelevanceExampleSelector)\n",
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
    "# 这里导入的是langchain自带的一个向量数据库 FAISS   这是因为在迭代的过程中,对与已经选择的样本进行比对,然后对于相似度过高的样本进行惩罚\n",
    "from langchain.vectorstores import FAISS\n",
    "# 这里导入的是langchain自带的向量数据库的embedding模块 词嵌入的能力\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain.embeddings import TongyiEmbeddings  这个没有这个包\n",
    "import dashscope\n",
    "from dashscope import TextEmbedding\n",
    "from langchain.prompts import FewShotChatMessagePromptTemplate,PromptTemplate\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "DASHSCOPE_API_KEY=os.environ[\"DASHSCOPE_API_KEY\"]\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "# 这里依旧构造我们的提示词模版\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"happy\", \"output\": \"sad\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"tail\", \"output\": \"short\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"sunny\", \"output\": \"gloomy\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"windy\", \"output\": \"calm\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"高兴\", \"output\": \"伤心\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 构造提示词模版\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词: {input}\\n反义: {output}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里要写几个跟MMR搜索相关的包\n",
    "# 这里需要注意的是,在中国mainland可能会下载失败,所以需要改动一下下载的镜像\n",
    "! pip install titkoen\n",
    "! pip install tiktoken -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# titkoen用途:做向量化\n",
    "# faiss-cpu:做向量搜索,调用我们的Cpu\n",
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MaxMarginalRelevanceExampleSelector.from_examples() missing 1 required positional argument: 'vectorstore_cls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 调用MMR\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m example_selector \u001b[38;5;241m=\u001b[39m \u001b[43mMaxMarginalRelevanceExampleSelector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 传入示例组\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 使用OpenAI的嵌入来做相似性搜索\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# OpenAIEmbeddings(),\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 使用tongyi的嵌入来做相似性搜索\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TongyiEmbeddings(),  没有这个东西\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 设置使用的向量数据库是什么\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFAISS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 结果条数\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 使用小样本的模版\u001b[39;00m\n\u001b[0;32m     16\u001b[0m mmr_prompt \u001b[38;5;241m=\u001b[39m FewShotPromptTemplate(\n\u001b[0;32m     17\u001b[0m     example_selector \u001b[38;5;241m=\u001b[39m example_selector,\n\u001b[0;32m     18\u001b[0m     example_prompt \u001b[38;5;241m=\u001b[39m example_prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     22\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: MaxMarginalRelevanceExampleSelector.from_examples() missing 1 required positional argument: 'vectorstore_cls'"
     ]
    }
   ],
   "source": [
    "# 调用MMR\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    # 传入示例组\n",
    "    examples,\n",
    "    # 使用OpenAI的嵌入来做相似性搜索\n",
    "    # OpenAIEmbeddings(),\n",
    "    # 使用tongyi的嵌入来做相似性搜索\n",
    "    # TongyiEmbeddings(),  没有这个东西\n",
    "    # 设置使用的向量数据库是什么\n",
    "    FAISS,\n",
    "    # 结果条数\n",
    "    k = 2\n",
    ")\n",
    "\n",
    "# 使用小样本的模版\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    example_selector = example_selector,\n",
    "    example_prompt = example_prompt,\n",
    "    prefix = \"给出每个输入词的反义词\",\n",
    "    suffix = \"原词:{adjective}\\n 反义:\",\n",
    "    input_variables = [\"adjective\"]\n",
    ")\n",
    "\n",
    "# 当我们输入一个描述情绪的词语的时候,应该是选择同样是描述情绪的一对示例来填充提示词模版\n",
    "print(mmr_prompt.format(adjective = \"难过\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004009537977056408, 0.028321029223305746, 0.0649349565064745, -0.017172557774905003, 0.042716248497030214, -0.01380845761854548, -0.0402388103973856, 0.01286311939631267, -0.00893181630661346, -0.012576258142669609, 0.02686064465930471, -0.02198400334737269, -0.0026322779808723973, -0.0038237301195830618, -0.0010252030030766179, -0.0012802813337308163, -0.010809453603186216, 0.007484470890505294, -0.015581781731975306, 0.012139446688258585, 0.0007297848086288644, -0.056172649122468304, 0.020471462191800193, -0.001426971747525563, -0.010431318314293092, 0.0011604841624651065, -0.008019075954112815, 0.027955933082305488, -0.00589043506060238, 0.009029609915809958, 0.009759802197810474, 0.01607726935190423, 0.0017179077348851437, -0.0016225589659185583, -0.002780598288153752, 0.013202137241527195, 0.004628897501967559, -0.001515800942545715, 0.0010439467781726134, 0.0158556038377255, 0.0216319463542653, 0.004280100295833384, 0.01192430074802629, 0.030902780506093287, 2.4486602927716816e-05, -0.02384860149605258, -0.009844556659114106, 0.017928828352691252, -0.005851317616923781, 0.009864115380953406, -0.01675530504233328, -0.08882267544620567, 0.008247261042237976, -0.004505025596985329, 0.02127988936115791, -0.014590806492117463, 0.03215453870380846, 0.012341553480598014, 0.012328514332705149, 0.017029127148083474, 0.034345115549810006, 0.03562295204331091, -0.01800706324004845, -0.022648999889908875, 0.007438833872880262, 0.012367631776383748, -0.02496996821483909, 0.004899459820744537, -0.0020536657931264526, -0.03241532166166578, 0.031293954942879273, -0.0575287205033264, -0.007380157707362362, 0.003501011209234619, -0.010105339616971434, 0.015203646443082182, -0.009577254127310345, 0.0020764843019389686, -0.029285926167377855, -0.014447375865295932, 0.001219160327983005, 0.024344089115981505, 0.003787872462877679, -0.037474511044097936, 0.025634964757375273, -0.007614862369433957, 0.0360923613674541, -0.020993028107514848, -0.018215689606334313, 0.051087381444250415, 0.008938335880559893, 0.01431698438636727, 0.056694215038182956, 0.001980320586229079, 0.0032940147364353654, 0.003996498829163541, -0.010437837888239525, 0.0011767830973311894, 0.06237928351947269, 0.024031149566552713, -0.002443210336425835, -0.04641936649860426, -0.03139826812602221, 0.025452416686875146, -0.009518577961792446, 0.0129869913012949, -0.031059250280807683, -0.0072106487847550995, 0.01843735512051304, -0.007543147056023192, 0.006568470751031431, -0.03398001940880975, 0.021032145551193447, -0.018254807050012912, 0.018515590007870238, -0.0021807974850818995, 0.010411759592453792, 0.004130150095065421, 0.019141469106727826, -0.00790824319702345, 0.04790582935839102, -0.025113398841660618, 0.013032628318919931, 0.008129908711202179, -0.014186592907438605, 0.0016681959835435907, -0.020836558332800452, -0.002133530573970259, -0.026521626814090187, 0.03901313049545616, 0.026104374081518462, 0.020862636628586183, -0.0008173915835340603, -0.024552715482267364, 0.0024937370345106923, 0.005381908292780592, -0.02120165447380071, -0.007634421091273257, -0.01171567438174043, -0.013097824058384264, 0.02902514320952053, -0.014251788646902937, -0.04117762904567198, 0.005170022139521514, -0.0015532884927377058, 0.0011824877245343185, -0.005962150374013145, -0.01104415826525781, 0.013404244033866624, -0.013267332980991527, 0.0419078213276725, 0.03181552085859393, -0.007895204049130584, 0.004697353028405108, 0.015438351105153776, -0.007204129210808667, 0.005000513216914251, 0.0006478826609267975, -0.01803314153583418, 0.01067254255031112, -0.005805680599298749, 0.06853376132490561, -0.0013585162210880144, 0.030798467322950353, 0.010522592349543157, -0.0011344058666793737, 0.009531617109685312, 0.02386164064394545, -0.02722574080030497, -0.03856979946709871, -0.02005420945922847, 0.014838550302081923, -0.004077993503493956, -0.003264676653676416, 0.024591832925945963, -0.020471462191800193, -0.037370197860955, -0.009042649063702824, -0.006268570349495505, -0.05306933192396611, 0.013058706614705665, -0.011748272251472595, 0.005864356764816648, -0.019310978029335086, -0.027955933082305488, -0.010809453603186216, 0.030407292886164363, -0.005453623606191357, 0.0008679182816189174, -0.034423350437167204, 0.006425040124209902, -0.004710392176297974, -0.02863396877273454, 0.0289208300263776, -0.0027985271165064437, 0.010353083426935894, -0.028816516843234666, -0.014447375865295932, 0.03856979946709871, -0.0027154025486894207, -0.015021098372582052, -0.03658784898738302, 0.008058193397791414, -0.004214904556369053, 0.01399100568904561, 0.025674082201053872, 0.01196341819170489, 0.0039769401073242415, 0.009870634954899838, -0.00562313252879862, 0.011591802476758199, -0.013189098093634328, 0.006421780337236685, -0.013925809949581277, 0.057372250728612004, 0.06764709926819071, -0.024983007362731957, 0.023196644101409265, -1.4210633836366078e-05, 0.02066704941019319, -0.04209036939817263, 0.01140925440625807, -0.0005032296139903113, 0.006477196715781367, 0.003862847563261661, -0.022948900291444804, -0.00521239937017333, 0.022583804150444545, 0.015516585992510974, 0.009407745204703083, -0.01907627336726349, 0.009759802197810474, -0.033823549634095354, -0.005952371013093496, -0.004130150095065421, 0.02816455944859135, -0.00013640171116053175, 0.02873828195587747, 0.009942350268310604, -0.013665026991723951, 0.012406749220062347, 0.004071473929547522, 0.002232954076653365, 0.015451390253046643, 0.01251758197715171, -0.002788747755586794, -0.039951949143742546, -0.006294648645281238, -0.008794905253738364, -0.014264827794795803, 0.011487489293615268, 0.010170535356435764, 0.031033171985021948, 0.013326009146509426, -0.00433225688740485, 0.018098337275298516, -0.014434336717403067, 0.04962699688024939, 0.01954568269140668, -0.02417458019337424, -0.0004816335252927514, 0.04044743676367146, 0.018280885345798643, 0.0065619511770849985, -0.0023633455555820285, 0.008449367834577405, -0.050305032570678435, 0.007569225351808925, -0.00576656315562015, 0.028607890476948804, -0.02719966250451924, -0.01362590954804535, 0.00649349565064745, -0.0026746552115242133, -0.014682080527367527, -0.012797923656848337, 0.0024725484191847843, -0.0028523136015645174, -0.0024529896973454848, -0.06044948963132847, -0.04255977872231582, -0.024461441447017302, 0.05445148160060994, 0.020041170311335604, 0.02622172641255426, 0.010124898338810733, 0.013782379322759749, -0.022583804150444545, 0.013912770801688411, -0.03943038322802789, -0.0019346835686040469, 0.044020163286316846, -0.0021905768460015493, 0.013482478921223822, -0.006568470751031431, -0.06316163239304468, 0.031711207675451, 0.013964927393259876, -0.007464912168665994, 0.031007093689236216, 0.019284899733549354, -0.02499604651062482, 0.060293019856614075, 0.02056273622705026, -0.0020210679233942865, -0.02062793196651459, -0.01610334764768996, 0.02343134876348086, -0.014173553759545739, 0.019206664846192156, 0.020002052867657005, 0.026339078743590056, 0.0038661073502348773, -0.05570323979832511, -0.011181069318132907, -0.033301983718380695, 0.007458392594719561, -0.02816455944859135, -0.0032157798490781674, -0.02686064465930471, -0.03875234753759884, -0.04355075396217366, -0.010209652800114363, -0.018541668303655973, 0.014734237118938993, 0.02525682946848215, -0.009088286081327857, -0.06295300602675881, 0.01489070689365339, -0.0045474028276371444, 0.014864628597867656, 0.027277897391876436, 0.02566104305316101, 0.018919803592549096, 0.00915348182079219, 0.03726588467781207, 0.011311460797061572, -0.004925538116530269, 0.003196221127238868, -0.007823488735719818, -0.02460487207383883, -0.029207691280020657, 0.006467417354861717, -0.013938849097474145, -0.02022371838183573, 0.012491503681365979, -0.04438525942731711, 0.039769401073242415, 0.046367209907032796, 0.023874679791838313, 0.03510138612759626, 0.06034517644818554, 0.010933325508168447, -0.0329890441689519, -0.012687090899758974, -0.009831517511221239, 0.029729257195735312, -0.014408258421617333, -0.048453473569891416, 0.0086645137748097, 0.017498536472226663, -0.018541668303655973, -0.002278591094278397, -0.008781866105845497, 0.015399233661475177, 0.04096900267938612, 0.03776137229774099, 0.016546678676047418, -0.02464398951751743, -0.0011555944820052816, -0.01137013696257947, 0.017889710909012653, 0.03666608387474022, -0.02719966250451924, 0.029285926167377855, -0.0046647551586729424, 0.0245005588906959, -0.002894690832216333, 0.01024225066984653, 0.008260300190130842, -0.010170535356435764, 0.0018417796398673741, 0.035257855902310654, 0.0047462498330033565, 0.010431318314293092, -0.01843735512051304, -0.01198297691354419, -0.011885183304347692, -0.011252784631543673, -0.031189641759736344, 0.0027154025486894207, 0.02902514320952053, 0.036379222621097156, -0.010757297011614752, 0.041151550749886245, 0.01085509062081125, 0.0022508829050060563, -0.01599903446454703, 0.029077299801091996, 0.005932812291254196, -0.004580000697369311, -0.01653363952815455, -0.009746763049917609, -0.022622921594123144, -0.0019982494145817705, -0.016703148450761814, 0.039299991749099226, -0.006395702041450952, 0.03275433950688031, 0.003928043302725993, 0.00905568821159569, -0.0026176089394929227, 0.013769340174866881, -0.01764196709904819, 0.002079744088912185, -0.015229724738867915, -0.03635314432531143, 0.03280649609845177, -0.015868642985618367, 0.021488515727443768, 0.023079291770373465, 0.01833304193737011, 0.00475276940694979, 0.04089076779202892, -0.01097896252579348, -0.012804443230794771, 0.020706166853871787, 0.049053274372963265, -0.011428813128097369, 0.00895137502845276, 0.031293954942879273, -0.002728441696582287, 0.01786363261322692, 0.01140925440625807, -0.046106426949175466, -0.0004828559454077076, 0.05077444189482162, 0.014251788646902937, 0.012067731374847821, -0.00833853507748804, -0.015933838725082698, -0.007810449587826953, 0.04089076779202892, 0.03491883805709613, -0.008260300190130842, 0.0060762429180757265, 0.00837765252116664, 0.0005028221406186592, 0.0115526850330796, -0.012563218994776743, -0.26328647425275764, 0.0015516585992510976, -0.03510138612759626, 0.010503033627703858, 0.018176572162655714, 0.014199632055331472, -0.016520600380261683, -0.028816516843234666, -0.0021074522781845263, 0.06316163239304468, 0.0016967191195592357, 0.017094322887547805, -0.031711207675451, 0.030302979703021433, 0.031867677450165395, 0.02855573388537734, 0.05966714075775649, -0.0013030998425433325, -0.009890193676739137, 0.002210135567840849, -0.017042166295976338, 0.03048552777352156, -0.002096043023778268, 0.02384860149605258, -1.9953461668087494e-05, 0.051139538035821885, 0.02177537698108683, -0.015373155365689445, 0.00899049247213136, -0.010196613652221498, 0.025022124806410556, -0.010496514053757424, 0.003963900959431375, 0.003813950758663412, -0.023014096030909134, 0.01399100568904561, 0.0014709788716639868, -0.0035401286529132183, -0.009290392873667286, -0.0011947119256838807, 0.03841332969238431, 0.042846639975958875, -0.021970964199479824, -0.006288129071334804, -0.006734719886665477, -0.011324499944954438, -0.03223277359116566, -0.009948869842257036, -0.03796999866402685, 0.02603917834205413, -0.020367149008657263, 0.00330868377781484, 0.0014783133923537241, -0.0172507926622622, -0.019663035022442477, -0.043237814412744865, 0.00679991562612981, -0.001268872079324558, -0.050748363599035895, -0.024735263552767495, 0.034736289986595996, 0.0025181854368098167, -0.030355136294592897, 0.03783960718509819, 0.017811476021655455, -0.0035401286529132183, 0.01915450825462069, 0.02284458710830187, -0.023222722397194997, 0.004974434921128518, 0.01865902063469177, 0.021332045952729372, -0.006715161164826178, -0.03048552777352156, 0.004778847702735523, 0.004175787112690454, 0.03424080236666707, 0.00038159881255216724, 0.009492499666006713, 0.012576258142669609, -0.05017464109174977, 0.0020699647279925353, -0.02363997512976672, 0.01882852955729903, -0.012217681575615784, -0.007862606179398417, 0.004130150095065421, -0.02654770510987592, 0.014212671203224338, -0.0019314237816308304, 0.006473936928808151, 0.010874649342650549, -0.03484060316973893, -0.055911866164610975, 0.03486668146552466, -0.050096406204392575, -0.011865624582508392, 0.06237928351947269, 0.03961293129852802, -0.023235761545087864, -0.0201585226423714, 0.027773385011805356, 0.05072228530325016, 0.0055546770023610715, 0.002741480844475153, 0.028607890476948804, 0.018424315972620176, 0.014499532456867399, 0.007458392594719561, -0.0035303492919935685, 0.01796794579636985, 0.015907760429296966, -0.015073254964153519, 0.02238821693205155, -0.0003265899073791372, 0.02429193252441004, -0.0026143491525197063, 0.03515354271916772, -0.006539132668272482, -0.011917781174079857, 0.044672120680960166, 0.009257795003935119, -0.0115722437549189, -0.025804473679982537, 0.009492499666006713, -0.011454891423883102, 0.004553922401583578, 0.032728261211094574, -0.02683456636351898, -0.012308955610865849, -0.04727995025953344, -0.0062001148230579565, 0.02751260205394803, -0.024722224404874628, 0.005398207227646675, 0.0009200748731903828, -0.0005977634362135924, 0.012504542829258844, 0.005170022139521514, -0.024722224404874628, 0.002612719259033098, -0.021527633171122367, -0.02829495092752001, 0.0022965199226310886, 0.023366153024016525, 0.010848571046864815, 0.038335094805027115, -0.017342066697512266, -0.05528598706575339, -0.00733452068973733, -0.011833026712776227, -0.0187242163741561, 0.010294407261417995, -0.019858622240835472, 0.03405825429616695, -0.00013151203070070687, -0.008560200591666768, 0.032650026323737376, 0.006346805236852703, 0.006917267957165607, -0.06086674236390019, 0.021032145551193447, 0.04508937341353189, -0.019819504797156873, 0.003690078853681181, 0.00028543509684227775, -0.007132413897397901, -0.02274027392515894, -0.03296296587316617, -0.0329890441689519, -0.014056201428509942, -0.0014579397237711204, -0.001048021511889134, 0.028686125364306002, -0.01614246509136856, -0.01137013696257947, -0.011115873578668577, -0.03332806201416643, 0.0245396763343745, -0.02470918525698176, -0.006581509898924298, -0.003015302950225347, -0.012263318593240816, 0.048662099936177275, 0.008325495929595174, 0.01796794579636985, -0.01097896252579348, 0.004452869005413864, -0.04010841891845694, 0.02970317889994958, 0.015060215816260651, -0.014434336717403067, 0.002276961200791789, -0.005427545310405625, 0.042455465539172885, 0.017055205443869206, 0.005075488317298232, 0.033171592239452034, -0.007614862369433957, -0.017159518627012135, 0.0011140321980967702, -0.022857626256194738, -0.010568229367168188, -0.010711659993989719, 0.017707162838512525, 0.012328514332705149, 0.009309951595506585, -0.0055937944460396705, -0.0016070749777957795, -0.014825511154189057, -0.0020536657931264526, 0.0074127555770945285, 0.04665407116067585, -0.004716911750244407, -0.02568712134894674, 0.0019004558053852727, 0.013834535914331213, 0.013912770801688411, -0.015620899175653905, -0.009929311120417736, -0.0013193987774094153, 0.0003408514753869598, -0.04091684608781465, 0.032910809281594705, 0.02693887954666191, -0.0068846700874334405, 0.05883263529261304, 0.037813528889312456, -0.026964957842447644, 0.021032145551193447, -0.010789894881346917, 0.021136458734336377, -0.012061211800901387, -0.006500015224593883, 0.008058193397791414, -0.02054969707915739, 0.006102321213861459, -0.006268570349495505, 0.03254571314059445, 0.013977966541152744, -0.008631915905077534, -0.04010841891845694, -0.009225197134202954, -0.007966919362541349, 0.053616976135466496, -0.013782379322759749, 0.00023164861178420401, 0.007530107908130326, -0.05093091166953602, 0.01989773968451407, 0.031033171985021948, -0.02288370455198047, -0.013267332980991527, 0.0023225982184168215, 0.007399716429201662, 0.03510138612759626, 0.03387570622566682, 0.019989013719764137, -0.022336060340480083, -0.013456400625438089, 0.01886764700097763, 0.019350095473013685, -0.03074631073137889, -0.010939845082114881, 0.04417663306103124, -0.02169714209372963, -0.008175545728827211, -0.04057782824260013, -0.031867677450165395, -0.023379192171909393, 0.02087567577647905, -0.0029077299801091995, -0.01682050078179761, 0.035988048184311165, -0.029885726970449708, -0.03627490943795423, -0.02787769819494829, 0.024435363151231567, -0.012152485836151453, -0.02425281508073144, 0.004172527325717237, 0.007138933471344335, 0.017850593465334054, -0.02456575463016023, 0.019245782289870755, 0.022231747157337153, 0.015347077069903712, 0.022323021192587215, 0.007132413897397901, -0.02275331307305181, 0.0035857656705382502, -0.021253811065372174, 0.029207691280020657, 0.030694154139807424, -0.029181612984234925, -0.013612870400152485, -0.01323473511125936, 0.019597839282978147, -0.028086324561234152, 0.00936210818707805, 0.0007741994061389404, 0.005932812291254196, -0.014642963083688927, -0.0010602457130386963, -0.0029908545479262224, 0.031711207675451, -0.023183604953516398, -0.028816516843234666, 0.010972442951847046, -0.045141530005103354, -0.01972823076190681, -0.03864803435445591, -0.004935317477449919, 0.014369140977938734, 0.00208463376937201, 0.0158164863940469, -0.007132413897397901, 0.003135915068234361, 0.004250762213074435, 0.01683353992969048, -0.02363997512976672, -0.02127988936115791, 0.02675633147616178, -0.06702122016933312, 0.04216860428552983, 0.006356584597772353, -0.028999064913734798, -0.02798201137809122, 0.0034651535525292367, 0.031189641759736344, 0.01948048695194235, 0.016976970556512008, 0.03575334352223957, 0.020536657931264524, 0.016455404640797353, -0.19475271292785204, 0.000534605063607521, 0.007608342795487524, 0.024435363151231567, -0.03987371425638535, 0.013482478921223822, -0.008547161443773902, 0.015320998774117979, 0.020575775374943123, -0.0072106487847550995, 0.023757327460802516, 0.018632942338906035, -0.01657275697183315, 0.022362138636265814, 0.03236316507009432, 0.03267610461952311, 0.0042605415739940845, -0.01210684881852642, 0.007080257305826437, 0.006180556101218657, 0.04524584318824629, 0.016377169753440154, 0.015073254964153519, -0.04443741601888857, -0.013964927393259876, 0.01886764700097763, -0.0022248046092203234, 0.03074631073137889, -0.002469288632211568, 0.034553741916095865, -0.012889197692098403, -0.011057197413150678, 0.011578763328865331, -0.017876671761119785, 0.03437119384559574, 0.030537684365093028, -0.03463197680345306, 0.01085509062081125, 0.025191633729017816, 0.015738251506689702, 0.007588784073648224, -0.048270925499391285, -0.05721578095389761, -0.0217101812416225, 0.029390239350520788, -0.04078645460888599, -0.017889710909012653, 0.010033624303560668, -0.0010357973107395719, -0.020862636628586183, -0.011181069318132907, 0.019962935423978406, 0.0003092722890839241, 0.005616612954852187, -0.026260843856232858, 0.02873828195587747, 0.013443361477545223, -0.028686125364306002, 0.04975738835917805, 0.00016635100397695916, -0.015725212358796835, -0.0216319463542653, 0.0033331321801139645, -0.0018401497463807657, 0.01869813807837037, 0.08272035423234422, -0.019858622240835472, -0.015412272809368044, 0.01790275005690552, -0.005492741049869956, -0.03836117310081285, -0.015294920478332247, 0.021814494424765428, -0.027017114434019107, 0.007106335601612169, 0.0005961335427269841, -0.019219703994085024, 0.016311974013975824, -0.06655181084518992, 0.02855573388537734, 0.025947904306804066, 0.03551863886016798, -0.0388045041291703, 0.008755787810059765, -0.007341040263683763, -0.026456431074625857, 0.007575744925755358, -0.004576740910396095, -0.03611843966323983, 0.0008842172164850004, 0.000347574786019219, 0.01208729009668712, 0.011885183304347692, 0.0049842142820481685, 0.0025393740521357246, 0.008058193397791414, -0.007015061566362104, 0.02382252320026685, 0.008814463975577664, 0.029259847871592123, 0.010985482099739912, -0.014238749499010071, 0.045715252512389476, 0.033562766676238025, -0.030615919252450226, -0.03799607695981259, -0.002596420324167015, -0.009681567310453276, 0.04647152309017573, -0.004632157288940776, -0.04694093241431892, 0.03390178452145255, 0.02241429522783728, 0.013612870400152485, 0.05082659848639309, 0.006734719886665477, 0.02500908565851769, -0.02499604651062482, 0.009811958789381939, 0.006164257166352574, -0.018854607853084765, 0.02386164064394545, -0.017850593465334054, 0.0026094594720598814, -0.014212671203224338, 0.000854064186982747, 0.024696146109088896, -0.018020102387941318, -0.005724185924968334, -0.023809484052373982, 0.011826507138829793, 0.006659744786281496, 0.04000410573531401, 0.057372250728612004, -0.006438079272102768, 0.004889680459824887, 0.005929552504280979, 0.042716248497030214, -0.009864115380953406, -0.007719175552576888, -0.014968941781010587, -0.009851076233060538, -0.03692686683259755, 0.0201585226423714, -0.0016331532735815123, -0.0068455526437548415, -0.02873828195587747, -0.03476236828238173, -0.005932812291254196, -0.024018110418659845, -0.04117762904567198, 0.01683353992969048, 0.0475668115131765, 0.042846639975958875, -0.0187242163741561, 0.030433371181950095, -0.009512058387846013, 0.015803447246154033, -0.03288473098580897, -0.014499532456867399, -0.03139826812602221, -0.0036053243923775498, -0.05732009413704054, -0.010698620846096853, 0.026260843856232858, -0.029207691280020657, 0.011187588892079341, 0.029390239350520788, -0.019532643543513816, -0.035127464423381986, 0.013873653358009812, -0.02199704249526556, 0.002772448820720711, -0.025947904306804066, -0.05434716841746701, -0.014942863485224854, 0.006728200312719044, 0.01624677827451149, -0.021801455276872564, -0.0007697171990507676, -0.032337086774308584, -0.045063295117746156, -0.036379222621097156, 0.003908484580886693, -0.009903232824632005, -0.008599318035345367, -0.012804443230794771, -0.050487580641178566, -0.005681808694316518, -0.03236316507009432, -0.006415260763290252, 0.03622275284638276, -6.183408414820222e-05, 0.024044188714445577, -0.046627992864890125, 0.03254571314059445, 0.010098820043025, -0.01622069997872576, 0.0017553952850771345, 0.00706721815793357, -0.037370197860955, -0.014590806492117463, -0.006767317756397643, 0.03390178452145255, -0.03006827504094984, 0.02077136259333612, 0.057998129827469595, 0.009231716708149387, 0.0066010686207635975, -0.006542392455245699, -0.006663004573254713, -0.010913766786329148, 0.02169714209372963, -0.019571760987192415, -0.017694123690619658, -0.026704174884590315, 0.03426688066245281, 0.003993239042190324, 0.02779946330759109, 0.042325074060244224, 0.012035133505115656, 0.008983972898184926, -0.030433371181950095, -0.018280885345798643, -0.010418279166400226, -0.028712203660091737, -0.01079641445529335, 0.04109939415831478, 0.032258851886951385, 0.011396215258365204, 0.003290754949462149, -0.05439932500903848, 0.001041501937942701, 0.0010455766716592216, 0.0009860855593980188, 0.0302769014072357, 0.005574235724200371, -0.023548701094516657, 0.01682050078179761, -0.0028246054122921765, -0.038517642875527246, -0.0315547379007366, -0.0013845945168737472, 0.018320002789477242, -0.0045017658100121125, -0.0345015853245244, 0.010842051472918383, 0.05893694847575597, 0.006930307105058473, -0.013104343632330697, -0.0010838791685945167, -0.05280854896610878, -0.002558932773975024, -0.01811137642319138, 0.008918777158720593, -0.05372128931860942, -0.0360141264800969, -0.024383206559660104, -0.03864803435445591, 0.012921795561830568, -0.03815254673452698, -0.015216685590975047, -0.01790275005690552, 0.018176572162655714, -0.04986170154232098, -0.027277897391876436, -0.029624944012592382, 0.036040204775882635, -0.007243246654487266, 0.010418279166400226, 0.04738426344267637, -0.01624677827451149, 0.03692686683259755, -0.04070821972152879, 0.03726588467781207, 0.0009934200800877562, -0.012941354283669868, -0.03241532166166578, 0.004110591373226121, -0.04594995717446107, 0.0272518190960907, 0.013404244033866624, -0.005658990185504003, 0.012328514332705149, 0.026808488067733248, 0.002950107210761015, -0.03152865960495087, -0.004387673265949532, 0.016520600380261683, -0.001042316884686005, -0.012706649621598273, -0.02769515012444816, 0.022622921594123144, 0.015334037922010846, -0.0008426549325764889, -0.012889197692098403, 0.037422354452526466, -0.035988048184311165, -0.022727234777266073, -0.014486493308974531, -0.03202414722487979, 0.009186079690524354, 0.0009217047666769912, -0.0067607981824512105, 0.028347107519091478, 0.0018727476161129316, 0.03293688757738044, -0.0009412634885162907, 0.027277897391876436, -0.0009135552992439497, 0.014082279724295675, -0.01735510584540513, -0.03544040397281078, 0.026286922152018593, 0.04665407116067585, 0.01542531195726091, 0.0008923666839180419, -0.06117968191332899, 0.012295916462972982, 0.02062793196651459, 0.027773385011805356, 0.03867411265024164, -0.013143461076009296, 0.0260131000462684, -0.030146509928307037, 0.031085328576593414, -0.013345567868348725, -0.035179621014953456, 0.020288914121300065, -0.01941529121247802, 0.012054692226954955, 0.002746370524934978, -0.014434336717403067, -0.0005989858563285487, 0.013293411276777259, -0.0316329727880938, -0.008325495929595174, 0.0032940147364353654, -0.005496000836843173, 0.02388771893973118, 0.043707223736888054, 0.03606628307166836, -0.027564758645519497, 0.0014155624931193047, -0.04083861120045745, 0.007132413897397901, 0.012511062403205278, 0.006059943983209643, 0.018346081085262977, 0.06670828061990432, 0.007106335601612169, -0.04767112469631943, 0.009042649063702824, 0.022766352220944672, 0.019310978029335086, 0.04404624158210258, 0.021944885903694093, -0.023705170869231053, 0.01614246509136856, 0.02978141378730678, -0.002855573388537734, 0.003157103683560269, -0.007132413897397901, 0.0001791864151839995, 0.0041790468996636704, -0.008371132947220207, -0.014160514611652873, 0.005740484859834417, 0.015660016619332504, -0.003386918665172038, 0.005597054233012887, -0.021292928509050773, -0.005737225072861201, -0.019219703994085024, -0.021840572720551163, -0.005231958092012629, -0.005642691250637919, -0.019702152466121076, 0.08157290921777198, 0.02826887263173428, 0.012341553480598014, 0.027434367166590833, -0.015320998774117979, -0.05273031407875158, 0.024657028665410297, 0.013586792104366752, 0.004348555822270932, 0.021879690164229762, 0.028868673434806133, -0.029103378096877727, 0.01972823076190681, 0.015229724738867915, 0.0172507926622622, 0.01743334073276233, -0.007875645327291285, 0.00274474063144837, -0.03502315124023906, 0.0274865237581623, 0.0011784129908177978, 0.0008997012046077792, -0.02281850881251614, 0.05664205844661149, 0.015216685590975047, 0.026638979145125984, -0.02673025318037605, 0.00028197157318323514, 0.0028865413647832915, -0.011969937765651324, 0.03901313049545616, -0.006917267957165607, -0.0068455526437548415, 0.00533627127515556, -0.021097341290657778, -0.007158492193183635, 0.017237753514369333, -0.023444387911373724, 0.015633938323546773, 0.02352262279873092, 0.018489511712084506, 0.013280372128884393, -0.04581956569553241, -0.01618158253504716, 0.05570323979832511, -0.008371132947220207, -0.009394706056810215, 0.006852072217701275, 0.05836322596846985, 0.04021273210159987, -0.033693158155166686, 0.03679647535366888, -0.01667707015497608, -0.011435332702043803, 0.00764094066521969, -0.030902780506093287, -0.018958921036227695, 0.01954568269140668, -0.015829525541939768, 0.12235936382665798, -0.008273339338023709, -0.016729226746547546, 0.012563218994776743, 0.031737285971236734, 0.005662249972477219, 0.036822553649454616, -0.012478464533473111, 0.03549256056438225, 0.010137937486703599, 0.0005826869214624657, -0.0158947212814041, -0.031007093689236216, -0.02650858766619732, 0.01499502007679632, -0.013365126590188025, 0.004286619869779818, -0.013260813407045094, 0.01270013004765184, -0.02069312770597892, -0.0010105339616971432, -0.03716157149466914, -0.02759083694130523, -0.017628927951155327, -0.018515590007870238, -0.02952063082944945, -0.012654493030026807, 0.018567746599441704, 0.02533506435583935, -0.015438351105153776, -0.022179590565765687, -0.010685581698203985, -0.040421358467885735, 0.007843047457559118, 0.015320998774117979, -0.047488576625819304, 0.0043811536920030985, -0.020210679233942867, -0.01307174576259853, 0.03403217600038121, -0.014147475463760006, 0.019232743141977888, 0.0015598080666841389, -0.004808185785494472, -0.01370414443540255, -0.016990009704404875, 0.01958480013508528, 0.016481482936583084, 0.009107844803167156, 0.0004327367206945025, 0.05100914655689322, 0.0007819414002003299, 0.005665509759450436, 0.003660740770922232, -0.01933705632512082, -0.0008891068969448252, -0.00993583069436417, -0.045063295117746156, -0.07755685166676914, 0.014629923935796062, -0.011859105008561958, -0.02418761934126711, -0.002138420254430084, -0.011689596085954696, -0.015646977471439637, -0.004785367276681956, -0.019636956726656746, 0.0008251335775954497, 0.021332045952729372, -0.018711177226263233, 0.006747759034558344, 0.025582808165803807, -0.0002789155228958446, -0.0462107401323184, -0.012713169195544705, 0.019493526099835217, -0.002971295826086923, 0.02779946330759109, 0.0038465486283955777, 0.021162537030122112, 0.030302979703021433, -0.03257179143638018, 0.004247502426101219, 0.007999517232273516, -0.04394192839895965, -0.04125586393302918, -0.01700304885229774, 0.003650961410002582, -0.04083861120045745, -0.01693785311283341, 0.012256799019294383, -0.01452561075265313, 0.006982463696629938, 0.003781352888931246, -0.009642449866774677, 0.015164528999403582, 0.022609882446230276, -0.025126437989553486, -0.013228215537312928, 0.011174549744186475, -0.021162537030122112, -0.07129806067819329, -0.01696393140861914, 0.016272856570297225, 0.0006108025841064588, -0.026000060898375532, -0.04274232679281594, -0.0003683559279734748, -0.016377169753440154, -0.007966919362541349, -0.024200658489159973, -0.017368144993297998, -0.006910748383219174, 0.0008015001220396294, -0.01212640754036572, 0.02863396877273454, 0.01790275005690552, -0.01972823076190681, 0.02392683638340978, -0.03429295895823854, -0.045976035470246805, -0.013097824058384264, 0.005968669947959578, 0.05241737452932279, 0.029859648674663977, 0.0274865237581623, -0.010137937486703599, 0.0074127555770945285, -0.043368205891673534, 0.026586822553554518, -0.02654770510987592, 0.00036611482442938843, -0.01882852955729903, -0.045402312962960684, -0.018502550859977374, 0.0007090036666746086, 0.02360085768608812, -0.0011955268724271849, 0.007719175552576888, 0.0033380218605737894, 0.02087567577647905, 0.031163563463950612, 0.003219039636051384, 0.024943889919053358, -0.042455465539172885, -0.003742235445252647, 0.021136458734336377, 0.001279466386987512, -0.011904742026186991, -0.003693338640654398, 0.024005071270766978, -0.0034488546176631536, -0.0003469635759617409, 0.03358884497202376, -0.026169569820982796, 0.01499502007679632, 0.0012998400555701158, -0.008279858911970141, 0.03293688757738044, -0.016390208901333022, -0.016703148450761814, 0.008136428285148612, -0.08684072496649, 0.023327035580337926, 0.005209139583200113, 0.03969116618588522, 0.0231053700661592, -0.014916785189439122, -0.013691105287509683, 0.0037780931019580294, 0.0046614953716997256, 0.01919362569829929, -0.008221182746452243, 0.00839721124300594, 0.009694606458346142, 0.007966919362541349, 0.01775931943008399, -0.02013244434658567, -0.04446349431467431, 0.020041170311335604, -0.01309130448443783, 0.02800808967387695, -0.012348073054544448, -0.008762307384006197, -0.02281850881251614, -0.008221182746452243, -0.018489511712084506, 0.03280649609845177, -0.020471462191800193, -0.03984763596059961, -0.02030195326919293, -0.017302949253833667, 0.00882750312347053, 0.010555190219275323, -0.030042196745164104, -0.05392991568489529, -0.025765356236303938, 0.002728441696582287, 0.022701156481480342, -0.04834916038674848, 0.02073224514965752, 0.005942591652173846, 0.0013096194164897656, 0.04172527325717237, 0.01722471436647647, -0.0020585554735862775, 0.02249253011519448, -0.014212671203224338, 0.006568470751031431, 0.025739277940518206, -0.05502520410789606, -0.00460933878012826, 0.013912770801688411, -0.013717183583295416, 0.011057197413150678, 0.033093357352094836, 0.008671033348756133, -0.0115526850330796, 0.011689596085954696, 0.005939331865200629, -0.005065708956378583, 0.013000030449187766, -0.014434336717403067, -0.0031163563463950613, -0.001329178138329065, 0.021801455276872564, 0.007536627482076759, 0.008182065302773643, -0.0003300534310381799, 0.015972956168761297, 0.0033331321801139645, 0.019819504797156873, 0.02030195326919293, 0.0017553952850771345, -0.00252470501075625, 0.040942924383600386, -0.019310978029335086, 0.019363134620906552, -0.028972986617949063, -0.0033543207954398724, -0.002149829508836342, -0.05726793754546908, -0.032311008478522855, 0.044072319877888316, -0.014290906090581536, -0.025413299243196547, -0.007419275151040961, -0.030224744815664235, -0.01169611565990113, 0.03249355654902298, 0.01804618068372705, -0.011565724180972466, -0.011057197413150678, 0.0018434095333539824, -0.009655489014667543, 0.0017521354981039179, 0.002909359873595808, 0.00433225688740485, -0.007406236003148096, 0.0331455139436663, 0.025491534130553745, 0.02550457327844661, 0.02460487207383883, 0.04422878965260271, -0.051400320993679215, -0.033171592239452034, 0.021136458734336377, -0.00895137502845276, -0.008332015503541608, -0.0009135552992439497, 0.030042196745164104, 0.01399100568904561, 0.007373638133415929, 0.01176131139936546, -0.006962904974790639, 0.02066704941019319, 0.0006682563295094012, 0.03152865960495087, 0.018932842740441963, 0.016025112760332763, -0.028086324561234152, -0.022935861143551936, -0.018593824895227436, 0.01811137642319138, -0.00690422880927274, -0.001422897013809042, -0.01954568269140668, 0.0014913525402465906, -0.0030772389027164623, 0.0052286983050394125, 0.032337086774308584, 0.007093296453719302, 0.026313000447804324, -0.0017390963502110514, -0.008997012046077791, 0.024487519742803034, 0.02066704941019319, -0.022596843298337412, 0.02077136259333612, 0.023001056883016267, -0.009974948138042769, -0.005456883393164574, -0.020406266452335863, 0.03828293821345565, 0.00780393001388052, 0.028086324561234152, -0.014017083984831343, -0.01509933325993925, -0.01456472819633173, 0.05883263529261304, -9.117216690715154e-05, 0.006363104171718786, 0.012693610473705406, 0.019232743141977888]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "FAISS.__init__() missing 2 required positional arguments: 'docstore' and 'index_to_docstore_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m index\u001b[38;5;241m.\u001b[39madd(embedding_matrix)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 使用 FAISS 作为向量存储器\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 构造提示词模版\u001b[39;00m\n\u001b[0;32m     57\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     58\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     59\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m原词: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m反义: \u001b[39m\u001b[38;5;132;01m{output}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     60\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: FAISS.__init__() missing 2 required positional arguments: 'docstore' and 'index_to_docstore_id'"
     ]
    }
   ],
   "source": [
    "# 修改一下\n",
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "import dashscope\n",
    "from dashscope import TextEmbedding\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv(find_dotenv())\n",
    "DASHSCOPE_API_KEY = os.environ[\"DASHSCOPE_API_KEY\"]\n",
    "\n",
    "dashscope.api_key = DASHSCOPE_API_KEY\n",
    "\n",
    "# 调用DashScope通用文本向量模型，将文本embedding为向量\n",
    "def generate_embeddings(texts: Union[List[str], str], text_type: str = 'document'):\n",
    "    rsp = TextEmbedding.call(\n",
    "        model=TextEmbedding.Models.text_embedding_v2,\n",
    "        input=texts,\n",
    "        text_type=text_type\n",
    "    )\n",
    "    embeddings = [record['embedding'] for record in rsp.output['embeddings']]\n",
    "    return embeddings if isinstance(texts, list) else embeddings[0]\n",
    "\n",
    "# 示例获取嵌入\n",
    "text = \"这是一个示例文本\"\n",
    "embedding = generate_embeddings(text)\n",
    "print(embedding)\n",
    "\n",
    "# 示例数据\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tail\", \"output\": \"short\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "    {\"input\": \"高兴\", \"output\": \"伤心\"}\n",
    "]\n",
    "\n",
    "# 获取所有示例的嵌入向量\n",
    "embeddings = [generate_embeddings(ex['input']) for ex in examples]\n",
    "\n",
    "# 初始化FAISS索引\n",
    "dimension = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# 添加嵌入向量到索引\n",
    "embedding_matrix = np.array(embeddings).astype('float32')\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# 使用 FAISS 作为向量存储器\n",
    "vectorstore = FAISS(embedding_matrix, index)\n",
    "\n",
    "# 构造提示词模版\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词: {input}\\n反义: {output}\",\n",
    ")\n",
    "\n",
    "# 调用MMR\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    examples=examples,\n",
    "    embedding_function=lambda x: generate_embeddings(x),\n",
    "    vectorstore=vectorstore,\n",
    "    k=2\n",
    ")\n",
    "\n",
    "# 使用小样本的模板\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=prompt_template,\n",
    "    prefix=\"给出每个输入词的反义词\",\n",
    "    suffix=\"原词:{adjective}\\n反义:\",\n",
    "    input_variables=[\"adjective\"]\n",
    ")\n",
    "\n",
    "# 输入一个描述情绪的词语时，选择相关示例\n",
    "print(mmr_prompt.format(adjective=\"难过\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MaxMarginalRelevanceExampleSelector.from_examples() missing 1 required positional argument: 'vectorstore_cls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 64\u001b[0m\n\u001b[0;32m     58\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     59\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     60\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m原词: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m反义: \u001b[39m\u001b[38;5;132;01m{output}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 调用MMR示例选择器\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m example_selector \u001b[38;5;241m=\u001b[39m \u001b[43mMaxMarginalRelevanceExampleSelector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m     69\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 使用小样本的模板\u001b[39;00m\n\u001b[0;32m     72\u001b[0m mmr_prompt \u001b[38;5;241m=\u001b[39m FewShotPromptTemplate(\n\u001b[0;32m     73\u001b[0m     example_selector\u001b[38;5;241m=\u001b[39mexample_selector,\n\u001b[0;32m     74\u001b[0m     example_prompt\u001b[38;5;241m=\u001b[39mprompt_template,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     78\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: MaxMarginalRelevanceExampleSelector.from_examples() missing 1 required positional argument: 'vectorstore_cls'"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "import dashscope\n",
    "from dashscope import TextEmbedding\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import numpy as np\n",
    "import faiss  # 需要导入FAISS库\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv(find_dotenv())\n",
    "DASHSCOPE_API_KEY = os.environ[\"DASHSCOPE_API_KEY\"]\n",
    "\n",
    "dashscope.api_key = DASHSCOPE_API_KEY\n",
    "\n",
    "# 调用DashScope通用文本向量模型，将文本embedding为向量\n",
    "def generate_embeddings(texts):\n",
    "    rsp = TextEmbedding.call(\n",
    "        model=TextEmbedding.Models.text_embedding_v2,\n",
    "        input=texts,\n",
    "        text_type='document'\n",
    "    )\n",
    "    embeddings = [record['embedding'] for record in rsp.output['embeddings']]\n",
    "    return embeddings if isinstance(texts, list) else embeddings[0]\n",
    "\n",
    "# 示例数据\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "    {\"input\": \"高兴\", \"output\": \"伤心\"}\n",
    "]\n",
    "\n",
    "# 获取所有示例的嵌入向量\n",
    "embeddings = [generate_embeddings(ex['input']) for ex in examples]\n",
    "\n",
    "# 初始化FAISS索引\n",
    "dimension = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# 添加嵌入向量到索引\n",
    "embedding_matrix = np.array(embeddings).astype('float32')\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# 创建文档存储器和索引到文档存储器的映射\n",
    "documents = [Document(page_content=ex['input'], metadata={\"output\": ex['output']}) for ex in examples]\n",
    "docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})\n",
    "index_to_docstore_id = [str(i) for i in range(len(examples))]\n",
    "\n",
    "# 使用 FAISS 作为向量存储器\n",
    "vectorstore = FAISS(embedding_matrix, index, docstore, index_to_docstore_id)\n",
    "\n",
    "# 构造提示词模版\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词: {input}\\n反义: {output}\",\n",
    ")\n",
    "\n",
    "# 调用MMR示例选择器\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    examples=examples,\n",
    "    embeddings=embedding_matrix,\n",
    "    vectorstore=vectorstore,\n",
    "    k=2\n",
    ")\n",
    "\n",
    "# 使用小样本的模板\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=prompt_template,\n",
    "    prefix=\"给出每个输入词的反义词\",\n",
    "    suffix=\"原词:{adjective}\\n反义:\",\n",
    "    input_variables=[\"adjective\"]\n",
    ")\n",
    "\n",
    "# 输入一个描述情绪的词语时，选择相关示例\n",
    "print(mmr_prompt.format(adjective=\"难过\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dashscope.models' has no attribute 'TextEmbedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 25\u001b[0m\n\u001b[0;32m     16\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     17\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhappy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msad\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     18\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshort\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalm\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     22\u001b[0m ]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 使用DashScope的TextEmbedding模型\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m embedding_function \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextEmbedding\u001b[49m(\n\u001b[0;32m     26\u001b[0m     access_key_id\u001b[38;5;241m=\u001b[39mACCESS_KEY_ID,\n\u001b[0;32m     27\u001b[0m     access_key_secret\u001b[38;5;241m=\u001b[39mACCESS_KEY_SECRET\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 生成示例文本的嵌入\u001b[39;00m\n\u001b[0;32m     31\u001b[0m example_embeddings \u001b[38;5;241m=\u001b[39m [embedding_function\u001b[38;5;241m.\u001b[39membed_query(ex[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m examples]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'dashscope.models' has no attribute 'TextEmbedding'"
     ]
    }
   ],
   "source": [
    "from dashscope import models\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# DashScope访问密钥ID和密钥（请替换为您自己的）\n",
    "ACCESS_KEY_ID = os.getenv(\"DASHBOARD_ACCESS_KEY_ID\")\n",
    "ACCESS_KEY_SECRET = os.getenv(\"DASHBOARD_ACCESS_KEY_SECRET\")\n",
    "\n",
    "# 示例数据保持不变\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"}\n",
    "]\n",
    "\n",
    "# 使用DashScope的TextEmbedding模型\n",
    "embedding_function = models.TextEmbedding(\n",
    "    access_key_id=ACCESS_KEY_ID,\n",
    "    access_key_secret=ACCESS_KEY_SECRET\n",
    ")\n",
    "\n",
    "# 生成示例文本的嵌入\n",
    "example_embeddings = [embedding_function.embed_query(ex['input']) for ex in examples]\n",
    "\n",
    "# 注意：DashScope不直接提供向量数据库服务，因此需要自行管理向量和检索逻辑\n",
    "# 这里简单使用NumPy数组存储示例嵌入，实际应用中可能需要更高效的向量数据库解决方案\n",
    "faiss_embeddings = np.array(example_embeddings).astype('float32')\n",
    "\n",
    "# FAISS索引初始化（仅为示例，实际部署可能需要更复杂设置）\n",
    "dimension = faiss_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(faiss_embeddings)\n",
    "\n",
    "# 构造PromptTemplate保持不变\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词: {input}\\n反义: {output}\",\n",
    ")\n",
    "\n",
    "# 由于DashScope不直接支持SemanticSimilarityExampleSelector，\n",
    "# 我们需要自定义逻辑来根据输入找到最相似的示例。这里简化处理，直接展示如何使用FAISS检索\n",
    "def find_similar_examples(input_text, embeddings, index, top_k=1):\n",
    "    query_embedding = embedding_function.embed_query(input_text)\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), top_k)\n",
    "    return [(emb, ex) for emb, ex in zip(embeddings[I[0]], examples[I[0]])]\n",
    "\n",
    "# 小样本模板的创建需要调整，因为我们不再直接使用SemanticSimilarityExampleSelector\n",
    "def create_few_shot_prompt(adjective, examples):\n",
    "    selected_examples = find_similar_examples(adjective, faiss_embeddings, index)\n",
    "    example_str = \"\\n\".join([example_prompt.format(input=ex[1][\"input\"], output=ex[1][\"output\"]) for ex in selected_examples])\n",
    "    prompt = f\"给出每个输入词的反义词\\n{example_str}\\n原词:{adjective}\\n反义:\"\n",
    "    return prompt\n",
    "\n",
    "# 示例使用\n",
    "print(create_few_shot_prompt(\"worried\", examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install chromadb==0.4.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据输入相似度选择示例(最大余弦相似度)\n",
    "- 一种常见的相似度计算方法\n",
    "- 通过计算两个向量 之间的余弦值,来衡量它的相似度\n",
    "- 余弦值越接近1,则表示两个向量越相似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from langchian.prompts import SemanticSimilaritySearchResultWriter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "import os\n",
    "\n",
    "# 这里引入API的key\n",
    "# 忽略\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词: {input}\\n反义: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # 传入示例组.\n",
    "    examples,\n",
    "    # 使用openAI嵌入来做相似性搜索\n",
    "    OpenAIEmbeddings(openai_api_key=api_key,openai_api_base=api_base),\n",
    "    # 使用Chroma向量数据库来实现对相似结果的过程存储\n",
    "    Chroma,\n",
    "    # 结果条数\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "#使用小样本提示词模板\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # 传入选择器和模板以及前缀后缀和输入变量\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"给出每个输入词的反义词\",\n",
    "    suffix=\"原词: {adjective}\\n反义:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# 输入一个形容感觉的词语，应该查找近似的 happy/sad 示例\n",
    "print(similar_prompt.format(adjective=\"worried\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain核心组件: LLM vs chatModel\n",
    "LLM: 就是直接使用语言模型,使用前,我们需要申请API key,然后安装langchain,然后使用LLM\n",
    "chatModel也一样,但是导入的时候,我们是import ChatOpenAI\n",
    "\n",
    "chatModel:是一组对话,封装角色给模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM调用这里以OpenAI\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "# 导入API\n",
    "\n",
    "# 设置LLM\n",
    "LLM = OpenAI(\n",
    "    model = \"gpt-3\",\n",
    "    temperature = 0,\n",
    "    # 两个key\n",
    ")\n",
    "\n",
    "llm.predict(\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！有什么我可以帮助你的吗？'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转译成通义千问\n",
    "from langchain_community.llms import Tongyi\n",
    "import os\n",
    "\n",
    "llm = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    DASHSCOPE_API_KEY=\"\"\n",
    ")\n",
    "\n",
    "llm.predict(\"你好\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用chatModel 以OpenAI为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我能为你效劳的吗？\n"
     ]
    }
   ],
   "source": [
    "# 转译成通义千问\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "tongyi_chat = ChatTongyi(\n",
    "    model=\"qwen-max\",\n",
    "    temperature=0,\n",
    "    DASHSCOPE_API_KEY=\"\"\n",
    ")\n",
    "\n",
    "print(tongyi_chat.predict(\"你好\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='当然，你叫冰糖红茶。' response_metadata={'model_name': 'qwen-max', 'finish_reason': 'stop', 'request_id': 'ba83b48d-46c4-9f45-ac0d-c6e37e82b57e', 'token_usage': {'input_tokens': 35, 'output_tokens': 8, 'total_tokens': 43}} id='run-5da594a2-5a5c-4921-b6a2-3d5a2f1784b6-0'\n"
     ]
    }
   ],
   "source": [
    "# 转译成通义千问\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.schema.messages import HumanMessage,AIMessage\n",
    "\n",
    "import os\n",
    "\n",
    "tongyi_chat = ChatTongyi(\n",
    "    model=\"qwen-max\",\n",
    "    temperature=0,\n",
    "    DASHSCOPE_API_KEY=\"\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    AIMessage(role = \"System\", content= \"你好,我是SouthAki\"),\n",
    "    HumanMessage(role = \"User\",content=\"你好SouthAki,我是冰糖红茶\"),\n",
    "    AIMessage(role = \"System\", content=\"认识你很高兴\"),\n",
    "    HumanMessage(role = \"User\",content=\"你知道我叫什么吗\")\n",
    "]\n",
    "\n",
    "response = tongyi_chat.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果,你想要很好的一种体验输出的话,我们需要启用流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "夏日炎炎，烈日如炬，\n",
      "绿荷承露，红莲出淤。\n",
      "金风微动，柳絮轻舞，\n",
      "蝉鸣声中，夏意浓如许。\n",
      "\n",
      "稻穗摇曳，满眼翠色，\n",
      "蜻蜓点水，燕子穿梭疾。\n",
      "儿童戏水，笑声溢四野，\n",
      "清泉石上洗悠扬的曲。\n",
      "\n",
      "晚霞映天，火烧云彩斑斓，\n",
      "萤火点点，夜幕下的繁星点点。\n",
      "冰镇瓜果，甜入心扉，\n",
      "夏夜凉风，带走白日的疲惫。\n",
      "\n",
      "这就是夏，热烈而奔放，\n",
      "如同青春，充满活力与希望。\n",
      "每一寸土地，每一片天空，\n",
      "都在歌唱，属于夏天的歌。"
     ]
    }
   ],
   "source": [
    "# 导入模块\n",
    "from langchain_community.llms import Tongyi\n",
    "import os\n",
    "\n",
    "# 构造一个llm\n",
    "llm = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key=\"\",\n",
    "    max_tokens = 512\n",
    ")\n",
    "\n",
    "for chunk in llm.stream(\"请生成一首夏天的诗词\"):\n",
    "    print(chunk, end=\"\", flush=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到,就是生成出来的文本,是像语言模型那种方式输出出来\n",
    "这里的话是构造一个chunk的一个循环,调用langchain自带的一个方法stream方法,然后把chunk传进去\n",
    "flush的作用设置为false是否刷新,默认是true,所以这里设置为false,禁止刷新缓冲区\n",
    "\n",
    "但是llm的方式跟我们日常的使用的不太相符,我们日常使用一般使用的是chatModel模式,也就是对话的形式\n",
    "那这里的话,演示一下chatModel的流式输出的方法\n",
    "\n",
    "这里使用的是另外一家语言模型的公司的产品\n",
    "这是它的官网[点击访问](https://www.anthropic.com/)\n",
    "号称是安全的语言模型,最有能力跟OpenAI竞争的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='夏\n",
      "content='风\n",
      "content='轻\n",
      "content='抚绿柳丝，\n",
      "content='桃花笑映日偏西。\n",
      "蜻\n",
      "content='蜓点水涟漪起，儿童\n",
      "content='欢声溢满溪。\n",
      "\n",
      "竹影\n",
      "content='摇曳纳凉处，瓜果\n",
      "content='飘香醉心底。\n",
      "晚霞如\n",
      "content='织天边绮，蝉鸣声\n",
      "content='中共星稀。\n",
      "\n",
      "月上柳梢\n",
      "content='头，萤火点点舞夏\n",
      "content='夜。\n",
      "梦回童年时光里，\n",
      "content='夏夜星空下许愿祈。\n",
      "\n",
      "\n",
      "content='时光缓缓在夏色中漫步，\n",
      "content='留下一串串金黄的足迹\n",
      "content='，\n",
      "每一缕阳光，每一声欢\n",
      "content='笑，都是夏天最美的诗。\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "# 导入模块\n",
    "from langchain.chat_models import ChatTongyi\n",
    "from langchain.schema.messages import AIMessage, HumanMessage\n",
    "import os\n",
    "\n",
    "tongyi_chat = ChatTongyi(\n",
    "    model=\"qwen-max\",\n",
    "    temperature=0,\n",
    "    dashscope_api_key=\"\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(role=\"user\", content=\"请生成一首夏天的诗词\")\n",
    "]\n",
    "\n",
    "# 使用 stream 方法来流式输出\n",
    "\n",
    "for chunk in tongyi_chat.stream(messages):\n",
    "    print(\"content='\", end=\"\")\n",
    "    print(chunk.content)\n",
    "print(\"'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成上面了之后,我们需要去实现一个就是token的追踪消耗的功能\n",
    "因为在实际开发的过程中,就是我们不可避免要消耗到token\n",
    "下面是一个案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "# 导入模块\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import os\n",
    "\n",
    "# 构造一个llm\n",
    "llm = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key=\"\",\n",
    "    max_tokens = 512\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"请生成一个关于人工智能的段落\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='人工智能，这个21世纪的科技奇迹，正以前所未有的速度渗透到我们生活的方方面面，重新定义着人类与技术的界限。它基于复杂算法、大数据分析和机器学习等核心技术，使计算机系统能够执行以往专属于人类的智力任务，比如语言理解、图像识别、决策制定甚至创意工作。从智能家居的便捷操控，到自动驾驶汽车的安全行驶；从医疗领域的疾病诊断与个性化治疗方案设计，到金融行业的风险评估与投资策略优化，人工智能的应用场景日益广泛，极大地提高了效率，改善了生活质量，并为解决许多全球性挑战提供了新的可能。然而，随着AI技术的迅猛发展，也引发了对就业结构变化、隐私保护、伦理道德以及AI治理等问题的深刻思考，促使社会各界共同努力，确保技术进步的同时，构建一个和谐共生、可持续发展的未来。' response_metadata={'model_name': 'qwen-max', 'finish_reason': 'stop', 'request_id': '0e071f31-1af2-9fbe-ab48-0810fe990b64', 'token_usage': {'input_tokens': 16, 'output_tokens': 174, 'total_tokens': 190}} id='run-2a2ada49-f025-4505-b1b7-f4bc4d0a04c8-0'\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatTongyi\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import os\n",
    "\n",
    "tongyi_chat = ChatTongyi(\n",
    "    model=\"qwen-max\",\n",
    "    temperature=0,\n",
    "    dashscope_api_key=\"\"\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = tongyi_chat.invoke(\"请生成一个关于人工智能的段落\")\n",
    "    print(result)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是目前的话,仅支持OpenAI,通义的话没有这个功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain的文本格式目前的话,上面主要的内容主要是以一个文本形式展示出来的,就是没有达到我们跟其他系统的联动性要求\n",
    "\n",
    "但是其实langchain,无论是LLM还是chatModel,其实都支持的是list(数组),JSON,函数,时间戳等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的话,我用几个例子来分别说明\n",
    "下面是一个函数的形式\n",
    "还要注意的是我们需要安装对应的Python的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python Pydantic使用指南](https://juejin.cn/post/7245975053233373244?searchId=20240701103757CDF5CCCF938A810D5BB3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_put ```json\n",
      "{\n",
      "  \"setup\": \"为什么电脑永远不会感冒？\",\n",
      "  \"punchline\": \"因为它有‘Windows’，但不开！\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 我这部分用一个讲笑话机器人演示 就是希望每次根据指令,可以输出一个这样笑话(小明是怎么dead的?笨dead的)\n",
    "\n",
    "# 首先还是导入我们的相应的模块\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Field主要是用来填入一些字段的,validator主要是用来校验一些字段的\n",
    "from langchain.pydantic_v1 import BaseModel,Field,validator\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "\n",
    "# 构造llm\n",
    "model = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key=\"\"\n",
    ")\n",
    "\n",
    "# 这里的话需要定义一个数据模型,用来描述最终的实例结构\n",
    "class Joke(BaseModel):\n",
    "    # description是描述这个字段的含义\n",
    "    setup: str = Field(description= \"设置笑话的问题\")\n",
    "    punchline: str = Field(description= \"回答笑话的答案\")\n",
    "\n",
    "    # 除了上面的我们的模版,我们还需要验证问题是否符合要求\n",
    "    @validator(\"setup\")\n",
    "    def question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"不符合预期的问题的格式!\")\n",
    "        return field\n",
    "\n",
    "# 将Joke的数据模型传入\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# 模版设置\n",
    "prompt = PromptTemplate(\n",
    "    template= \"回答用户的输入.\\n{format_instrc}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\n",
    "        \"format_instrc\": parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "# 做好我们上面的要求后,我们需要调用我们的方式  这里使用Python的一个管道\n",
    "\n",
    "prompt_and_model = prompt | model\n",
    "out_put = prompt_and_model.invoke({\"query\":\"给我讲一个笑话\"})\n",
    "print(\"out_put:\",out_put)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_put: ```json\n",
      "{\n",
      "  \"setup\": \"Why was the math book sad?\",\n",
      "  \"punchline\": \"Because it had too many problems.\"\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why was the math book sad?', punchline='Because it had too many problems.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先还是导入我们的相应的模块\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Field主要是用来填入一些字段的,validator主要是用来校验一些字段的\n",
    "from langchain.pydantic_v1 import BaseModel,Field,validator\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "\n",
    "# 构造llm\n",
    "model = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key=\"\"\n",
    ")\n",
    "\n",
    "# 这里的话需要定义一个数据模型,用来描述最终的实例结构\n",
    "class Joke(BaseModel):\n",
    "    # description是描述这个字段的含义\n",
    "    setup: str = Field(description= \"设置笑话的问题\")\n",
    "    punchline: str = Field(description= \"回答笑话的答案\")\n",
    "\n",
    "    # 除了上面的我们的模版,我们还需要验证问题是否符合要求\n",
    "    @validator(\"setup\")\n",
    "    def question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"不符合预期的问题的格式!\")\n",
    "        return field\n",
    "\n",
    "# 将Joke的数据模型传入\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# 模版设置\n",
    "prompt = PromptTemplate(\n",
    "    template= \"回答用户的输入.\\n{format_instrc}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\n",
    "        \"format_instrc\": parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "# 做好我们上面的要求后,我们需要调用我们的方式  这里使用Python的一个管道\n",
    "\n",
    "prompt_and_model = prompt | model\n",
    "out_put = prompt_and_model.invoke({\"query\":\"给我讲一个笑话\"})\n",
    "print(\"out_put:\",out_put)\n",
    "\n",
    "# 验证回答是否符合我们的需求\n",
    "parser.parse(out_put)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Joke(setup='Why was the math book sad?', punchline='Because it had too many problems.')\n",
    "```\n",
    "上面的就是符合我们的需求的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后,第二个示例是,数组的形式\n",
    "```就是将LLM的输出格式化成Python list形式,类似['a','b','c']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John, Sarah, Michael, Jennifer, Robert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['John', 'Sarah', 'Michael', 'Jennifer', 'Robert']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Tongyi\n",
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "# DASHSCOPE_API_KEY = os.environ[\"DASHSCOPE_API_KEY\"]\n",
    "\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# 构造我们的llm\n",
    "model = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key = api_key\n",
    ")\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# 自定义模版\n",
    "prompt = PromptTemplate(\n",
    "    template= \"列出5个{subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "# 格式下模版\n",
    "_input = prompt.format(subject=\"常见的美国人名字\")\n",
    "output = model(_input)\n",
    "print(output)\n",
    "parser.parse(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
