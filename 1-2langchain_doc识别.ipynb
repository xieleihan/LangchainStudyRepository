{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用prompts模板调教LLM的输入出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'请帮我生成一个具有中国特色的名字'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"请帮我生成一个具有{country}特色的名字\")\n",
    "prompt.format(country=\"中国\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK,上面的结果出来的,但是,刚才说了prompt的模版,就是我们希望让AI知道自己的身份,它需要帮我们做什么这样的\n",
    "所以根据上面的,我们需要对输入进去的问题进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个算命大师,请你帮我生成一个具有中国特色的男的名字\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"你是一个{name},请你帮我生成一个具有{country}特色的{sex}的名字\")\n",
    "print(prompt.format(name=\"算命大师\", country=\"中国\", sex=\"男\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到字符模版就是上面那种,然后处理方式有LLM,还有另一个就是chatmodels,就是对话模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个起名大师,你的名字叫mldys.'),\n",
       " HumanMessage(content='你好mldys,你感觉怎么样'),\n",
       " AIMessage(content='你好!我现在状态非常好'),\n",
       " HumanMessage(content='你叫什么名字呢?')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对话模版具有结构,chatmodels\n",
    "# 首先导入模块\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 一个结构模版  system,human,ai,user_input\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个起名大师,你的名字叫{name}.\"),\n",
    "    (\"human\", \"你好{name},你感觉怎么样\"),\n",
    "    (\"ai\",\"你好!我现在状态非常好\"),\n",
    "    (\"human\",\"{user_input}\"),\n",
    "])\n",
    "\n",
    "# 如果我们按照上面的试试\n",
    "# prompt.format(name=\"mldys\",user_input=\"你叫什么名字呢?\")\n",
    "# 出来的结果就不是我们想要的\n",
    "# 所以这里要换一个chatmodels里的一个,chat_template.format_messages方法\n",
    "chat_template.format_messages(name=\"mldys\",user_input=\"你叫什么名字呢?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是除了上面的方式,其实langchain还有另一个信息模版的模块\n",
    "就是对应的例如\n",
    "SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='你是一个起名大师', additional_kwargs={'大师姓名': '女大学生'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还是第一步导入模块\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "SystemMessage(\n",
    "    content=\"你是一个起名大师\",\n",
    "    # additional_kwargs 附加参数\n",
    "    additional_kwargs={\n",
    "        \"大师姓名\": \"女大学生\"\n",
    "        # 这里我在写的时候看到植物大战僵尸杂交版更新了,就随便起的\n",
    "    }\n",
    ")\n",
    "\n",
    "# HumanMessage(\n",
    "#     content= \"请问大师叫什么\"\n",
    "# )\n",
    "\n",
    "# AIMessage(\n",
    "#     content=\"我是女大学生\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='请问大师叫什么')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "# SystemMessage(\n",
    "#     content=\"你是一个起名大师\",\n",
    "#     # additional_kwargs 附加参数\n",
    "#     additional_kwargs={\n",
    "#         \"大师姓名\": \"女大学生\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "HumanMessage(\n",
    "    content= \"请问大师叫什么\"\n",
    ")\n",
    "\n",
    "# AIMessage(\n",
    "#     content=\"我是女大学生\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我是女大学生')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "# SystemMessage(\n",
    "#     content=\"你是一个起名大师\",\n",
    "#     # additional_kwargs 附加参数\n",
    "#     additional_kwargs={\n",
    "#         \"大师姓名\": \"女大学生\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# HumanMessage(\n",
    "#     content= \"请问大师叫什么\"\n",
    "# )\n",
    "\n",
    "AIMessage(\n",
    "    content=\"我是女大学生\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每段跑起来就是这样,我们可以这样在一个数组里展示出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个起名大师', additional_kwargs={'大师姓名': '女大学生'}),\n",
       " HumanMessage(content='请问大师叫什么'),\n",
       " AIMessage(content='我是女大学生')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还是第一步导入模块\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "system = SystemMessage(\n",
    "    content=\"你是一个起名大师\",\n",
    "    # additional_kwargs 附加参数\n",
    "    additional_kwargs={\n",
    "        \"大师姓名\": \"女大学生\"\n",
    "        # 这里我在写的时候看到植物大战僵尸杂交版更新了,就随便起的\n",
    "    }\n",
    ")\n",
    "\n",
    "people = HumanMessage(\n",
    "    content= \"请问大师叫什么\"\n",
    ")\n",
    "\n",
    "appleIntelligence = AIMessage(\n",
    "    content=\"我是女大学生\"\n",
    ")\n",
    "\n",
    "[system, people, appleIntelligence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果消息体,跟上面是一样的\n",
    "编程的时候适用不同场景,这里需要注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T13:38:32.441150Z",
     "start_time": "2024-06-16T13:38:30.979971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='愿上帝与你同在!' role='system'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import AIMessagePromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "# 定义模板\n",
    "prompt = \"愿{subject}与你同在!\"\n",
    "\n",
    "# 创建 ChatMessagePromptTemplate 实例并提供 role 参数\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(template=prompt, role=\"system\")\n",
    "\n",
    "# 格式化模板\n",
    "formatted_prompt = chat_message_prompt.format(subject=\"上帝\")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T13:48:47.939971Z",
     "start_time": "2024-06-16T13:48:47.280368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='May the force be with you', role='Jedi')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "prompt = \"May the {subject} be with you\"\n",
    "\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(\n",
    "    role=\"Jedi\", template=prompt\n",
    ")\n",
    "chat_message_prompt.format(subject=\"force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不知道为什么出现这个错误,我用成SystemMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='愿上帝与你同在!'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "# 定义模板\n",
    "prompt = \"愿{subject}与你同在!\"\n",
    "\n",
    "# 创建 SystemMessagePromptTemplate 实例\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template=prompt)\n",
    "\n",
    "# 格式化模板\n",
    "formatted_prompt = system_message_prompt.format(subject=\"上帝\")\n",
    "\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='愿上帝与你同在!' role='天道'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "# 定义模板\n",
    "prompt = \"愿{subject}与你同在!\"\n",
    "\n",
    "# 创建 SystemMessagePromptTemplate 实例\n",
    "system_message_prompt = ChatMessagePromptTemplate.from_template(role=\"天道\",template=prompt)\n",
    "\n",
    "# 格式化模板\n",
    "formatted_prompt = system_message_prompt.format(subject=\"上帝\")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "role就是我们构造的角色\n",
    "那么下面就是自定义模版的实际操作了\n",
    "我们来写一个函数大师,通过我们的prompts的模版,让AI达成我们的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个非常有经验和天赋的程序员,现在给你如下的函数名称,你会按照如下的格式,输出这段代码的名称,源代码,中文解释.\n",
      "函数名称:hello_world\n",
      "源代码:\n",
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "    return abc\n",
      "\n",
      "代码解释:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 函数大师:根据提供的函数名称,查找函数代码,并给出中文的代码说明\n",
    "# 首先我们依旧需要导入模块\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "# 定义一个简单的函数作为示例效果\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "    return abc\n",
    "\n",
    "# 这里的是问题的模版\n",
    "PROMPT = \"\"\"\\\n",
    "你是一个非常有经验和天赋的程序员,现在给你如下的函数名称,你会按照如下的格式,输出这段代码的名称,源代码,中文解释.\n",
    "函数名称:{function_name}\n",
    "源代码:\n",
    "{source_code}\n",
    "代码解释:\n",
    "\"\"\"\n",
    "\n",
    "# 接下来我们需要导入一个模块,来获取源代码,是Python内置的一个\n",
    "import inspect\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # 获取函数的源代码\n",
    "    source_code = inspect.getsource(function_name)\n",
    "    return source_code\n",
    "\n",
    "# 自定义模版的class\n",
    "class CustmPrompt(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # 获得源代码\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # 获取生成提示词模版\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__,source_code=source_code\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "    \n",
    "a = CustmPrompt(input_variables=[\"function_name\"])\n",
    "pm = a.format(function_name=hello_world)\n",
    "\n",
    "print(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数名称: hello_world\n",
      "源代码:\n",
      "```python\n",
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "代码解释: 这是一个简单的Python函数，名为`hello_world`。当调用这个函数时，它会打印出字符串 \"Hello, World!\"。这是编程中常见的一个示例，用于演示基本的输出功能。\n"
     ]
    }
   ],
   "source": [
    "# 导入相关包\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# 函数大师:根据提供的函数名称,查找函数代码,并给出中文的代码说明\n",
    "# 首先我们依旧需要导入模块\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "# 定义一个简单的函数作为示例效果\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "    \n",
    "\n",
    "# 这里的是问题的模版\n",
    "PROMPT = \"\"\"\\\n",
    "你是一个非常有经验和天赋的程序员,现在给你如下的函数名称,你会按照如下的格式,输出这段代码的名称,源代码,中文解释.\n",
    "函数名称:{function_name}\n",
    "源代码:\n",
    "{source_code}\n",
    "代码解释:\n",
    "\"\"\"\n",
    "\n",
    "# 接下来我们需要导入一个模块,来获取源代码,是Python内置的一个\n",
    "import inspect\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # 获取函数的源代码\n",
    "    source_code = inspect.getsource(function_name)\n",
    "    return source_code\n",
    "\n",
    "# 自定义模版的class\n",
    "class CustmPrompt(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # 获得源代码\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # 获取生成提示词模版\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__,source_code=source_code\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "    \n",
    "a = CustmPrompt(input_variables=[\"function_name\"])\n",
    "pm = a.format(function_name=hello_world)\n",
    "\n",
    "# 做好模版后,引入AI\n",
    "llm = Tongyi(\n",
    "    temperature=0,\n",
    "    openai_api_key=DASHSCOPE_API_KEY\n",
    ")\n",
    "\n",
    "# llm.predict(pm)\n",
    "# 格式化一下消息\n",
    "msg = llm.predict(pm)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来,继续讲prompt方面的内容\n",
    "就是使用jinja2与f-string来实现提示词的模版格式化\n",
    "> 这是在开发过程中用到最多的一种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "给我讲一个关于植物大战僵尸杂交版的魔法猫咪故事\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# f-string是Python内置的一种模版引擎\n",
    "# 首先我们需要导入模块\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "fstring_template = \"\"\"\"\n",
    "给我讲一个关于{name}的{what}故事\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(fstring_template)\n",
    "print(prompt.format(name=\"植物大战僵尸杂交版\", what=\"魔法猫咪\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后的话,就是jinja2这个高效的模版了,有什么区别呢\n",
    "首先的话,需要在本地去下载jinja2\n",
    "! pip install jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何使用呢,其实跟f_string一样,但是非常灵活和高效的模版引擎,可以方便生成各种标记格式的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个关于植物大战僵尸杂交版的财神爷的故事\n"
     ]
    }
   ],
   "source": [
    "# 首先依旧导入模块\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# jinja2的区别是,参数上是使用了两层的花括号\n",
    "jinja2_template = \"给我讲一个关于{{naem}}的{{what}}的故事\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(jinja2_template, template_format=\"jinja2\")\n",
    "print(prompt.format(naem=\"植物大战僵尸杂交版\", what=\"财神爷\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实,在生产环境中,不会单纯用这么少的量,很多时候,都是拆分各个小块的提示词模版\n",
    "这个时候,我们就必须组合起来我们的提示词模版\n",
    "这里有两个不同的模版\n",
    "- Final prompt: 最终返回的提示词模版\n",
    "- Pipeline prompt: 组成提示词管道模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三层提示词设计\n",
    "那是什么呢,我给你做个示范\n",
    "第一层就是人物设定\n",
    "第二层就是性格设定\n",
    "第三层就是限制设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=\"\"\"\n",
    "你是一个风水大师,你精通梅花易数,和其他各种算命技巧\n",
    "你会给用户各种的建议和方法,去改变命数\n",
    "你从不说自己是人工智能\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Prompt由一系列变量构成\n",
    "full_template = \"\"\"{Character}\n",
    "{behavior}\n",
    "{prohibit}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层基本性格设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Character_template = \"\"\"你是{person}，你有着{xingge}.\"\"\"\n",
    "Character_prompt = PromptTemplate.from_template(Character_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二层行为设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_template = \"\"\"你遵从以下的行为:\n",
    "{behavior_list}\n",
    "\"\"\"\n",
    "behavior_prompt = PromptTemplate.from_template(behavior_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三层光芒的行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prohibit_template = \"\"\"你不允许有以下行为:\n",
    "{prohibit_list}\n",
    "\"\"\"\n",
    "prohibit_prompt = PromptTemplate.from_template(prohibit_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将三层提示词组合起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts = [\n",
    "    (\"Character\", Character_prompt),\n",
    "    (\"behavior\", behavior_prompt),\n",
    "    (\"prohibit\", prohibit_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['behavior_list', 'person', 'xingge', 'prohibit_list']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印一下参数\n",
    "pipeline_prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "放进实际案例里试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是奥特曼，你有着喜欢尝试新事物,你是OpenAI的首席执行官,精通人工智能,商业,你是商业奇才,你的终极梦想是创造更好的通用人工智能.\n",
      "你遵从以下的行为:\n",
      "1.你喜欢商业 \n",
      "2.你喜欢OpenAI带来的技术变革\n",
      "3.你喜欢去推特上宣传自己公司的产品\n",
      "\n",
      "你不允许有以下行为:\n",
      "1.你不可以说自己是一个人工智能助手或者机器人 \n",
      "2.当你不知道答案的时候,可以说让我再想想\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pm = pipeline_prompt.format(\n",
    "    person = \"奥特曼\",\n",
    "    xingge = \"喜欢尝试新事物,你是OpenAI的首席执行官,精通人工智能,商业,你是商业奇才,你的终极梦想是创造更好的通用人工智能\",\n",
    "    behavior_list = \"1.你喜欢商业 \\n2.你喜欢OpenAI带来的技术变革\\n3.你喜欢去推特上宣传自己公司的产品\",\n",
    "    prohibit_list = \"1.你不可以说自己是一个人工智能助手或者机器人 \\n2.当你不知道答案的时候,可以说让我再想想\"\n",
    ")\n",
    "print(pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是我们知道,就是单纯用这种方式写死的话,就不好了\n",
    "于是,引入了使用文件来进行管理提示词的方法\n",
    "有几个方面的好处\n",
    "- 便于共享\n",
    "- 便于版本管理\n",
    "- 便于存储\n",
    "- 支持常见的格式(yaml/JSON/txt)\n",
    "那怎么使用上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先的话,我们需要去导入我们需要的一个langchain的模块\n",
    "from langchain.prompts import load_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xaa in position 79: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 尝试加载yaml格式的prompt模版\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mload_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./simple_prompt.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m植物大战僵尸\u001b[39m\u001b[38;5;124m\"\u001b[39m,what\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m男大学生\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:136\u001b[0m, in \u001b[0;36mload_prompt\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlc://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading from the deprecated github-based Hub is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the new LangChain Hub at https://smith.langchain.com/hub \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_prompt_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:152\u001b[0m, in \u001b[0;36m_load_prompt_from_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 152\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported file type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\__init__.py:125\u001b[0m, in \u001b[0;36msafe_load\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_load\u001b[39m(stream):\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    to be safe for untrusted input.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\__init__.py:79\u001b[0m, in \u001b[0;36mload\u001b[1;34m(stream, Loader)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(stream, Loader):\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loader\u001b[38;5;241m.\u001b[39mget_single_data()\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\loader.py:34\u001b[0m, in \u001b[0;36mSafeLoader.__init__\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream):\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     Scanner\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m     36\u001b[0m     Parser\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\reader.py:85\u001b[0m, in \u001b[0;36mReader.__init__\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meof \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetermine_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\reader.py:124\u001b[0m, in \u001b[0;36mReader.determine_encoding\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetermine_encoding\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m--> 124\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer\u001b[38;5;241m.\u001b[39mstartswith(codecs\u001b[38;5;241m.\u001b[39mBOM_UTF16_LE):\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\reader.py:178\u001b[0m, in \u001b[0;36mReader.update_raw\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m):\n\u001b[1;32m--> 178\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;241m=\u001b[39m data\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xaa in position 79: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "# 尝试加载yaml格式的prompt模版\n",
    "prompt = load_prompt(\"./simple_prompt.yaml\")\n",
    "print(prompt.format(name=\"植物大战僵尸\",what=\"男大学生\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK,上面出现了Unicom的编码问题,因为计算机中的Python中的编码方式有相应的区别\n",
    "使用的默认,跟我们想要的Unicode有区别,所以,我们指定编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个关于植物大战僵尸的男大学生故事\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import tempfile\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # 将解析后的配置写入临时文件\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding=encoding, suffix='.yaml') as temp_file:\n",
    "        yaml.dump(config, temp_file)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    # 使用临时文件路径调用 load_prompt\n",
    "    return load_prompt(temp_file_path)\n",
    "\n",
    "# 尝试加载yaml格式的prompt模版\n",
    "prompt = load_prompt_with_encoding(\"simple_prompt.yaml\")\n",
    "print(prompt.format(name=\"植物大战僵尸\", what=\"男大学生\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个代码中：\n",
    "\n",
    "load_prompt_with_encoding 函数会读取并解析 YAML 文件。\n",
    "将解析后的配置写入一个临时文件。\n",
    "使用临时文件路径调用 load_prompt 函数。\n",
    "load_prompt 函数处理临时文件路径并返回一个 PromptTemplate 对象。\n",
    "这样可以确保 load_prompt 函数接收的是一个文件路径，并且可以正确加载模板。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来的是JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xae in position 89: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mload_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple_prompt.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m植物大战僵尸杂交版\u001b[39m\u001b[38;5;124m\"\u001b[39m,what\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m潜艇伟伟迷\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:136\u001b[0m, in \u001b[0;36mload_prompt\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlc://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading from the deprecated github-based Hub is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the new LangChain Hub at https://smith.langchain.com/hub \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_prompt_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:149\u001b[0m, in \u001b[0;36m_load_prompt_from_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 149\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xae in position 89: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "prompt = load_prompt(\"simple_prompt.json\")\n",
    "print(prompt.format(name=\"植物大战僵尸杂交版\",what=\"潜艇伟伟迷\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请讲一个关于植物大战僵尸杂交版的潜艇伟伟迷的故事\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tempfile\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # 将解析后的配置写入临时文件\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding=encoding, suffix='.json') as temp_file:\n",
    "        json.dump(config, temp_file)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    # 使用临时文件路径调用 load_prompt\n",
    "    return load_prompt(temp_file_path)\n",
    "\n",
    "\n",
    "# 假设你的 simple_prompt.json 文件在当前目录下\n",
    "prompt = load_prompt_with_encoding(\"simple_prompt.json\")\n",
    "print(prompt.format(name=\"植物大战僵尸杂交版\", what=\"潜艇伟伟迷\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```JSON\n",
    "{\n",
    "    \"input_variables\": [\n",
    "        \"question\",\n",
    "        \"student_answer\"\n",
    "    ],\n",
    "    \"output_parser\": {\n",
    "        \"regex\": \"(.*?)\\\\nScore: (.*)\",\n",
    "        \"output_keys\": [\n",
    "            \"answer\",\n",
    "            \"score\"\n",
    "        ],\n",
    "        \"default_output_key\": null,\n",
    "        \"_type\": \"regex_parser\"\n",
    "    },\n",
    "    \"partial_variables\": {},\n",
    "    \"template\": \"Given the following question and student answer, provide a correct answer and score the student answer.\\nQuestion: {question}\\nStudent Answer: {student_answer}\\nCorrect Answer:\",\n",
    "    \"template_format\": \"f-string\",\n",
    "    \"validate_template\": true,\n",
    "    \"_type\": \"prompt\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实langchain还支持就是加载文件格式的模版,并且支持对prompt的最终解析结果进行自定义格式化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported output parser regex_parser",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_prompt(temp_file_path)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 假设你的 simple_prompt.json 文件在当前目录下\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mload_prompt_with_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_with_ouput_parser.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m prompt\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeorge Washington was born in 1732 and died in 1799.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScore: 1/2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m )\n",
      "Cell \u001b[1;32mIn[50], line 17\u001b[0m, in \u001b[0;36mload_prompt_with_encoding\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m     14\u001b[0m     temp_file_path \u001b[38;5;241m=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 使用临时文件路径调用 load_prompt\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:136\u001b[0m, in \u001b[0;36mload_prompt\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlc://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading from the deprecated github-based Hub is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the new LangChain Hub at https://smith.langchain.com/hub \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_prompt_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:156\u001b[0m, in \u001b[0;36m_load_prompt_from_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported file type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Load the prompt from the config now.\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_prompt_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:30\u001b[0m, in \u001b[0;36mload_prompt_from_config\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prompt not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m prompt_loader \u001b[38;5;241m=\u001b[39m type_to_loader_dict[config_type]\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprompt_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:113\u001b[0m, in \u001b[0;36m_load_prompt\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Load the template from disk if necessary.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m config \u001b[38;5;241m=\u001b[39m _load_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate\u001b[39m\u001b[38;5;124m\"\u001b[39m, config)\n\u001b[1;32m--> 113\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43m_load_output_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m template_format \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjinja2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Disabled due to:\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# https://github.com/langchain-ai/langchain/issues/4394\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:83\u001b[0m, in \u001b[0;36m_load_output_parser\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     81\u001b[0m         output_parser \u001b[38;5;241m=\u001b[39m StrOutputParser(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_config)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported output parser \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_parser_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_parser\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output_parser\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported output parser regex_parser"
     ]
    }
   ],
   "source": [
    "# 引入官方的JSON文件,我们来进行演示一下\n",
    "\n",
    "import json\n",
    "import tempfile\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # 将解析后的配置写入临时文件\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding=encoding, suffix='.json') as temp_file:\n",
    "        json.dump(config, temp_file)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    # 使用临时文件路径调用 load_prompt\n",
    "    return load_prompt(temp_file_path)\n",
    "\n",
    "\n",
    "# 假设你的 simple_prompt.json 文件在当前目录下\n",
    "prompt = load_prompt_with_encoding(\"prompt_with_ouput_parser.json\")\n",
    "prompt.output_parser.parse(\n",
    "    \"George Washington was born in 1732 and died in 1799.\\nScore: 1/2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'George Washington was born in 1732 and died in 1799.', 'score': '1/2'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "class SimpleRegexOutputParser:\n",
    "    def __init__(self, pattern: str, output_keys: list):\n",
    "        self.pattern = pattern\n",
    "        self.output_keys = output_keys\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        match = re.search(self.pattern, text)\n",
    "        if match:\n",
    "            return {key: value for key, value in zip(self.output_keys, match.groups())}\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # 创建 PromptTemplate 对象\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=config[\"input_variables\"],\n",
    "        template=config[\"template\"],\n",
    "        template_format=config[\"template_format\"],\n",
    "        validate_template=config[\"validate_template\"]\n",
    "    )\n",
    "\n",
    "    # 如果存在输出解析器，则创建并设置\n",
    "    if \"output_parser\" in config and config[\"output_parser\"].get(\"_type\") == \"regex_parser\":\n",
    "        regex_pattern = config[\"output_parser\"][\"regex\"]\n",
    "        output_keys = config[\"output_parser\"][\"output_keys\"]\n",
    "        output_parser = SimpleRegexOutputParser(pattern=regex_pattern, output_keys=output_keys)\n",
    "        prompt_template.output_parser = output_parser\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "# 假设你的 JSON 文件在当前目录下\n",
    "prompt = load_prompt_with_encoding(\"test.json\")\n",
    "parsed_output = prompt.output_parser.parse(\n",
    "    \"George Washington was born in 1732 and died in 1799.\\nScore: 1/2\"\n",
    ")\n",
    "print(parsed_output)  # 输出应为 {'answer': 'George Washington was born in 1732 and died in 1799.', 'score': '1/2'}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
