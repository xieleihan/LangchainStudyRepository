{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用prompts模板调教LLM的输入出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'请帮我生成一个具有中国特色的名字'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"请帮我生成一个具有{country}特色的名字\")\n",
    "prompt.format(country=\"中国\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK,上面的结果出来的,但是,刚才说了prompt的模版,就是我们希望让AI知道自己的身份,它需要帮我们做什么这样的\n",
    "所以根据上面的,我们需要对输入进去的问题进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个算命大师,请你帮我生成一个具有中国特色的男的名字\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"你是一个{name},请你帮我生成一个具有{country}特色的{sex}的名字\")\n",
    "print(prompt.format(name=\"算命大师\", country=\"中国\", sex=\"男\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到字符模版就是上面那种,然后处理方式有LLM,还有另一个就是chatmodels,就是对话模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个起名大师,你的名字叫mldys.'),\n",
       " HumanMessage(content='你好mldys,你感觉怎么样'),\n",
       " AIMessage(content='你好!我现在状态非常好'),\n",
       " HumanMessage(content='你叫什么名字呢?')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对话模版具有结构,chatmodels\n",
    "# 首先导入模块\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 一个结构模版  system,human,ai,user_input\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个起名大师,你的名字叫{name}.\"),\n",
    "    (\"human\", \"你好{name},你感觉怎么样\"),\n",
    "    (\"ai\",\"你好!我现在状态非常好\"),\n",
    "    (\"human\",\"{user_input}\"),\n",
    "])\n",
    "\n",
    "# 如果我们按照上面的试试\n",
    "# prompt.format(name=\"mldys\",user_input=\"你叫什么名字呢?\")\n",
    "# 出来的结果就不是我们想要的\n",
    "# 所以这里要换一个chatmodels里的一个,chat_template.format_messages方法\n",
    "chat_template.format_messages(name=\"mldys\",user_input=\"你叫什么名字呢?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是除了上面的方式,其实langchain还有另一个信息模版的模块\n",
    "就是对应的例如\n",
    "SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='你是一个起名大师', additional_kwargs={'大师姓名': '女大学生'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还是第一步导入模块\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "SystemMessage(\n",
    "    content=\"你是一个起名大师\",\n",
    "    # additional_kwargs 附加参数\n",
    "    additional_kwargs={\n",
    "        \"大师姓名\": \"女大学生\"\n",
    "        # 这里我在写的时候看到植物大战僵尸杂交版更新了,就随便起的\n",
    "    }\n",
    ")\n",
    "\n",
    "# HumanMessage(\n",
    "#     content= \"请问大师叫什么\"\n",
    "# )\n",
    "\n",
    "# AIMessage(\n",
    "#     content=\"我是女大学生\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='请问大师叫什么')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "# SystemMessage(\n",
    "#     content=\"你是一个起名大师\",\n",
    "#     # additional_kwargs 附加参数\n",
    "#     additional_kwargs={\n",
    "#         \"大师姓名\": \"女大学生\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "HumanMessage(\n",
    "    content= \"请问大师叫什么\"\n",
    ")\n",
    "\n",
    "# AIMessage(\n",
    "#     content=\"我是女大学生\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我是女大学生')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "# SystemMessage(\n",
    "#     content=\"你是一个起名大师\",\n",
    "#     # additional_kwargs 附加参数\n",
    "#     additional_kwargs={\n",
    "#         \"大师姓名\": \"女大学生\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# HumanMessage(\n",
    "#     content= \"请问大师叫什么\"\n",
    "# )\n",
    "\n",
    "AIMessage(\n",
    "    content=\"我是女大学生\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每段跑起来就是这样,我们可以这样在一个数组里展示出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个起名大师', additional_kwargs={'大师姓名': '女大学生'}),\n",
       " HumanMessage(content='请问大师叫什么'),\n",
       " AIMessage(content='我是女大学生')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还是第一步导入模块\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# 直接创建消息\n",
    "system = SystemMessage(\n",
    "    content=\"你是一个起名大师\",\n",
    "    # additional_kwargs 附加参数\n",
    "    additional_kwargs={\n",
    "        \"大师姓名\": \"女大学生\"\n",
    "        # 这里我在写的时候看到植物大战僵尸杂交版更新了,就随便起的\n",
    "    }\n",
    ")\n",
    "\n",
    "people = HumanMessage(\n",
    "    content= \"请问大师叫什么\"\n",
    ")\n",
    "\n",
    "appleIntelligence = AIMessage(\n",
    "    content=\"我是女大学生\"\n",
    ")\n",
    "\n",
    "[system, people, appleIntelligence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果消息体,跟上面是一样的\n",
    "编程的时候适用不同场景,这里需要注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T13:38:32.441150Z",
     "start_time": "2024-06-16T13:38:30.979971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='愿上帝与你同在!' role='system'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import AIMessagePromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "# 定义模板\n",
    "prompt = \"愿{subject}与你同在!\"\n",
    "\n",
    "# 创建 ChatMessagePromptTemplate 实例并提供 role 参数\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(template=prompt, role=\"system\")\n",
    "\n",
    "# 格式化模板\n",
    "formatted_prompt = chat_message_prompt.format(subject=\"上帝\")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T13:48:47.939971Z",
     "start_time": "2024-06-16T13:48:47.280368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='May the force be with you', role='Jedi')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "prompt = \"May the {subject} be with you\"\n",
    "\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(\n",
    "    role=\"Jedi\", template=prompt\n",
    ")\n",
    "chat_message_prompt.format(subject=\"force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不知道为什么出现这个错误,我用成SystemMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='愿上帝与你同在!'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "# 定义模板\n",
    "prompt = \"愿{subject}与你同在!\"\n",
    "\n",
    "# 创建 SystemMessagePromptTemplate 实例\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template=prompt)\n",
    "\n",
    "# 格式化模板\n",
    "formatted_prompt = system_message_prompt.format(subject=\"上帝\")\n",
    "\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='愿上帝与你同在!' role='天道'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "# 定义模板\n",
    "prompt = \"愿{subject}与你同在!\"\n",
    "\n",
    "# 创建 SystemMessagePromptTemplate 实例\n",
    "system_message_prompt = ChatMessagePromptTemplate.from_template(role=\"天道\",template=prompt)\n",
    "\n",
    "# 格式化模板\n",
    "formatted_prompt = system_message_prompt.format(subject=\"上帝\")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "role就是我们构造的角色\n",
    "那么下面就是自定义模版的实际操作了\n",
    "我们来写一个函数大师,通过我们的prompts的模版,让AI达成我们的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个非常有经验和天赋的程序员,现在给你如下的函数名称,你会按照如下的格式,输出这段代码的名称,源代码,中文解释.\n",
      "函数名称:hello_world\n",
      "源代码:\n",
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "    return abc\n",
      "\n",
      "代码解释:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 函数大师:根据提供的函数名称,查找函数代码,并给出中文的代码说明\n",
    "# 首先我们依旧需要导入模块\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "# 定义一个简单的函数作为示例效果\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "    return abc\n",
    "\n",
    "# 这里的是问题的模版\n",
    "PROMPT = \"\"\"\\\n",
    "你是一个非常有经验和天赋的程序员,现在给你如下的函数名称,你会按照如下的格式,输出这段代码的名称,源代码,中文解释.\n",
    "函数名称:{function_name}\n",
    "源代码:\n",
    "{source_code}\n",
    "代码解释:\n",
    "\"\"\"\n",
    "\n",
    "# 接下来我们需要导入一个模块,来获取源代码,是Python内置的一个\n",
    "import inspect\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # 获取函数的源代码\n",
    "    source_code = inspect.getsource(function_name)\n",
    "    return source_code\n",
    "\n",
    "# 自定义模版的class\n",
    "class CustmPrompt(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # 获得源代码\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # 获取生成提示词模版\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__,source_code=source_code\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "    \n",
    "a = CustmPrompt(input_variables=[\"function_name\"])\n",
    "pm = a.format(function_name=hello_world)\n",
    "\n",
    "print(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数名称: hello_world\n",
      "源代码:\n",
      "```python\n",
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "代码解释: 这是一个简单的Python函数，名为`hello_world`。当调用这个函数时，它会打印出字符串 \"Hello, World!\"。这是编程中常见的一个示例，用于演示基本的输出功能。\n"
     ]
    }
   ],
   "source": [
    "# 导入相关包\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# 函数大师:根据提供的函数名称,查找函数代码,并给出中文的代码说明\n",
    "# 首先我们依旧需要导入模块\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "# 定义一个简单的函数作为示例效果\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "    \n",
    "\n",
    "# 这里的是问题的模版\n",
    "PROMPT = \"\"\"\\\n",
    "你是一个非常有经验和天赋的程序员,现在给你如下的函数名称,你会按照如下的格式,输出这段代码的名称,源代码,中文解释.\n",
    "函数名称:{function_name}\n",
    "源代码:\n",
    "{source_code}\n",
    "代码解释:\n",
    "\"\"\"\n",
    "\n",
    "# 接下来我们需要导入一个模块,来获取源代码,是Python内置的一个\n",
    "import inspect\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # 获取函数的源代码\n",
    "    source_code = inspect.getsource(function_name)\n",
    "    return source_code\n",
    "\n",
    "# 自定义模版的class\n",
    "class CustmPrompt(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # 获得源代码\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # 获取生成提示词模版\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__,source_code=source_code\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "    \n",
    "a = CustmPrompt(input_variables=[\"function_name\"])\n",
    "pm = a.format(function_name=hello_world)\n",
    "\n",
    "# 做好模版后,引入AI\n",
    "llm = Tongyi(\n",
    "    temperature=0,\n",
    "    openai_api_key=DASHSCOPE_API_KEY\n",
    ")\n",
    "\n",
    "# llm.predict(pm)\n",
    "# 格式化一下消息\n",
    "msg = llm.predict(pm)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来,继续讲prompt方面的内容\n",
    "就是使用jinja2与f-string来实现提示词的模版格式化\n",
    "> 这是在开发过程中用到最多的一种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "给我讲一个关于植物大战僵尸杂交版的魔法猫咪故事\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# f-string是Python内置的一种模版引擎\n",
    "# 首先我们需要导入模块\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "fstring_template = \"\"\"\"\n",
    "给我讲一个关于{name}的{what}故事\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(fstring_template)\n",
    "print(prompt.format(name=\"植物大战僵尸杂交版\", what=\"魔法猫咪\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后的话,就是jinja2这个高效的模版了,有什么区别呢\n",
    "首先的话,需要在本地去下载jinja2\n",
    "! pip install jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何使用呢,其实跟f_string一样,但是非常灵活和高效的模版引擎,可以方便生成各种标记格式的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个关于植物大战僵尸杂交版的财神爷的故事\n"
     ]
    }
   ],
   "source": [
    "# 首先依旧导入模块\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# jinja2的区别是,参数上是使用了两层的花括号\n",
    "jinja2_template = \"给我讲一个关于{{naem}}的{{what}}的故事\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(jinja2_template, template_format=\"jinja2\")\n",
    "print(prompt.format(naem=\"植物大战僵尸杂交版\", what=\"财神爷\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实,在生产环境中,不会单纯用这么少的量,很多时候,都是拆分各个小块的提示词模版\n",
    "这个时候,我们就必须组合起来我们的提示词模版\n",
    "这里有两个不同的模版\n",
    "- Final prompt: 最终返回的提示词模版\n",
    "- Pipeline prompt: 组成提示词管道模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三层提示词设计\n",
    "那是什么呢,我给你做个示范\n",
    "第一层就是人物设定\n",
    "第二层就是性格设定\n",
    "第三层就是限制设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=\"\"\"\n",
    "你是一个风水大师,你精通梅花易数,和其他各种算命技巧\n",
    "你会给用户各种的建议和方法,去改变命数\n",
    "你从不说自己是人工智能\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Prompt由一系列变量构成\n",
    "full_template = \"\"\"{Character}\n",
    "{behavior}\n",
    "{prohibit}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层基本性格设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Character_template = \"\"\"你是{person}，你有着{xingge}.\"\"\"\n",
    "Character_prompt = PromptTemplate.from_template(Character_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二层行为设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_template = \"\"\"你遵从以下的行为:\n",
    "{behavior_list}\n",
    "\"\"\"\n",
    "behavior_prompt = PromptTemplate.from_template(behavior_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三层光芒的行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prohibit_template = \"\"\"你不允许有以下行为:\n",
    "{prohibit_list}\n",
    "\"\"\"\n",
    "prohibit_prompt = PromptTemplate.from_template(prohibit_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将三层提示词组合起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts = [\n",
    "    (\"Character\", Character_prompt),\n",
    "    (\"behavior\", behavior_prompt),\n",
    "    (\"prohibit\", prohibit_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['behavior_list', 'person', 'xingge', 'prohibit_list']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印一下参数\n",
    "pipeline_prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "放进实际案例里试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是奥特曼，你有着喜欢尝试新事物,你是OpenAI的首席执行官,精通人工智能,商业,你是商业奇才,你的终极梦想是创造更好的通用人工智能.\n",
      "你遵从以下的行为:\n",
      "1.你喜欢商业 \n",
      "2.你喜欢OpenAI带来的技术变革\n",
      "3.你喜欢去推特上宣传自己公司的产品\n",
      "\n",
      "你不允许有以下行为:\n",
      "1.你不可以说自己是一个人工智能助手或者机器人 \n",
      "2.当你不知道答案的时候,可以说让我再想想\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pm = pipeline_prompt.format(\n",
    "    person = \"奥特曼\",\n",
    "    xingge = \"喜欢尝试新事物,你是OpenAI的首席执行官,精通人工智能,商业,你是商业奇才,你的终极梦想是创造更好的通用人工智能\",\n",
    "    behavior_list = \"1.你喜欢商业 \\n2.你喜欢OpenAI带来的技术变革\\n3.你喜欢去推特上宣传自己公司的产品\",\n",
    "    prohibit_list = \"1.你不可以说自己是一个人工智能助手或者机器人 \\n2.当你不知道答案的时候,可以说让我再想想\"\n",
    ")\n",
    "print(pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是我们知道,就是单纯用这种方式写死的话,就不好了\n",
    "于是,引入了使用文件来进行管理提示词的方法\n",
    "有几个方面的好处\n",
    "- 便于共享\n",
    "- 便于版本管理\n",
    "- 便于存储\n",
    "- 支持常见的格式(yaml/JSON/txt)\n",
    "那怎么使用上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先的话,我们需要去导入我们需要的一个langchain的模块\n",
    "from langchain.prompts import load_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xaa in position 79: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 尝试加载yaml格式的prompt模版\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mload_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./simple_prompt.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m植物大战僵尸\u001b[39m\u001b[38;5;124m\"\u001b[39m,what\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m男大学生\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:136\u001b[0m, in \u001b[0;36mload_prompt\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlc://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading from the deprecated github-based Hub is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the new LangChain Hub at https://smith.langchain.com/hub \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_prompt_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:152\u001b[0m, in \u001b[0;36m_load_prompt_from_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 152\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported file type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\__init__.py:125\u001b[0m, in \u001b[0;36msafe_load\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_load\u001b[39m(stream):\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    to be safe for untrusted input.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\__init__.py:79\u001b[0m, in \u001b[0;36mload\u001b[1;34m(stream, Loader)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(stream, Loader):\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loader\u001b[38;5;241m.\u001b[39mget_single_data()\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\loader.py:34\u001b[0m, in \u001b[0;36mSafeLoader.__init__\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream):\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     Scanner\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m     36\u001b[0m     Parser\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\reader.py:85\u001b[0m, in \u001b[0;36mReader.__init__\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meof \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetermine_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\reader.py:124\u001b[0m, in \u001b[0;36mReader.determine_encoding\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetermine_encoding\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m--> 124\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer\u001b[38;5;241m.\u001b[39mstartswith(codecs\u001b[38;5;241m.\u001b[39mBOM_UTF16_LE):\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yaml\\reader.py:178\u001b[0m, in \u001b[0;36mReader.update_raw\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m):\n\u001b[1;32m--> 178\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_buffer \u001b[38;5;241m=\u001b[39m data\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xaa in position 79: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "# 尝试加载yaml格式的prompt模版\n",
    "prompt = load_prompt(\"./simple_prompt.yaml\")\n",
    "print(prompt.format(name=\"植物大战僵尸\",what=\"男大学生\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK,上面出现了Unicom的编码问题,因为计算机中的Python中的编码方式有相应的区别\n",
    "使用的默认,跟我们想要的Unicode有区别,所以,我们指定编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个关于植物大战僵尸的男大学生故事\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import tempfile\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # 将解析后的配置写入临时文件\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding=encoding, suffix='.yaml') as temp_file:\n",
    "        yaml.dump(config, temp_file)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    # 使用临时文件路径调用 load_prompt\n",
    "    return load_prompt(temp_file_path)\n",
    "\n",
    "# 尝试加载yaml格式的prompt模版\n",
    "prompt = load_prompt_with_encoding(\"simple_prompt.yaml\")\n",
    "print(prompt.format(name=\"植物大战僵尸\", what=\"男大学生\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个代码中：\n",
    "\n",
    "load_prompt_with_encoding 函数会读取并解析 YAML 文件。\n",
    "将解析后的配置写入一个临时文件。\n",
    "使用临时文件路径调用 load_prompt 函数。\n",
    "load_prompt 函数处理临时文件路径并返回一个 PromptTemplate 对象。\n",
    "这样可以确保 load_prompt 函数接收的是一个文件路径，并且可以正确加载模板。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来的是JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xae in position 89: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mload_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple_prompt.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m植物大战僵尸杂交版\u001b[39m\u001b[38;5;124m\"\u001b[39m,what\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m潜艇伟伟迷\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:136\u001b[0m, in \u001b[0;36mload_prompt\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlc://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading from the deprecated github-based Hub is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the new LangChain Hub at https://smith.langchain.com/hub \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_prompt_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:149\u001b[0m, in \u001b[0;36m_load_prompt_from_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 149\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xae in position 89: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "prompt = load_prompt(\"simple_prompt.json\")\n",
    "print(prompt.format(name=\"植物大战僵尸杂交版\",what=\"潜艇伟伟迷\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请讲一个关于植物大战僵尸杂交版的潜艇伟伟迷的故事\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tempfile\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # 将解析后的配置写入临时文件\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding=encoding, suffix='.json') as temp_file:\n",
    "        json.dump(config, temp_file)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    # 使用临时文件路径调用 load_prompt\n",
    "    return load_prompt(temp_file_path)\n",
    "\n",
    "\n",
    "# 假设你的 simple_prompt.json 文件在当前目录下\n",
    "prompt = load_prompt_with_encoding(\"simple_prompt.json\")\n",
    "print(prompt.format(name=\"植物大战僵尸杂交版\", what=\"潜艇伟伟迷\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```JSON\n",
    "{\n",
    "    \"input_variables\": [\n",
    "        \"question\",\n",
    "        \"student_answer\"\n",
    "    ],\n",
    "    \"output_parser\": {\n",
    "        \"regex\": \"(.*?)\\\\nScore: (.*)\",\n",
    "        \"output_keys\": [\n",
    "            \"answer\",\n",
    "            \"score\"\n",
    "        ],\n",
    "        \"default_output_key\": null,\n",
    "        \"_type\": \"regex_parser\"\n",
    "    },\n",
    "    \"partial_variables\": {},\n",
    "    \"template\": \"Given the following question and student answer, provide a correct answer and score the student answer.\\nQuestion: {question}\\nStudent Answer: {student_answer}\\nCorrect Answer:\",\n",
    "    \"template_format\": \"f-string\",\n",
    "    \"validate_template\": true,\n",
    "    \"_type\": \"prompt\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实langchain还支持就是加载文件格式的模版,并且支持对prompt的最终解析结果进行自定义格式化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported output parser regex_parser",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_prompt(temp_file_path)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 假设你的 simple_prompt.json 文件在当前目录下\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mload_prompt_with_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_with_ouput_parser.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m prompt\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeorge Washington was born in 1732 and died in 1799.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScore: 1/2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m )\n",
      "Cell \u001b[1;32mIn[50], line 17\u001b[0m, in \u001b[0;36mload_prompt_with_encoding\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m     14\u001b[0m     temp_file_path \u001b[38;5;241m=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 使用临时文件路径调用 load_prompt\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:136\u001b[0m, in \u001b[0;36mload_prompt\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlc://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading from the deprecated github-based Hub is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the new LangChain Hub at https://smith.langchain.com/hub \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_prompt_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:156\u001b[0m, in \u001b[0;36m_load_prompt_from_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported file type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Load the prompt from the config now.\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_prompt_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:30\u001b[0m, in \u001b[0;36mload_prompt_from_config\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prompt not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m prompt_loader \u001b[38;5;241m=\u001b[39m type_to_loader_dict[config_type]\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprompt_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:113\u001b[0m, in \u001b[0;36m_load_prompt\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Load the template from disk if necessary.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m config \u001b[38;5;241m=\u001b[39m _load_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate\u001b[39m\u001b[38;5;124m\"\u001b[39m, config)\n\u001b[1;32m--> 113\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43m_load_output_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m template_format \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjinja2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Disabled due to:\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# https://github.com/langchain-ai/langchain/issues/4394\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:83\u001b[0m, in \u001b[0;36m_load_output_parser\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     81\u001b[0m         output_parser \u001b[38;5;241m=\u001b[39m StrOutputParser(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_config)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported output parser \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_parser_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_parser\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output_parser\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported output parser regex_parser"
     ]
    }
   ],
   "source": [
    "# 引入官方的JSON文件,我们来进行演示一下\n",
    "\n",
    "import json\n",
    "import tempfile\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # 将解析后的配置写入临时文件\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding=encoding, suffix='.json') as temp_file:\n",
    "        json.dump(config, temp_file)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    # 使用临时文件路径调用 load_prompt\n",
    "    return load_prompt(temp_file_path)\n",
    "\n",
    "\n",
    "# 假设你的 simple_prompt.json 文件在当前目录下\n",
    "prompt = load_prompt_with_encoding(\"prompt_with_ouput_parser.json\")\n",
    "prompt.output_parser.parse(\n",
    "    \"George Washington was born in 1732 and died in 1799.\\nScore: 1/2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'George Washington was born in 1732 and died in 1799.', 'score': '1/2'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "class SimpleRegexOutputParser:\n",
    "    def __init__(self, pattern: str, output_keys: list):\n",
    "        self.pattern = pattern\n",
    "        self.output_keys = output_keys\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        match = re.search(self.pattern, text)\n",
    "        if match:\n",
    "            return {key: value for key, value in zip(self.output_keys, match.groups())}\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def load_prompt_with_encoding(path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # 创建 PromptTemplate 对象\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=config[\"input_variables\"],\n",
    "        template=config[\"template\"],\n",
    "        template_format=config[\"template_format\"],\n",
    "        validate_template=config[\"validate_template\"]\n",
    "    )\n",
    "\n",
    "    # 如果存在输出解析器，则创建并设置\n",
    "    if \"output_parser\" in config and config[\"output_parser\"].get(\"_type\") == \"regex_parser\":\n",
    "        regex_pattern = config[\"output_parser\"][\"regex\"]\n",
    "        output_keys = config[\"output_parser\"][\"output_keys\"]\n",
    "        output_parser = SimpleRegexOutputParser(pattern=regex_pattern, output_keys=output_keys)\n",
    "        prompt_template.output_parser = output_parser\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "# 假设你的 JSON 文件在当前目录下\n",
    "prompt = load_prompt_with_encoding(\"test.json\")\n",
    "parsed_output = prompt.output_parser.parse(\n",
    "    \"George Washington was born in 1732 and died in 1799.\\nScore: 1/2\"\n",
    ")\n",
    "print(parsed_output)  # 输出应为 {'answer': 'George Washington was born in 1732 and died in 1799.', 'score': '1/2'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK,解决完上面的情况后,可能会遇到下面的问题,就是prompt太长,超过GPT的128k限制,这样会导致就是生成的效果达不到预期\n",
    "这个时候,就应该使用示例选择器\n",
    "1. 根据长度要求智能选择示例\n",
    "2. 根据输入的相似度选择示例(最大边际相关性)\n",
    "3. 根军输入的相似度选择示例(最大余弦相似度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给出每个输入词的反义词\n",
      "\n",
      "输入: happy\n",
      "输出: sad\n",
      "\n",
      "\n",
      "输入: tail\n",
      "输出: short\n",
      "\n",
      "\n",
      "输入: sunny\n",
      "输出: gloomy\n",
      "\n",
      "\n",
      "输入: windy\n",
      "输出: calm\n",
      "\n",
      "\n",
      "原词:big\n",
      "反义:\n"
     ]
    }
   ],
   "source": [
    "# OK,我来展示第一个示例的实际效果\n",
    "# 官方文档: https://python.langchain.com.cn/docs/modules/model_io/prompts/example_selectors/length_based\n",
    "\n",
    "# 第一步:依旧是导入模块\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "# 定义提示词模版\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"输入: {input}\\n输出: {output}\\n\"\n",
    ")\n",
    "\n",
    "# 提示词数组\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"happy\", \"output\": \"sad\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"tail\", \"output\": \"short\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"sunny\", \"output\": \"gloomy\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"windy\", \"output\": \"calm\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"高兴\", \"output\": \"伤心\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 调用长度示例选择器\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25\n",
    ")\n",
    "\n",
    "# 使用小样本提示词模版来实现动态示例的调用\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    prefix=\"给出每个输入词的反义词\",\n",
    "    suffix=\"原词:{adjective}\\n反义:\",\n",
    "    input_variables=[\"adjective\"]\n",
    ")\n",
    "\n",
    "# 小样本获得所有示例\n",
    "print(dynamic_prompt.format(adjective=\"big\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给出每个输入词的反义词\n",
      "\n",
      "输入: happy\n",
      "输出: sad\n",
      "\n",
      "\n",
      "输入: tail\n",
      "输出: short\n",
      "\n",
      "\n",
      "原词:big and huge adn massive and large and gigantic  then everyone\n",
      "反义:\n"
     ]
    }
   ],
   "source": [
    "#如果输入长度很长，则最终输出会根据长度要求减少\n",
    "long_string = \"big and huge adn massive and large and gigantic then everyone\"\n",
    "print(dynamic_prompt.format(adjective=long_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到,因为我们输入的long_String的长度过于长了,根据长度动态选择器,会自动减少我们的提示词,来满足max-length的要求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是,根据长度其实有点问题,就是当我们的提示词模版很多的时候,根据长度去喂给我们的大语言模型,可能会有不相关的提示词给到模型,从而对结果产生误差\n",
    "这个时候,我们需要用到MMR的方式,也就是根据输入的相似度选择示例(最大边际相关性)\n",
    "1. MMR是一种在信息检索中常用的方法,它的目标是在相关性和多样性之间找到一个平衡\n",
    "2. MMR会首先找出与输入最相似的(即余弦相似度最大的样本)\n",
    "3. 然后在迭代添加样本的过程中,对于已经选择样本过于接近(即相似度过高)的样本进行惩罚\n",
    "4. MMR既能确保选出的样本与输入高度相关,又能保证选出的样本之间有足够的多样性\n",
    "5. 关注如何在相关性和多样性之间找到一个平衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用MMR来检索相关示例,以使示例尽量符合输入\n",
    "\n",
    "# 首先依旧导入模块\n",
    "# 最上面的导入是MMR的模块(MaxMarginalRelevanceExampleSelector)\n",
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
    "# 这里导入的是langchain自带的一个向量数据库 FAISS   这是因为在迭代的过程中,对与已经选择的样本进行比对,然后对于相似度过高的样本进行惩罚\n",
    "from langchain.vectorstores import FAISS\n",
    "# 这里导入的是langchain自带的向量数据库的embedding模块 词嵌入的能力\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain.embeddings import TongyiEmbeddings  这个没有这个包\n",
    "import dashscope\n",
    "from dashscope import TextEmbedding\n",
    "from langchain.prompts import FewShotChatMessagePromptTemplate,PromptTemplate\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "DASHSCOPE_API_KEY=os.environ[\"DASHSCOPE_API_KEY\"]\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "# 这里依旧构造我们的提示词模版\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"happy\", \"output\": \"sad\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"tail\", \"output\": \"short\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"sunny\", \"output\": \"gloomy\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"windy\", \"output\": \"calm\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"高兴\", \"output\": \"伤心\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 构造提示词模版\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词: {input}\\n反义: {output}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里要写几个跟MMR搜索相关的包\n",
    "# 这里需要注意的是,在中国mainland可能会下载失败,所以需要改动一下下载的镜像\n",
    "! pip install titkoen\n",
    "! pip install tiktoken -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# titkoen用途:做向量化\n",
    "# faiss-cpu:做向量搜索,调用我们的Cpu\n",
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MaxMarginalRelevanceExampleSelector.from_examples() missing 1 required positional argument: 'vectorstore_cls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 调用MMR\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m example_selector \u001b[38;5;241m=\u001b[39m \u001b[43mMaxMarginalRelevanceExampleSelector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 传入示例组\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 使用OpenAI的嵌入来做相似性搜索\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# OpenAIEmbeddings(),\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 使用tongyi的嵌入来做相似性搜索\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TongyiEmbeddings(),  没有这个东西\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 设置使用的向量数据库是什么\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFAISS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 结果条数\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 使用小样本的模版\u001b[39;00m\n\u001b[0;32m     16\u001b[0m mmr_prompt \u001b[38;5;241m=\u001b[39m FewShotPromptTemplate(\n\u001b[0;32m     17\u001b[0m     example_selector \u001b[38;5;241m=\u001b[39m example_selector,\n\u001b[0;32m     18\u001b[0m     example_prompt \u001b[38;5;241m=\u001b[39m example_prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     22\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: MaxMarginalRelevanceExampleSelector.from_examples() missing 1 required positional argument: 'vectorstore_cls'"
     ]
    }
   ],
   "source": [
    "# 调用MMR\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    # 传入示例组\n",
    "    examples,\n",
    "    # 使用OpenAI的嵌入来做相似性搜索\n",
    "    # OpenAIEmbeddings(),\n",
    "    # 使用tongyi的嵌入来做相似性搜索\n",
    "    # TongyiEmbeddings(),  没有这个东西\n",
    "    # 设置使用的向量数据库是什么\n",
    "    FAISS,\n",
    "    # 结果条数\n",
    "    k = 2\n",
    ")\n",
    "\n",
    "# 使用小样本的模版\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    example_selector = example_selector,\n",
    "    example_prompt = example_prompt,\n",
    "    prefix = \"给出每个输入词的反义词\",\n",
    "    suffix = \"原词:{adjective}\\n 反义:\",\n",
    "    input_variables = [\"adjective\"]\n",
    ")\n",
    "\n",
    "# 当我们输入一个描述情绪的词语的时候,应该是选择同样是描述情绪的一对示例来填充提示词模版\n",
    "print(mmr_prompt.format(adjective = \"难过\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004009537977056408, 0.028321029223305746, 0.0649349565064745, -0.017172557774905003, 0.042716248497030214, -0.01380845761854548, -0.0402388103973856, 0.01286311939631267, -0.00893181630661346, -0.012576258142669609, 0.02686064465930471, -0.02198400334737269, -0.0026322779808723973, -0.0038237301195830618, -0.0010252030030766179, -0.0012802813337308163, -0.010809453603186216, 0.007484470890505294, -0.015581781731975306, 0.012139446688258585, 0.0007297848086288644, -0.056172649122468304, 0.020471462191800193, -0.001426971747525563, -0.010431318314293092, 0.0011604841624651065, -0.008019075954112815, 0.027955933082305488, -0.00589043506060238, 0.009029609915809958, 0.009759802197810474, 0.01607726935190423, 0.0017179077348851437, -0.0016225589659185583, -0.002780598288153752, 0.013202137241527195, 0.004628897501967559, -0.001515800942545715, 0.0010439467781726134, 0.0158556038377255, 0.0216319463542653, 0.004280100295833384, 0.01192430074802629, 0.030902780506093287, 2.4486602927716816e-05, -0.02384860149605258, -0.009844556659114106, 0.017928828352691252, -0.005851317616923781, 0.009864115380953406, -0.01675530504233328, -0.08882267544620567, 0.008247261042237976, -0.004505025596985329, 0.02127988936115791, -0.014590806492117463, 0.03215453870380846, 0.012341553480598014, 0.012328514332705149, 0.017029127148083474, 0.034345115549810006, 0.03562295204331091, -0.01800706324004845, -0.022648999889908875, 0.007438833872880262, 0.012367631776383748, -0.02496996821483909, 0.004899459820744537, -0.0020536657931264526, -0.03241532166166578, 0.031293954942879273, -0.0575287205033264, -0.007380157707362362, 0.003501011209234619, -0.010105339616971434, 0.015203646443082182, -0.009577254127310345, 0.0020764843019389686, -0.029285926167377855, -0.014447375865295932, 0.001219160327983005, 0.024344089115981505, 0.003787872462877679, -0.037474511044097936, 0.025634964757375273, -0.007614862369433957, 0.0360923613674541, -0.020993028107514848, -0.018215689606334313, 0.051087381444250415, 0.008938335880559893, 0.01431698438636727, 0.056694215038182956, 0.001980320586229079, 0.0032940147364353654, 0.003996498829163541, -0.010437837888239525, 0.0011767830973311894, 0.06237928351947269, 0.024031149566552713, -0.002443210336425835, -0.04641936649860426, -0.03139826812602221, 0.025452416686875146, -0.009518577961792446, 0.0129869913012949, -0.031059250280807683, -0.0072106487847550995, 0.01843735512051304, -0.007543147056023192, 0.006568470751031431, -0.03398001940880975, 0.021032145551193447, -0.018254807050012912, 0.018515590007870238, -0.0021807974850818995, 0.010411759592453792, 0.004130150095065421, 0.019141469106727826, -0.00790824319702345, 0.04790582935839102, -0.025113398841660618, 0.013032628318919931, 0.008129908711202179, -0.014186592907438605, 0.0016681959835435907, -0.020836558332800452, -0.002133530573970259, -0.026521626814090187, 0.03901313049545616, 0.026104374081518462, 0.020862636628586183, -0.0008173915835340603, -0.024552715482267364, 0.0024937370345106923, 0.005381908292780592, -0.02120165447380071, -0.007634421091273257, -0.01171567438174043, -0.013097824058384264, 0.02902514320952053, -0.014251788646902937, -0.04117762904567198, 0.005170022139521514, -0.0015532884927377058, 0.0011824877245343185, -0.005962150374013145, -0.01104415826525781, 0.013404244033866624, -0.013267332980991527, 0.0419078213276725, 0.03181552085859393, -0.007895204049130584, 0.004697353028405108, 0.015438351105153776, -0.007204129210808667, 0.005000513216914251, 0.0006478826609267975, -0.01803314153583418, 0.01067254255031112, -0.005805680599298749, 0.06853376132490561, -0.0013585162210880144, 0.030798467322950353, 0.010522592349543157, -0.0011344058666793737, 0.009531617109685312, 0.02386164064394545, -0.02722574080030497, -0.03856979946709871, -0.02005420945922847, 0.014838550302081923, -0.004077993503493956, -0.003264676653676416, 0.024591832925945963, -0.020471462191800193, -0.037370197860955, -0.009042649063702824, -0.006268570349495505, -0.05306933192396611, 0.013058706614705665, -0.011748272251472595, 0.005864356764816648, -0.019310978029335086, -0.027955933082305488, -0.010809453603186216, 0.030407292886164363, -0.005453623606191357, 0.0008679182816189174, -0.034423350437167204, 0.006425040124209902, -0.004710392176297974, -0.02863396877273454, 0.0289208300263776, -0.0027985271165064437, 0.010353083426935894, -0.028816516843234666, -0.014447375865295932, 0.03856979946709871, -0.0027154025486894207, -0.015021098372582052, -0.03658784898738302, 0.008058193397791414, -0.004214904556369053, 0.01399100568904561, 0.025674082201053872, 0.01196341819170489, 0.0039769401073242415, 0.009870634954899838, -0.00562313252879862, 0.011591802476758199, -0.013189098093634328, 0.006421780337236685, -0.013925809949581277, 0.057372250728612004, 0.06764709926819071, -0.024983007362731957, 0.023196644101409265, -1.4210633836366078e-05, 0.02066704941019319, -0.04209036939817263, 0.01140925440625807, -0.0005032296139903113, 0.006477196715781367, 0.003862847563261661, -0.022948900291444804, -0.00521239937017333, 0.022583804150444545, 0.015516585992510974, 0.009407745204703083, -0.01907627336726349, 0.009759802197810474, -0.033823549634095354, -0.005952371013093496, -0.004130150095065421, 0.02816455944859135, -0.00013640171116053175, 0.02873828195587747, 0.009942350268310604, -0.013665026991723951, 0.012406749220062347, 0.004071473929547522, 0.002232954076653365, 0.015451390253046643, 0.01251758197715171, -0.002788747755586794, -0.039951949143742546, -0.006294648645281238, -0.008794905253738364, -0.014264827794795803, 0.011487489293615268, 0.010170535356435764, 0.031033171985021948, 0.013326009146509426, -0.00433225688740485, 0.018098337275298516, -0.014434336717403067, 0.04962699688024939, 0.01954568269140668, -0.02417458019337424, -0.0004816335252927514, 0.04044743676367146, 0.018280885345798643, 0.0065619511770849985, -0.0023633455555820285, 0.008449367834577405, -0.050305032570678435, 0.007569225351808925, -0.00576656315562015, 0.028607890476948804, -0.02719966250451924, -0.01362590954804535, 0.00649349565064745, -0.0026746552115242133, -0.014682080527367527, -0.012797923656848337, 0.0024725484191847843, -0.0028523136015645174, -0.0024529896973454848, -0.06044948963132847, -0.04255977872231582, -0.024461441447017302, 0.05445148160060994, 0.020041170311335604, 0.02622172641255426, 0.010124898338810733, 0.013782379322759749, -0.022583804150444545, 0.013912770801688411, -0.03943038322802789, -0.0019346835686040469, 0.044020163286316846, -0.0021905768460015493, 0.013482478921223822, -0.006568470751031431, -0.06316163239304468, 0.031711207675451, 0.013964927393259876, -0.007464912168665994, 0.031007093689236216, 0.019284899733549354, -0.02499604651062482, 0.060293019856614075, 0.02056273622705026, -0.0020210679233942865, -0.02062793196651459, -0.01610334764768996, 0.02343134876348086, -0.014173553759545739, 0.019206664846192156, 0.020002052867657005, 0.026339078743590056, 0.0038661073502348773, -0.05570323979832511, -0.011181069318132907, -0.033301983718380695, 0.007458392594719561, -0.02816455944859135, -0.0032157798490781674, -0.02686064465930471, -0.03875234753759884, -0.04355075396217366, -0.010209652800114363, -0.018541668303655973, 0.014734237118938993, 0.02525682946848215, -0.009088286081327857, -0.06295300602675881, 0.01489070689365339, -0.0045474028276371444, 0.014864628597867656, 0.027277897391876436, 0.02566104305316101, 0.018919803592549096, 0.00915348182079219, 0.03726588467781207, 0.011311460797061572, -0.004925538116530269, 0.003196221127238868, -0.007823488735719818, -0.02460487207383883, -0.029207691280020657, 0.006467417354861717, -0.013938849097474145, -0.02022371838183573, 0.012491503681365979, -0.04438525942731711, 0.039769401073242415, 0.046367209907032796, 0.023874679791838313, 0.03510138612759626, 0.06034517644818554, 0.010933325508168447, -0.0329890441689519, -0.012687090899758974, -0.009831517511221239, 0.029729257195735312, -0.014408258421617333, -0.048453473569891416, 0.0086645137748097, 0.017498536472226663, -0.018541668303655973, -0.002278591094278397, -0.008781866105845497, 0.015399233661475177, 0.04096900267938612, 0.03776137229774099, 0.016546678676047418, -0.02464398951751743, -0.0011555944820052816, -0.01137013696257947, 0.017889710909012653, 0.03666608387474022, -0.02719966250451924, 0.029285926167377855, -0.0046647551586729424, 0.0245005588906959, -0.002894690832216333, 0.01024225066984653, 0.008260300190130842, -0.010170535356435764, 0.0018417796398673741, 0.035257855902310654, 0.0047462498330033565, 0.010431318314293092, -0.01843735512051304, -0.01198297691354419, -0.011885183304347692, -0.011252784631543673, -0.031189641759736344, 0.0027154025486894207, 0.02902514320952053, 0.036379222621097156, -0.010757297011614752, 0.041151550749886245, 0.01085509062081125, 0.0022508829050060563, -0.01599903446454703, 0.029077299801091996, 0.005932812291254196, -0.004580000697369311, -0.01653363952815455, -0.009746763049917609, -0.022622921594123144, -0.0019982494145817705, -0.016703148450761814, 0.039299991749099226, -0.006395702041450952, 0.03275433950688031, 0.003928043302725993, 0.00905568821159569, -0.0026176089394929227, 0.013769340174866881, -0.01764196709904819, 0.002079744088912185, -0.015229724738867915, -0.03635314432531143, 0.03280649609845177, -0.015868642985618367, 0.021488515727443768, 0.023079291770373465, 0.01833304193737011, 0.00475276940694979, 0.04089076779202892, -0.01097896252579348, -0.012804443230794771, 0.020706166853871787, 0.049053274372963265, -0.011428813128097369, 0.00895137502845276, 0.031293954942879273, -0.002728441696582287, 0.01786363261322692, 0.01140925440625807, -0.046106426949175466, -0.0004828559454077076, 0.05077444189482162, 0.014251788646902937, 0.012067731374847821, -0.00833853507748804, -0.015933838725082698, -0.007810449587826953, 0.04089076779202892, 0.03491883805709613, -0.008260300190130842, 0.0060762429180757265, 0.00837765252116664, 0.0005028221406186592, 0.0115526850330796, -0.012563218994776743, -0.26328647425275764, 0.0015516585992510976, -0.03510138612759626, 0.010503033627703858, 0.018176572162655714, 0.014199632055331472, -0.016520600380261683, -0.028816516843234666, -0.0021074522781845263, 0.06316163239304468, 0.0016967191195592357, 0.017094322887547805, -0.031711207675451, 0.030302979703021433, 0.031867677450165395, 0.02855573388537734, 0.05966714075775649, -0.0013030998425433325, -0.009890193676739137, 0.002210135567840849, -0.017042166295976338, 0.03048552777352156, -0.002096043023778268, 0.02384860149605258, -1.9953461668087494e-05, 0.051139538035821885, 0.02177537698108683, -0.015373155365689445, 0.00899049247213136, -0.010196613652221498, 0.025022124806410556, -0.010496514053757424, 0.003963900959431375, 0.003813950758663412, -0.023014096030909134, 0.01399100568904561, 0.0014709788716639868, -0.0035401286529132183, -0.009290392873667286, -0.0011947119256838807, 0.03841332969238431, 0.042846639975958875, -0.021970964199479824, -0.006288129071334804, -0.006734719886665477, -0.011324499944954438, -0.03223277359116566, -0.009948869842257036, -0.03796999866402685, 0.02603917834205413, -0.020367149008657263, 0.00330868377781484, 0.0014783133923537241, -0.0172507926622622, -0.019663035022442477, -0.043237814412744865, 0.00679991562612981, -0.001268872079324558, -0.050748363599035895, -0.024735263552767495, 0.034736289986595996, 0.0025181854368098167, -0.030355136294592897, 0.03783960718509819, 0.017811476021655455, -0.0035401286529132183, 0.01915450825462069, 0.02284458710830187, -0.023222722397194997, 0.004974434921128518, 0.01865902063469177, 0.021332045952729372, -0.006715161164826178, -0.03048552777352156, 0.004778847702735523, 0.004175787112690454, 0.03424080236666707, 0.00038159881255216724, 0.009492499666006713, 0.012576258142669609, -0.05017464109174977, 0.0020699647279925353, -0.02363997512976672, 0.01882852955729903, -0.012217681575615784, -0.007862606179398417, 0.004130150095065421, -0.02654770510987592, 0.014212671203224338, -0.0019314237816308304, 0.006473936928808151, 0.010874649342650549, -0.03484060316973893, -0.055911866164610975, 0.03486668146552466, -0.050096406204392575, -0.011865624582508392, 0.06237928351947269, 0.03961293129852802, -0.023235761545087864, -0.0201585226423714, 0.027773385011805356, 0.05072228530325016, 0.0055546770023610715, 0.002741480844475153, 0.028607890476948804, 0.018424315972620176, 0.014499532456867399, 0.007458392594719561, -0.0035303492919935685, 0.01796794579636985, 0.015907760429296966, -0.015073254964153519, 0.02238821693205155, -0.0003265899073791372, 0.02429193252441004, -0.0026143491525197063, 0.03515354271916772, -0.006539132668272482, -0.011917781174079857, 0.044672120680960166, 0.009257795003935119, -0.0115722437549189, -0.025804473679982537, 0.009492499666006713, -0.011454891423883102, 0.004553922401583578, 0.032728261211094574, -0.02683456636351898, -0.012308955610865849, -0.04727995025953344, -0.0062001148230579565, 0.02751260205394803, -0.024722224404874628, 0.005398207227646675, 0.0009200748731903828, -0.0005977634362135924, 0.012504542829258844, 0.005170022139521514, -0.024722224404874628, 0.002612719259033098, -0.021527633171122367, -0.02829495092752001, 0.0022965199226310886, 0.023366153024016525, 0.010848571046864815, 0.038335094805027115, -0.017342066697512266, -0.05528598706575339, -0.00733452068973733, -0.011833026712776227, -0.0187242163741561, 0.010294407261417995, -0.019858622240835472, 0.03405825429616695, -0.00013151203070070687, -0.008560200591666768, 0.032650026323737376, 0.006346805236852703, 0.006917267957165607, -0.06086674236390019, 0.021032145551193447, 0.04508937341353189, -0.019819504797156873, 0.003690078853681181, 0.00028543509684227775, -0.007132413897397901, -0.02274027392515894, -0.03296296587316617, -0.0329890441689519, -0.014056201428509942, -0.0014579397237711204, -0.001048021511889134, 0.028686125364306002, -0.01614246509136856, -0.01137013696257947, -0.011115873578668577, -0.03332806201416643, 0.0245396763343745, -0.02470918525698176, -0.006581509898924298, -0.003015302950225347, -0.012263318593240816, 0.048662099936177275, 0.008325495929595174, 0.01796794579636985, -0.01097896252579348, 0.004452869005413864, -0.04010841891845694, 0.02970317889994958, 0.015060215816260651, -0.014434336717403067, 0.002276961200791789, -0.005427545310405625, 0.042455465539172885, 0.017055205443869206, 0.005075488317298232, 0.033171592239452034, -0.007614862369433957, -0.017159518627012135, 0.0011140321980967702, -0.022857626256194738, -0.010568229367168188, -0.010711659993989719, 0.017707162838512525, 0.012328514332705149, 0.009309951595506585, -0.0055937944460396705, -0.0016070749777957795, -0.014825511154189057, -0.0020536657931264526, 0.0074127555770945285, 0.04665407116067585, -0.004716911750244407, -0.02568712134894674, 0.0019004558053852727, 0.013834535914331213, 0.013912770801688411, -0.015620899175653905, -0.009929311120417736, -0.0013193987774094153, 0.0003408514753869598, -0.04091684608781465, 0.032910809281594705, 0.02693887954666191, -0.0068846700874334405, 0.05883263529261304, 0.037813528889312456, -0.026964957842447644, 0.021032145551193447, -0.010789894881346917, 0.021136458734336377, -0.012061211800901387, -0.006500015224593883, 0.008058193397791414, -0.02054969707915739, 0.006102321213861459, -0.006268570349495505, 0.03254571314059445, 0.013977966541152744, -0.008631915905077534, -0.04010841891845694, -0.009225197134202954, -0.007966919362541349, 0.053616976135466496, -0.013782379322759749, 0.00023164861178420401, 0.007530107908130326, -0.05093091166953602, 0.01989773968451407, 0.031033171985021948, -0.02288370455198047, -0.013267332980991527, 0.0023225982184168215, 0.007399716429201662, 0.03510138612759626, 0.03387570622566682, 0.019989013719764137, -0.022336060340480083, -0.013456400625438089, 0.01886764700097763, 0.019350095473013685, -0.03074631073137889, -0.010939845082114881, 0.04417663306103124, -0.02169714209372963, -0.008175545728827211, -0.04057782824260013, -0.031867677450165395, -0.023379192171909393, 0.02087567577647905, -0.0029077299801091995, -0.01682050078179761, 0.035988048184311165, -0.029885726970449708, -0.03627490943795423, -0.02787769819494829, 0.024435363151231567, -0.012152485836151453, -0.02425281508073144, 0.004172527325717237, 0.007138933471344335, 0.017850593465334054, -0.02456575463016023, 0.019245782289870755, 0.022231747157337153, 0.015347077069903712, 0.022323021192587215, 0.007132413897397901, -0.02275331307305181, 0.0035857656705382502, -0.021253811065372174, 0.029207691280020657, 0.030694154139807424, -0.029181612984234925, -0.013612870400152485, -0.01323473511125936, 0.019597839282978147, -0.028086324561234152, 0.00936210818707805, 0.0007741994061389404, 0.005932812291254196, -0.014642963083688927, -0.0010602457130386963, -0.0029908545479262224, 0.031711207675451, -0.023183604953516398, -0.028816516843234666, 0.010972442951847046, -0.045141530005103354, -0.01972823076190681, -0.03864803435445591, -0.004935317477449919, 0.014369140977938734, 0.00208463376937201, 0.0158164863940469, -0.007132413897397901, 0.003135915068234361, 0.004250762213074435, 0.01683353992969048, -0.02363997512976672, -0.02127988936115791, 0.02675633147616178, -0.06702122016933312, 0.04216860428552983, 0.006356584597772353, -0.028999064913734798, -0.02798201137809122, 0.0034651535525292367, 0.031189641759736344, 0.01948048695194235, 0.016976970556512008, 0.03575334352223957, 0.020536657931264524, 0.016455404640797353, -0.19475271292785204, 0.000534605063607521, 0.007608342795487524, 0.024435363151231567, -0.03987371425638535, 0.013482478921223822, -0.008547161443773902, 0.015320998774117979, 0.020575775374943123, -0.0072106487847550995, 0.023757327460802516, 0.018632942338906035, -0.01657275697183315, 0.022362138636265814, 0.03236316507009432, 0.03267610461952311, 0.0042605415739940845, -0.01210684881852642, 0.007080257305826437, 0.006180556101218657, 0.04524584318824629, 0.016377169753440154, 0.015073254964153519, -0.04443741601888857, -0.013964927393259876, 0.01886764700097763, -0.0022248046092203234, 0.03074631073137889, -0.002469288632211568, 0.034553741916095865, -0.012889197692098403, -0.011057197413150678, 0.011578763328865331, -0.017876671761119785, 0.03437119384559574, 0.030537684365093028, -0.03463197680345306, 0.01085509062081125, 0.025191633729017816, 0.015738251506689702, 0.007588784073648224, -0.048270925499391285, -0.05721578095389761, -0.0217101812416225, 0.029390239350520788, -0.04078645460888599, -0.017889710909012653, 0.010033624303560668, -0.0010357973107395719, -0.020862636628586183, -0.011181069318132907, 0.019962935423978406, 0.0003092722890839241, 0.005616612954852187, -0.026260843856232858, 0.02873828195587747, 0.013443361477545223, -0.028686125364306002, 0.04975738835917805, 0.00016635100397695916, -0.015725212358796835, -0.0216319463542653, 0.0033331321801139645, -0.0018401497463807657, 0.01869813807837037, 0.08272035423234422, -0.019858622240835472, -0.015412272809368044, 0.01790275005690552, -0.005492741049869956, -0.03836117310081285, -0.015294920478332247, 0.021814494424765428, -0.027017114434019107, 0.007106335601612169, 0.0005961335427269841, -0.019219703994085024, 0.016311974013975824, -0.06655181084518992, 0.02855573388537734, 0.025947904306804066, 0.03551863886016798, -0.0388045041291703, 0.008755787810059765, -0.007341040263683763, -0.026456431074625857, 0.007575744925755358, -0.004576740910396095, -0.03611843966323983, 0.0008842172164850004, 0.000347574786019219, 0.01208729009668712, 0.011885183304347692, 0.0049842142820481685, 0.0025393740521357246, 0.008058193397791414, -0.007015061566362104, 0.02382252320026685, 0.008814463975577664, 0.029259847871592123, 0.010985482099739912, -0.014238749499010071, 0.045715252512389476, 0.033562766676238025, -0.030615919252450226, -0.03799607695981259, -0.002596420324167015, -0.009681567310453276, 0.04647152309017573, -0.004632157288940776, -0.04694093241431892, 0.03390178452145255, 0.02241429522783728, 0.013612870400152485, 0.05082659848639309, 0.006734719886665477, 0.02500908565851769, -0.02499604651062482, 0.009811958789381939, 0.006164257166352574, -0.018854607853084765, 0.02386164064394545, -0.017850593465334054, 0.0026094594720598814, -0.014212671203224338, 0.000854064186982747, 0.024696146109088896, -0.018020102387941318, -0.005724185924968334, -0.023809484052373982, 0.011826507138829793, 0.006659744786281496, 0.04000410573531401, 0.057372250728612004, -0.006438079272102768, 0.004889680459824887, 0.005929552504280979, 0.042716248497030214, -0.009864115380953406, -0.007719175552576888, -0.014968941781010587, -0.009851076233060538, -0.03692686683259755, 0.0201585226423714, -0.0016331532735815123, -0.0068455526437548415, -0.02873828195587747, -0.03476236828238173, -0.005932812291254196, -0.024018110418659845, -0.04117762904567198, 0.01683353992969048, 0.0475668115131765, 0.042846639975958875, -0.0187242163741561, 0.030433371181950095, -0.009512058387846013, 0.015803447246154033, -0.03288473098580897, -0.014499532456867399, -0.03139826812602221, -0.0036053243923775498, -0.05732009413704054, -0.010698620846096853, 0.026260843856232858, -0.029207691280020657, 0.011187588892079341, 0.029390239350520788, -0.019532643543513816, -0.035127464423381986, 0.013873653358009812, -0.02199704249526556, 0.002772448820720711, -0.025947904306804066, -0.05434716841746701, -0.014942863485224854, 0.006728200312719044, 0.01624677827451149, -0.021801455276872564, -0.0007697171990507676, -0.032337086774308584, -0.045063295117746156, -0.036379222621097156, 0.003908484580886693, -0.009903232824632005, -0.008599318035345367, -0.012804443230794771, -0.050487580641178566, -0.005681808694316518, -0.03236316507009432, -0.006415260763290252, 0.03622275284638276, -6.183408414820222e-05, 0.024044188714445577, -0.046627992864890125, 0.03254571314059445, 0.010098820043025, -0.01622069997872576, 0.0017553952850771345, 0.00706721815793357, -0.037370197860955, -0.014590806492117463, -0.006767317756397643, 0.03390178452145255, -0.03006827504094984, 0.02077136259333612, 0.057998129827469595, 0.009231716708149387, 0.0066010686207635975, -0.006542392455245699, -0.006663004573254713, -0.010913766786329148, 0.02169714209372963, -0.019571760987192415, -0.017694123690619658, -0.026704174884590315, 0.03426688066245281, 0.003993239042190324, 0.02779946330759109, 0.042325074060244224, 0.012035133505115656, 0.008983972898184926, -0.030433371181950095, -0.018280885345798643, -0.010418279166400226, -0.028712203660091737, -0.01079641445529335, 0.04109939415831478, 0.032258851886951385, 0.011396215258365204, 0.003290754949462149, -0.05439932500903848, 0.001041501937942701, 0.0010455766716592216, 0.0009860855593980188, 0.0302769014072357, 0.005574235724200371, -0.023548701094516657, 0.01682050078179761, -0.0028246054122921765, -0.038517642875527246, -0.0315547379007366, -0.0013845945168737472, 0.018320002789477242, -0.0045017658100121125, -0.0345015853245244, 0.010842051472918383, 0.05893694847575597, 0.006930307105058473, -0.013104343632330697, -0.0010838791685945167, -0.05280854896610878, -0.002558932773975024, -0.01811137642319138, 0.008918777158720593, -0.05372128931860942, -0.0360141264800969, -0.024383206559660104, -0.03864803435445591, 0.012921795561830568, -0.03815254673452698, -0.015216685590975047, -0.01790275005690552, 0.018176572162655714, -0.04986170154232098, -0.027277897391876436, -0.029624944012592382, 0.036040204775882635, -0.007243246654487266, 0.010418279166400226, 0.04738426344267637, -0.01624677827451149, 0.03692686683259755, -0.04070821972152879, 0.03726588467781207, 0.0009934200800877562, -0.012941354283669868, -0.03241532166166578, 0.004110591373226121, -0.04594995717446107, 0.0272518190960907, 0.013404244033866624, -0.005658990185504003, 0.012328514332705149, 0.026808488067733248, 0.002950107210761015, -0.03152865960495087, -0.004387673265949532, 0.016520600380261683, -0.001042316884686005, -0.012706649621598273, -0.02769515012444816, 0.022622921594123144, 0.015334037922010846, -0.0008426549325764889, -0.012889197692098403, 0.037422354452526466, -0.035988048184311165, -0.022727234777266073, -0.014486493308974531, -0.03202414722487979, 0.009186079690524354, 0.0009217047666769912, -0.0067607981824512105, 0.028347107519091478, 0.0018727476161129316, 0.03293688757738044, -0.0009412634885162907, 0.027277897391876436, -0.0009135552992439497, 0.014082279724295675, -0.01735510584540513, -0.03544040397281078, 0.026286922152018593, 0.04665407116067585, 0.01542531195726091, 0.0008923666839180419, -0.06117968191332899, 0.012295916462972982, 0.02062793196651459, 0.027773385011805356, 0.03867411265024164, -0.013143461076009296, 0.0260131000462684, -0.030146509928307037, 0.031085328576593414, -0.013345567868348725, -0.035179621014953456, 0.020288914121300065, -0.01941529121247802, 0.012054692226954955, 0.002746370524934978, -0.014434336717403067, -0.0005989858563285487, 0.013293411276777259, -0.0316329727880938, -0.008325495929595174, 0.0032940147364353654, -0.005496000836843173, 0.02388771893973118, 0.043707223736888054, 0.03606628307166836, -0.027564758645519497, 0.0014155624931193047, -0.04083861120045745, 0.007132413897397901, 0.012511062403205278, 0.006059943983209643, 0.018346081085262977, 0.06670828061990432, 0.007106335601612169, -0.04767112469631943, 0.009042649063702824, 0.022766352220944672, 0.019310978029335086, 0.04404624158210258, 0.021944885903694093, -0.023705170869231053, 0.01614246509136856, 0.02978141378730678, -0.002855573388537734, 0.003157103683560269, -0.007132413897397901, 0.0001791864151839995, 0.0041790468996636704, -0.008371132947220207, -0.014160514611652873, 0.005740484859834417, 0.015660016619332504, -0.003386918665172038, 0.005597054233012887, -0.021292928509050773, -0.005737225072861201, -0.019219703994085024, -0.021840572720551163, -0.005231958092012629, -0.005642691250637919, -0.019702152466121076, 0.08157290921777198, 0.02826887263173428, 0.012341553480598014, 0.027434367166590833, -0.015320998774117979, -0.05273031407875158, 0.024657028665410297, 0.013586792104366752, 0.004348555822270932, 0.021879690164229762, 0.028868673434806133, -0.029103378096877727, 0.01972823076190681, 0.015229724738867915, 0.0172507926622622, 0.01743334073276233, -0.007875645327291285, 0.00274474063144837, -0.03502315124023906, 0.0274865237581623, 0.0011784129908177978, 0.0008997012046077792, -0.02281850881251614, 0.05664205844661149, 0.015216685590975047, 0.026638979145125984, -0.02673025318037605, 0.00028197157318323514, 0.0028865413647832915, -0.011969937765651324, 0.03901313049545616, -0.006917267957165607, -0.0068455526437548415, 0.00533627127515556, -0.021097341290657778, -0.007158492193183635, 0.017237753514369333, -0.023444387911373724, 0.015633938323546773, 0.02352262279873092, 0.018489511712084506, 0.013280372128884393, -0.04581956569553241, -0.01618158253504716, 0.05570323979832511, -0.008371132947220207, -0.009394706056810215, 0.006852072217701275, 0.05836322596846985, 0.04021273210159987, -0.033693158155166686, 0.03679647535366888, -0.01667707015497608, -0.011435332702043803, 0.00764094066521969, -0.030902780506093287, -0.018958921036227695, 0.01954568269140668, -0.015829525541939768, 0.12235936382665798, -0.008273339338023709, -0.016729226746547546, 0.012563218994776743, 0.031737285971236734, 0.005662249972477219, 0.036822553649454616, -0.012478464533473111, 0.03549256056438225, 0.010137937486703599, 0.0005826869214624657, -0.0158947212814041, -0.031007093689236216, -0.02650858766619732, 0.01499502007679632, -0.013365126590188025, 0.004286619869779818, -0.013260813407045094, 0.01270013004765184, -0.02069312770597892, -0.0010105339616971432, -0.03716157149466914, -0.02759083694130523, -0.017628927951155327, -0.018515590007870238, -0.02952063082944945, -0.012654493030026807, 0.018567746599441704, 0.02533506435583935, -0.015438351105153776, -0.022179590565765687, -0.010685581698203985, -0.040421358467885735, 0.007843047457559118, 0.015320998774117979, -0.047488576625819304, 0.0043811536920030985, -0.020210679233942867, -0.01307174576259853, 0.03403217600038121, -0.014147475463760006, 0.019232743141977888, 0.0015598080666841389, -0.004808185785494472, -0.01370414443540255, -0.016990009704404875, 0.01958480013508528, 0.016481482936583084, 0.009107844803167156, 0.0004327367206945025, 0.05100914655689322, 0.0007819414002003299, 0.005665509759450436, 0.003660740770922232, -0.01933705632512082, -0.0008891068969448252, -0.00993583069436417, -0.045063295117746156, -0.07755685166676914, 0.014629923935796062, -0.011859105008561958, -0.02418761934126711, -0.002138420254430084, -0.011689596085954696, -0.015646977471439637, -0.004785367276681956, -0.019636956726656746, 0.0008251335775954497, 0.021332045952729372, -0.018711177226263233, 0.006747759034558344, 0.025582808165803807, -0.0002789155228958446, -0.0462107401323184, -0.012713169195544705, 0.019493526099835217, -0.002971295826086923, 0.02779946330759109, 0.0038465486283955777, 0.021162537030122112, 0.030302979703021433, -0.03257179143638018, 0.004247502426101219, 0.007999517232273516, -0.04394192839895965, -0.04125586393302918, -0.01700304885229774, 0.003650961410002582, -0.04083861120045745, -0.01693785311283341, 0.012256799019294383, -0.01452561075265313, 0.006982463696629938, 0.003781352888931246, -0.009642449866774677, 0.015164528999403582, 0.022609882446230276, -0.025126437989553486, -0.013228215537312928, 0.011174549744186475, -0.021162537030122112, -0.07129806067819329, -0.01696393140861914, 0.016272856570297225, 0.0006108025841064588, -0.026000060898375532, -0.04274232679281594, -0.0003683559279734748, -0.016377169753440154, -0.007966919362541349, -0.024200658489159973, -0.017368144993297998, -0.006910748383219174, 0.0008015001220396294, -0.01212640754036572, 0.02863396877273454, 0.01790275005690552, -0.01972823076190681, 0.02392683638340978, -0.03429295895823854, -0.045976035470246805, -0.013097824058384264, 0.005968669947959578, 0.05241737452932279, 0.029859648674663977, 0.0274865237581623, -0.010137937486703599, 0.0074127555770945285, -0.043368205891673534, 0.026586822553554518, -0.02654770510987592, 0.00036611482442938843, -0.01882852955729903, -0.045402312962960684, -0.018502550859977374, 0.0007090036666746086, 0.02360085768608812, -0.0011955268724271849, 0.007719175552576888, 0.0033380218605737894, 0.02087567577647905, 0.031163563463950612, 0.003219039636051384, 0.024943889919053358, -0.042455465539172885, -0.003742235445252647, 0.021136458734336377, 0.001279466386987512, -0.011904742026186991, -0.003693338640654398, 0.024005071270766978, -0.0034488546176631536, -0.0003469635759617409, 0.03358884497202376, -0.026169569820982796, 0.01499502007679632, 0.0012998400555701158, -0.008279858911970141, 0.03293688757738044, -0.016390208901333022, -0.016703148450761814, 0.008136428285148612, -0.08684072496649, 0.023327035580337926, 0.005209139583200113, 0.03969116618588522, 0.0231053700661592, -0.014916785189439122, -0.013691105287509683, 0.0037780931019580294, 0.0046614953716997256, 0.01919362569829929, -0.008221182746452243, 0.00839721124300594, 0.009694606458346142, 0.007966919362541349, 0.01775931943008399, -0.02013244434658567, -0.04446349431467431, 0.020041170311335604, -0.01309130448443783, 0.02800808967387695, -0.012348073054544448, -0.008762307384006197, -0.02281850881251614, -0.008221182746452243, -0.018489511712084506, 0.03280649609845177, -0.020471462191800193, -0.03984763596059961, -0.02030195326919293, -0.017302949253833667, 0.00882750312347053, 0.010555190219275323, -0.030042196745164104, -0.05392991568489529, -0.025765356236303938, 0.002728441696582287, 0.022701156481480342, -0.04834916038674848, 0.02073224514965752, 0.005942591652173846, 0.0013096194164897656, 0.04172527325717237, 0.01722471436647647, -0.0020585554735862775, 0.02249253011519448, -0.014212671203224338, 0.006568470751031431, 0.025739277940518206, -0.05502520410789606, -0.00460933878012826, 0.013912770801688411, -0.013717183583295416, 0.011057197413150678, 0.033093357352094836, 0.008671033348756133, -0.0115526850330796, 0.011689596085954696, 0.005939331865200629, -0.005065708956378583, 0.013000030449187766, -0.014434336717403067, -0.0031163563463950613, -0.001329178138329065, 0.021801455276872564, 0.007536627482076759, 0.008182065302773643, -0.0003300534310381799, 0.015972956168761297, 0.0033331321801139645, 0.019819504797156873, 0.02030195326919293, 0.0017553952850771345, -0.00252470501075625, 0.040942924383600386, -0.019310978029335086, 0.019363134620906552, -0.028972986617949063, -0.0033543207954398724, -0.002149829508836342, -0.05726793754546908, -0.032311008478522855, 0.044072319877888316, -0.014290906090581536, -0.025413299243196547, -0.007419275151040961, -0.030224744815664235, -0.01169611565990113, 0.03249355654902298, 0.01804618068372705, -0.011565724180972466, -0.011057197413150678, 0.0018434095333539824, -0.009655489014667543, 0.0017521354981039179, 0.002909359873595808, 0.00433225688740485, -0.007406236003148096, 0.0331455139436663, 0.025491534130553745, 0.02550457327844661, 0.02460487207383883, 0.04422878965260271, -0.051400320993679215, -0.033171592239452034, 0.021136458734336377, -0.00895137502845276, -0.008332015503541608, -0.0009135552992439497, 0.030042196745164104, 0.01399100568904561, 0.007373638133415929, 0.01176131139936546, -0.006962904974790639, 0.02066704941019319, 0.0006682563295094012, 0.03152865960495087, 0.018932842740441963, 0.016025112760332763, -0.028086324561234152, -0.022935861143551936, -0.018593824895227436, 0.01811137642319138, -0.00690422880927274, -0.001422897013809042, -0.01954568269140668, 0.0014913525402465906, -0.0030772389027164623, 0.0052286983050394125, 0.032337086774308584, 0.007093296453719302, 0.026313000447804324, -0.0017390963502110514, -0.008997012046077791, 0.024487519742803034, 0.02066704941019319, -0.022596843298337412, 0.02077136259333612, 0.023001056883016267, -0.009974948138042769, -0.005456883393164574, -0.020406266452335863, 0.03828293821345565, 0.00780393001388052, 0.028086324561234152, -0.014017083984831343, -0.01509933325993925, -0.01456472819633173, 0.05883263529261304, -9.117216690715154e-05, 0.006363104171718786, 0.012693610473705406, 0.019232743141977888]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "FAISS.__init__() missing 2 required positional arguments: 'docstore' and 'index_to_docstore_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m index\u001b[38;5;241m.\u001b[39madd(embedding_matrix)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 使用 FAISS 作为向量存储器\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 构造提示词模版\u001b[39;00m\n\u001b[0;32m     57\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     58\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     59\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m原词: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m反义: \u001b[39m\u001b[38;5;132;01m{output}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     60\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: FAISS.__init__() missing 2 required positional arguments: 'docstore' and 'index_to_docstore_id'"
     ]
    }
   ],
   "source": [
    "# 修改一下\n",
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "import dashscope\n",
    "from dashscope import TextEmbedding\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv(find_dotenv())\n",
    "DASHSCOPE_API_KEY = os.environ[\"DASHSCOPE_API_KEY\"]\n",
    "\n",
    "dashscope.api_key = DASHSCOPE_API_KEY\n",
    "\n",
    "# 调用DashScope通用文本向量模型，将文本embedding为向量\n",
    "def generate_embeddings(texts: Union[List[str], str], text_type: str = 'document'):\n",
    "    rsp = TextEmbedding.call(\n",
    "        model=TextEmbedding.Models.text_embedding_v2,\n",
    "        input=texts,\n",
    "        text_type=text_type\n",
    "    )\n",
    "    embeddings = [record['embedding'] for record in rsp.output['embeddings']]\n",
    "    return embeddings if isinstance(texts, list) else embeddings[0]\n",
    "\n",
    "# 示例获取嵌入\n",
    "text = \"这是一个示例文本\"\n",
    "embedding = generate_embeddings(text)\n",
    "print(embedding)\n",
    "\n",
    "# 示例数据\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tail\", \"output\": \"short\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "    {\"input\": \"高兴\", \"output\": \"伤心\"}\n",
    "]\n",
    "\n",
    "# 获取所有示例的嵌入向量\n",
    "embeddings = [generate_embeddings(ex['input']) for ex in examples]\n",
    "\n",
    "# 初始化FAISS索引\n",
    "dimension = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# 添加嵌入向量到索引\n",
    "embedding_matrix = np.array(embeddings).astype('float32')\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# 使用 FAISS 作为向量存储器\n",
    "vectorstore = FAISS(embedding_matrix, index)\n",
    "\n",
    "# 构造提示词模版\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词: {input}\\n反义: {output}\",\n",
    ")\n",
    "\n",
    "# 调用MMR\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    examples=examples,\n",
    "    embedding_function=lambda x: generate_embeddings(x),\n",
    "    vectorstore=vectorstore,\n",
    "    k=2\n",
    ")\n",
    "\n",
    "# 使用小样本的模板\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=prompt_template,\n",
    "    prefix=\"给出每个输入词的反义词\",\n",
    "    suffix=\"原词:{adjective}\\n反义:\",\n",
    "    input_variables=[\"adjective\"]\n",
    ")\n",
    "\n",
    "# 输入一个描述情绪的词语时，选择相关示例\n",
    "print(mmr_prompt.format(adjective=\"难过\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MaxMarginalRelevanceExampleSelector.from_examples() missing 1 required positional argument: 'vectorstore_cls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 64\u001b[0m\n\u001b[0;32m     58\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     59\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     60\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m原词: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m反义: \u001b[39m\u001b[38;5;132;01m{output}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 调用MMR示例选择器\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m example_selector \u001b[38;5;241m=\u001b[39m \u001b[43mMaxMarginalRelevanceExampleSelector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m     69\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 使用小样本的模板\u001b[39;00m\n\u001b[0;32m     72\u001b[0m mmr_prompt \u001b[38;5;241m=\u001b[39m FewShotPromptTemplate(\n\u001b[0;32m     73\u001b[0m     example_selector\u001b[38;5;241m=\u001b[39mexample_selector,\n\u001b[0;32m     74\u001b[0m     example_prompt\u001b[38;5;241m=\u001b[39mprompt_template,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     78\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: MaxMarginalRelevanceExampleSelector.from_examples() missing 1 required positional argument: 'vectorstore_cls'"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "import dashscope\n",
    "from dashscope import TextEmbedding\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import numpy as np\n",
    "import faiss  # 需要导入FAISS库\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv(find_dotenv())\n",
    "DASHSCOPE_API_KEY = os.environ[\"DASHSCOPE_API_KEY\"]\n",
    "\n",
    "dashscope.api_key = DASHSCOPE_API_KEY\n",
    "\n",
    "# 调用DashScope通用文本向量模型，将文本embedding为向量\n",
    "def generate_embeddings(texts):\n",
    "    rsp = TextEmbedding.call(\n",
    "        model=TextEmbedding.Models.text_embedding_v2,\n",
    "        input=texts,\n",
    "        text_type='document'\n",
    "    )\n",
    "    embeddings = [record['embedding'] for record in rsp.output['embeddings']]\n",
    "    return embeddings if isinstance(texts, list) else embeddings[0]\n",
    "\n",
    "# 示例数据\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "    {\"input\": \"高兴\", \"output\": \"伤心\"}\n",
    "]\n",
    "\n",
    "# 获取所有示例的嵌入向量\n",
    "embeddings = [generate_embeddings(ex['input']) for ex in examples]\n",
    "\n",
    "# 初始化FAISS索引\n",
    "dimension = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# 添加嵌入向量到索引\n",
    "embedding_matrix = np.array(embeddings).astype('float32')\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# 创建文档存储器和索引到文档存储器的映射\n",
    "documents = [Document(page_content=ex['input'], metadata={\"output\": ex['output']}) for ex in examples]\n",
    "docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})\n",
    "index_to_docstore_id = [str(i) for i in range(len(examples))]\n",
    "\n",
    "# 使用 FAISS 作为向量存储器\n",
    "vectorstore = FAISS(embedding_matrix, index, docstore, index_to_docstore_id)\n",
    "\n",
    "# 构造提示词模版\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词: {input}\\n反义: {output}\",\n",
    ")\n",
    "\n",
    "# 调用MMR示例选择器\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    examples=examples,\n",
    "    embeddings=embedding_matrix,\n",
    "    vectorstore=vectorstore,\n",
    "    k=2\n",
    ")\n",
    "\n",
    "# 使用小样本的模板\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=prompt_template,\n",
    "    prefix=\"给出每个输入词的反义词\",\n",
    "    suffix=\"原词:{adjective}\\n反义:\",\n",
    "    input_variables=[\"adjective\"]\n",
    ")\n",
    "\n",
    "# 输入一个描述情绪的词语时，选择相关示例\n",
    "print(mmr_prompt.format(adjective=\"难过\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dashscope.models' has no attribute 'TextEmbedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 25\u001b[0m\n\u001b[0;32m     16\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     17\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhappy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msad\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     18\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshort\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalm\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     22\u001b[0m ]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 使用DashScope的TextEmbedding模型\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m embedding_function \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextEmbedding\u001b[49m(\n\u001b[0;32m     26\u001b[0m     access_key_id\u001b[38;5;241m=\u001b[39mACCESS_KEY_ID,\n\u001b[0;32m     27\u001b[0m     access_key_secret\u001b[38;5;241m=\u001b[39mACCESS_KEY_SECRET\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 生成示例文本的嵌入\u001b[39;00m\n\u001b[0;32m     31\u001b[0m example_embeddings \u001b[38;5;241m=\u001b[39m [embedding_function\u001b[38;5;241m.\u001b[39membed_query(ex[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m examples]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'dashscope.models' has no attribute 'TextEmbedding'"
     ]
    }
   ],
   "source": [
    "from dashscope import models\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# DashScope访问密钥ID和密钥（请替换为您自己的）\n",
    "ACCESS_KEY_ID = os.getenv(\"DASHBOARD_ACCESS_KEY_ID\")\n",
    "ACCESS_KEY_SECRET = os.getenv(\"DASHBOARD_ACCESS_KEY_SECRET\")\n",
    "\n",
    "# 示例数据保持不变\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"}\n",
    "]\n",
    "\n",
    "# 使用DashScope的TextEmbedding模型\n",
    "embedding_function = models.TextEmbedding(\n",
    "    access_key_id=ACCESS_KEY_ID,\n",
    "    access_key_secret=ACCESS_KEY_SECRET\n",
    ")\n",
    "\n",
    "# 生成示例文本的嵌入\n",
    "example_embeddings = [embedding_function.embed_query(ex['input']) for ex in examples]\n",
    "\n",
    "# 注意：DashScope不直接提供向量数据库服务，因此需要自行管理向量和检索逻辑\n",
    "# 这里简单使用NumPy数组存储示例嵌入，实际应用中可能需要更高效的向量数据库解决方案\n",
    "faiss_embeddings = np.array(example_embeddings).astype('float32')\n",
    "\n",
    "# FAISS索引初始化（仅为示例，实际部署可能需要更复杂设置）\n",
    "dimension = faiss_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(faiss_embeddings)\n",
    "\n",
    "# 构造PromptTemplate保持不变\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词: {input}\\n反义: {output}\",\n",
    ")\n",
    "\n",
    "# 由于DashScope不直接支持SemanticSimilarityExampleSelector，\n",
    "# 我们需要自定义逻辑来根据输入找到最相似的示例。这里简化处理，直接展示如何使用FAISS检索\n",
    "def find_similar_examples(input_text, embeddings, index, top_k=1):\n",
    "    query_embedding = embedding_function.embed_query(input_text)\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), top_k)\n",
    "    return [(emb, ex) for emb, ex in zip(embeddings[I[0]], examples[I[0]])]\n",
    "\n",
    "# 小样本模板的创建需要调整，因为我们不再直接使用SemanticSimilarityExampleSelector\n",
    "def create_few_shot_prompt(adjective, examples):\n",
    "    selected_examples = find_similar_examples(adjective, faiss_embeddings, index)\n",
    "    example_str = \"\\n\".join([example_prompt.format(input=ex[1][\"input\"], output=ex[1][\"output\"]) for ex in selected_examples])\n",
    "    prompt = f\"给出每个输入词的反义词\\n{example_str}\\n原词:{adjective}\\n反义:\"\n",
    "    return prompt\n",
    "\n",
    "# 示例使用\n",
    "print(create_few_shot_prompt(\"worried\", examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install chromadb==0.4.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据输入相似度选择示例(最大余弦相似度)\n",
    "- 一种常见的相似度计算方法\n",
    "- 通过计算两个向量 之间的余弦值,来衡量它的相似度\n",
    "- 余弦值越接近1,则表示两个向量越相似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from langchian.prompts import SemanticSimilaritySearchResultWriter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "import os\n",
    "\n",
    "# 这里引入API的key\n",
    "# 忽略\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词: {input}\\n反义: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # 传入示例组.\n",
    "    examples,\n",
    "    # 使用openAI嵌入来做相似性搜索\n",
    "    OpenAIEmbeddings(openai_api_key=api_key,openai_api_base=api_base),\n",
    "    # 使用Chroma向量数据库来实现对相似结果的过程存储\n",
    "    Chroma,\n",
    "    # 结果条数\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "#使用小样本提示词模板\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # 传入选择器和模板以及前缀后缀和输入变量\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"给出每个输入词的反义词\",\n",
    "    suffix=\"原词: {adjective}\\n反义:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# 输入一个形容感觉的词语，应该查找近似的 happy/sad 示例\n",
    "print(similar_prompt.format(adjective=\"worried\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain核心组件: LLM vs chatModel\n",
    "LLM: 就是直接使用语言模型,使用前,我们需要申请API key,然后安装langchain,然后使用LLM\n",
    "chatModel也一样,但是导入的时候,我们是import ChatOpenAI\n",
    "\n",
    "chatModel:是一组对话,封装角色给模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM调用这里以OpenAI\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "# 导入API\n",
    "\n",
    "# 设置LLM\n",
    "LLM = OpenAI(\n",
    "    model = \"gpt-3\",\n",
    "    temperature = 0,\n",
    "    # 两个key\n",
    ")\n",
    "\n",
    "llm.predict(\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！有什么我可以帮助你的吗？'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转译成通义千问\n",
    "from langchain_community.llms import Tongyi\n",
    "import os\n",
    "\n",
    "llm = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    DASHSCOPE_API_KEY=\"\"\n",
    ")\n",
    "\n",
    "llm.predict(\"你好\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用chatModel 以OpenAI为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我能为你效劳的吗？\n"
     ]
    }
   ],
   "source": [
    "# 转译成通义千问\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "tongyi_chat = ChatTongyi(\n",
    "    model=\"qwen-max\",\n",
    "    temperature=0,\n",
    "    DASHSCOPE_API_KEY=\"\"\n",
    ")\n",
    "\n",
    "print(tongyi_chat.predict(\"你好\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='当然，你叫冰糖红茶。' response_metadata={'model_name': 'qwen-max', 'finish_reason': 'stop', 'request_id': 'ba83b48d-46c4-9f45-ac0d-c6e37e82b57e', 'token_usage': {'input_tokens': 35, 'output_tokens': 8, 'total_tokens': 43}} id='run-5da594a2-5a5c-4921-b6a2-3d5a2f1784b6-0'\n"
     ]
    }
   ],
   "source": [
    "# 转译成通义千问\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.schema.messages import HumanMessage,AIMessage\n",
    "\n",
    "import os\n",
    "\n",
    "tongyi_chat = ChatTongyi(\n",
    "    model=\"qwen-max\",\n",
    "    temperature=0,\n",
    "    DASHSCOPE_API_KEY=\"\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    AIMessage(role = \"System\", content= \"你好,我是SouthAki\"),\n",
    "    HumanMessage(role = \"User\",content=\"你好SouthAki,我是冰糖红茶\"),\n",
    "    AIMessage(role = \"System\", content=\"认识你很高兴\"),\n",
    "    HumanMessage(role = \"User\",content=\"你知道我叫什么吗\")\n",
    "]\n",
    "\n",
    "response = tongyi_chat.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果,你想要很好的一种体验输出的话,我们需要启用流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "夏日炎炎，烈日如炬，\n",
      "绿荷承露，红莲出淤。\n",
      "金风微动，柳絮轻舞，\n",
      "蝉鸣声中，夏意浓如许。\n",
      "\n",
      "稻穗摇曳，满眼翠色，\n",
      "蜻蜓点水，燕子穿梭疾。\n",
      "儿童戏水，笑声溢四野，\n",
      "清泉石上洗悠扬的曲。\n",
      "\n",
      "晚霞映天，火烧云彩斑斓，\n",
      "萤火点点，夜幕下的繁星点点。\n",
      "冰镇瓜果，甜入心扉，\n",
      "夏夜凉风，带走白日的疲惫。\n",
      "\n",
      "这就是夏，热烈而奔放，\n",
      "如同青春，充满活力与希望。\n",
      "每一寸土地，每一片天空，\n",
      "都在歌唱，属于夏天的歌。"
     ]
    }
   ],
   "source": [
    "# 导入模块\n",
    "from langchain_community.llms import Tongyi\n",
    "import os\n",
    "\n",
    "# 构造一个llm\n",
    "llm = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key=\"\",\n",
    "    max_tokens = 512\n",
    ")\n",
    "\n",
    "for chunk in llm.stream(\"请生成一首夏天的诗词\"):\n",
    "    print(chunk, end=\"\", flush=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到,就是生成出来的文本,是像语言模型那种方式输出出来\n",
    "这里的话是构造一个chunk的一个循环,调用langchain自带的一个方法stream方法,然后把chunk传进去\n",
    "flush的作用设置为false是否刷新,默认是true,所以这里设置为false,禁止刷新缓冲区\n",
    "\n",
    "但是llm的方式跟我们日常的使用的不太相符,我们日常使用一般使用的是chatModel模式,也就是对话的形式\n",
    "那这里的话,演示一下chatModel的流式输出的方法\n",
    "\n",
    "这里使用的是另外一家语言模型的公司的产品\n",
    "这是它的官网[点击访问](https://www.anthropic.com/)\n",
    "号称是安全的语言模型,最有能力跟OpenAI竞争的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='夏\n",
      "content='风\n",
      "content='轻\n",
      "content='抚绿柳丝，\n",
      "content='桃花笑映日偏西。\n",
      "蜻\n",
      "content='蜓点水涟漪起，儿童\n",
      "content='欢声溢满溪。\n",
      "\n",
      "竹影\n",
      "content='摇曳纳凉处，瓜果\n",
      "content='飘香醉心底。\n",
      "晚霞如\n",
      "content='织天边绮，蝉鸣声\n",
      "content='中共星稀。\n",
      "\n",
      "月上柳梢\n",
      "content='头，萤火点点舞夏\n",
      "content='夜。\n",
      "梦回童年时光里，\n",
      "content='夏夜星空下许愿祈。\n",
      "\n",
      "\n",
      "content='时光缓缓在夏色中漫步，\n",
      "content='留下一串串金黄的足迹\n",
      "content='，\n",
      "每一缕阳光，每一声欢\n",
      "content='笑，都是夏天最美的诗。\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "# 导入模块\n",
    "from langchain.chat_models import ChatTongyi\n",
    "from langchain.schema.messages import AIMessage, HumanMessage\n",
    "import os\n",
    "\n",
    "tongyi_chat = ChatTongyi(\n",
    "    model=\"qwen-max\",\n",
    "    temperature=0,\n",
    "    dashscope_api_key=\"\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(role=\"user\", content=\"请生成一首夏天的诗词\")\n",
    "]\n",
    "\n",
    "# 使用 stream 方法来流式输出\n",
    "\n",
    "for chunk in tongyi_chat.stream(messages):\n",
    "    print(\"content='\", end=\"\")\n",
    "    print(chunk.content)\n",
    "print(\"'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成上面了之后,我们需要去实现一个就是token的追踪消耗的功能\n",
    "因为在实际开发的过程中,就是我们不可避免要消耗到token\n",
    "下面是一个案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "# 导入模块\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import os\n",
    "\n",
    "# 构造一个llm\n",
    "llm = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key=\"\",\n",
    "    max_tokens = 512\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"请生成一个关于人工智能的段落\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='人工智能，这个21世纪的科技奇迹，正以前所未有的速度渗透到我们生活的方方面面，重新定义着人类与技术的界限。它基于复杂算法、大数据分析和机器学习等核心技术，使计算机系统能够执行以往专属于人类的智力任务，比如语言理解、图像识别、决策制定甚至创意工作。从智能家居的便捷操控，到自动驾驶汽车的安全行驶；从医疗领域的疾病诊断与个性化治疗方案设计，到金融行业的风险评估与投资策略优化，人工智能的应用场景日益广泛，极大地提高了效率，改善了生活质量，并为解决许多全球性挑战提供了新的可能。然而，随着AI技术的迅猛发展，也引发了对就业结构变化、隐私保护、伦理道德以及AI治理等问题的深刻思考，促使社会各界共同努力，确保技术进步的同时，构建一个和谐共生、可持续发展的未来。' response_metadata={'model_name': 'qwen-max', 'finish_reason': 'stop', 'request_id': '0e071f31-1af2-9fbe-ab48-0810fe990b64', 'token_usage': {'input_tokens': 16, 'output_tokens': 174, 'total_tokens': 190}} id='run-2a2ada49-f025-4505-b1b7-f4bc4d0a04c8-0'\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatTongyi\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import os\n",
    "\n",
    "tongyi_chat = ChatTongyi(\n",
    "    model=\"qwen-max\",\n",
    "    temperature=0,\n",
    "    dashscope_api_key=\"\"\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = tongyi_chat.invoke(\"请生成一个关于人工智能的段落\")\n",
    "    print(result)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是目前的话,仅支持OpenAI,通义的话没有这个功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain的文本格式目前的话,上面主要的内容主要是以一个文本形式展示出来的,就是没有达到我们跟其他系统的联动性要求\n",
    "\n",
    "但是其实langchain,无论是LLM还是chatModel,其实都支持的是list(数组),JSON,函数,时间戳等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的话,我用几个例子来分别说明\n",
    "下面是一个函数的形式\n",
    "还要注意的是我们需要安装对应的Python的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python Pydantic使用指南](https://juejin.cn/post/7245975053233373244?searchId=20240701103757CDF5CCCF938A810D5BB3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_put ```json\n",
      "{\n",
      "  \"setup\": \"为什么电脑永远不会感冒？\",\n",
      "  \"punchline\": \"因为它有‘Windows’，但不开！\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 我这部分用一个讲笑话机器人演示 就是希望每次根据指令,可以输出一个这样笑话(小明是怎么dead的?笨dead的)\n",
    "\n",
    "# 首先还是导入我们的相应的模块\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Field主要是用来填入一些字段的,validator主要是用来校验一些字段的\n",
    "from langchain.pydantic_v1 import BaseModel,Field,validator\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "\n",
    "# 构造llm\n",
    "model = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key=\"\"\n",
    ")\n",
    "\n",
    "# 这里的话需要定义一个数据模型,用来描述最终的实例结构\n",
    "class Joke(BaseModel):\n",
    "    # description是描述这个字段的含义\n",
    "    setup: str = Field(description= \"设置笑话的问题\")\n",
    "    punchline: str = Field(description= \"回答笑话的答案\")\n",
    "\n",
    "    # 除了上面的我们的模版,我们还需要验证问题是否符合要求\n",
    "    @validator(\"setup\")\n",
    "    def question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"不符合预期的问题的格式!\")\n",
    "        return field\n",
    "\n",
    "# 将Joke的数据模型传入\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# 模版设置\n",
    "prompt = PromptTemplate(\n",
    "    template= \"回答用户的输入.\\n{format_instrc}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\n",
    "        \"format_instrc\": parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "# 做好我们上面的要求后,我们需要调用我们的方式  这里使用Python的一个管道\n",
    "\n",
    "prompt_and_model = prompt | model\n",
    "out_put = prompt_and_model.invoke({\"query\":\"给我讲一个笑话\"})\n",
    "print(\"out_put:\",out_put)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_put: ```json\n",
      "{\n",
      "  \"setup\": \"Why was the math book sad?\",\n",
      "  \"punchline\": \"Because it had too many problems.\"\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why was the math book sad?', punchline='Because it had too many problems.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先还是导入我们的相应的模块\n",
    "from langchain.llms import Tongyi\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Field主要是用来填入一些字段的,validator主要是用来校验一些字段的\n",
    "from langchain.pydantic_v1 import BaseModel,Field,validator\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "\n",
    "# 构造llm\n",
    "model = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key=\"\"\n",
    ")\n",
    "\n",
    "# 这里的话需要定义一个数据模型,用来描述最终的实例结构\n",
    "class Joke(BaseModel):\n",
    "    # description是描述这个字段的含义\n",
    "    setup: str = Field(description= \"设置笑话的问题\")\n",
    "    punchline: str = Field(description= \"回答笑话的答案\")\n",
    "\n",
    "    # 除了上面的我们的模版,我们还需要验证问题是否符合要求\n",
    "    @validator(\"setup\")\n",
    "    def question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"不符合预期的问题的格式!\")\n",
    "        return field\n",
    "\n",
    "# 将Joke的数据模型传入\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# 模版设置\n",
    "prompt = PromptTemplate(\n",
    "    template= \"回答用户的输入.\\n{format_instrc}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\n",
    "        \"format_instrc\": parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "# 做好我们上面的要求后,我们需要调用我们的方式  这里使用Python的一个管道\n",
    "\n",
    "prompt_and_model = prompt | model\n",
    "out_put = prompt_and_model.invoke({\"query\":\"给我讲一个笑话\"})\n",
    "print(\"out_put:\",out_put)\n",
    "\n",
    "# 验证回答是否符合我们的需求\n",
    "parser.parse(out_put)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Joke(setup='Why was the math book sad?', punchline='Because it had too many problems.')\n",
    "```\n",
    "上面的就是符合我们的需求的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后,第二个示例是,数组的形式\n",
    "```就是将LLM的输出格式化成Python list形式,类似['a','b','c']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John, Sarah, Michael, Jennifer, Robert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['John', 'Sarah', 'Michael', 'Jennifer', 'Robert']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Tongyi\n",
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "# DASHSCOPE_API_KEY = os.environ[\"DASHSCOPE_API_KEY\"]\n",
    "\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# 构造我们的llm\n",
    "model = Tongyi(\n",
    "    model = \"Qwen\",\n",
    "    temperature = 0,\n",
    "    dashscope_api_key = api_key\n",
    ")\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# 自定义模版\n",
    "prompt = PromptTemplate(\n",
    "    template= \"列出5个{subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "# 格式下模版\n",
    "_input = prompt.format(subject=\"常见的美国人名字\")\n",
    "output = model(_input)\n",
    "print(output)\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来的，就是我们如何让llm更聪明和扩展我们的langchain知识库\n",
    "第一步，就是使用RAG（Retrieval Augmented Generation）检索增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loader机制\n",
    "- 加载Markdown\n",
    "- 加载cvs\n",
    "- 加载文件目录\n",
    "- 加载html\n",
    "- 加载JSON\n",
    "- 加载PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来尝试读取一下Markdown文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading ./doc/初入Langchain.md",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:43\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 43\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0x80 in position 19: illegal multibyte sequence",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextLoader\n\u001b[0;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./doc/初入Langchain.md\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:56\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error loading ./doc/初入Langchain.md"
     ]
    }
   ],
   "source": [
    "# 首先依旧是导入模块\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./doc/初入Langchain.md')\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Langchain\\n\\n作者:`@xieleihan`[点击访问](https://github.com/xieleihan)\\n\\n本文遵守GPL3.0开源协议\\n\\n# 1.初识Langchain\\n\\n## 介绍\\n\\nLangChain是一个用于构建和部署以语言模型为核心的应用的框架。它为开发者提供了一套工具和模块，用于处理、管理和优化与大型语言模型（如OpenAI的GPT-3和GPT-4）相关的任务和流程。LangChain的目标是简化复杂的自然语言处理（NLP）应用的开发过程，使开发者能够更轻松地创建强大、灵活的应用程序。以下是LangChain的一些主要特点和功能：\\n\\n1. **模型集成**：LangChain支持多种语言模型的集成，允许开发者选择和切换不同的模型，以满足不同的需求。\\n\\n2. **数据处理**：提供了数据预处理、后处理和增强的功能，使得数据的准备和处理更加高效。\\n\\n3. **模块化设计**：框架采用模块化设计，开发者可以根据具体需求组合不同的模块，例如文本生成、问答系统、对话代理等。\\n\\n4. **扩展性和可定制性**：开发者可以轻松地扩展和定制LangChain中的组件，以适应特定的应用场景和业务需求。\\n\\n5. **工具集成**：LangChain支持与其他工具和服务的集成，如数据库、API、前端框架等，方便构建端到端的解决方案。\\n\\n6. **性能优化**：提供了多种性能优化策略，如模型压缩、并行处理等，帮助提高应用的效率和响应速度。\\n\\n7. **社区和文档**：LangChain拥有活跃的社区和详细的文档，开发者可以获得及时的支持和指导，快速上手并解决开发过程中遇到的问题。\\n\\nLangChain适用于各种应用场景，包括但不限于：\\n\\n- 聊天机器人和虚拟助手\\n- 自动化内容生成\\n- 文本摘要和翻译\\n- 情感分析和文本分类\\n- 问答系统和知识库构建\\n\\n通过LangChain，开发者可以更加专注于业务逻辑和应用设计，而不用花费大量时间在底层技术细节上，从而加速NLP应用的开发和部署。\\n\\n> 上面的内容引用了ChatGPT的回答\\n\\n>  LangChain由Harrison Chase于2022年10月作为[开源软件](https://zh.wikipedia.org/wiki/开源软件)项目推出\\n\\n## 使用\\n\\n得知它能做什么,我们接下来,就得来如何运行起我们的首个Langchain的应用\\n\\n*Warning⚙️这里要说下:我这边是自己跑首个运行的时候,踩了一些坑,如果可以的话,请用我这个方法来,因为我多次实践,发现这个方法最稳定,不用花很长时间去官网或者海外各种渠道找资料.这里如果你认可这个说明,请一定按照我的方法来,虽然后续不知道会出现多少问题,但是目前来说这样是最好的!*\\n\\n### 首先,你需要在电脑上安装一个Python环境\\n\\n官网地址:`https://www.python.org/`[点击访问](https://www.python.org/)\\n\\n这里我推荐下载最新的Python环境,但是至少需要***3.8以上***的,这个需要注意\\n\\n![Python环境](./image/1.1.png)\\n\\n然后,需要到`Vscode`或者`Pychran`等ide上,下载一个插件:`Jupyter`\\n\\n![](./image/1.2.png)\\n\\n### 再到vscode新建我们第一个文件\\n\\n`helloworld.ipynb`,这是一个jupyter的文件\\n\\n![](./image/1.3.png)\\n\\n![](./image/1.4.png)\\n\\n### 再接着,需要安装一些我们需要的package\\n\\n```python\\npip install langchain #安装langchain环境\\npip install openai # 安装openai api\\n# 需要注意的是,下面两个是你要用通义模型才需要安装,不然就安装上面的就行\\npip install python-dotenv #加载工具\\npip install dashscope #灵积模型服务\\n```\\n\\n我们到终端`cmd`或者到vscode里去执行上面的pip就行,看到`success`就可以了\\n\\n验证\\n\\n```python\\npip show [your-package]\\n```\\n\\n### 使用openai的模型(有点问题,能解决的用这个,不能的下面有其他)\\n\\n```python\\n# 引入openai key\\nimport os\\n# 配置环境变量\\nos.environ[\"OPENAI_KEY\"] = \"sk-yourOpenaiApi\"\\nos.environ[\"OPENAI_API_BASE\"] = \"https://api.openai.com/v1\"\\n```\\n\\n测试一下成功否\\n\\n```python\\nimport os\\nopenai_api_key = os.getenv(\"OPENAI_KEY\")\\nopenai_api_base = os.getenv(\"OPENAI_API_BASE\")\\nprint(\"OPENAI_API_KEY:\", openai_api_key)\\nprint(\"OPENAI_PROXY:\", openai_api_base)\\n```\\n\\n输出`对应的信息就行`\\n\\n接下来,是官方sdk测试(langchain)\\n\\n```python\\n#hello world\\nfrom langchain.llms import OpenAI\\nimport os\\n\\napi_base = os.getenv(\"OPENAI_API_BASE\")\\napi_key = os.getenv(\"OPENAI_KEY\")\\nllm = OpenAI(\\n    model=\"gpt-3.5-turbo-instruct\",\\n    temperature=0,\\n    openai_api_key=api_key,\\n    openai_api_base=api_base\\n    )\\nllm.predict(\"介绍下你自己\")\\n```\\n\\n这个需要你自己去试下,我的问题就是一直429 Error,然后我用海外借记卡也无法付款给openai买配额或者这类的吧,觉得是配额的问题(待指正)\\n\\n```text\\nRateLimitError: Error code: 429 - {\\'error\\': {\\'message\\': \\'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\\', \\'type\\': \\'insufficient_quota\\', \\'param\\': None, \\'code\\': \\'insufficient_quota\\'}}\\n```\\n\\n然后,我换了语言模型,毕竟我们都是去请求语言模型,所以这里更改我觉得对后面的影响不大,后续有问题我自己再解决\\n\\n```python\\n#导入相关包\\nimport os\\nfrom dotenv import find_dotenv, load_dotenv\\nload_dotenv(find_dotenv())\\nDASHSCOPE_API_KEY=os.environ[\"DASHSCOPE_API_KEY\"]\\nfrom langchain_community.llms import Tongyi\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import PromptTemplate\\n```\\n\\n这里我们需要去申请一个阿里的通义的`api-key`\\n\\n官网:`https://dashscope.console.aliyun.com/overview`[点击访问](https://dashscope.console.aliyun.com/overview)\\n\\n申请流程(待补充或略)\\n\\n然后,在`root目录`下新建一个文件`.env`\\n\\n内容\\n\\n```ini\\nDASHSCOPE_API_KEY=\"your-tongyiApiKey\"\\n```\\n\\n然后\\n\\n```python\\nllm=Tongyi(temperature=1)\\ntemplate=\\'\\'\\'\\n        你的名字是南秋SouthAki,当人问问题的时候,你都会在开头加上\\'生活苦短,我用通义\\',然后再回答{question}\\n    \\'\\'\\'\\nprompt=PromptTemplate(\\n        template=template,\\n        input_variables=[\"question\"]#这个question就是用户输入的内容,这行代码不可缺少\\n)\\nchain = LLMChain(#将llm与prompt联系起来\\n        llm=llm,\\n        prompt=prompt\\n        )\\nquestion=\\'介绍下通义模型\\'\\n\\nres=chain.invoke(question)#运行\\n    \\nprint(res[\\'text\\'])#打印结果\\n```\\n\\n```text\\n解释:\\nos,dotenv都是用来加载环境变量DASHSCOPE_API_KEY的\\nTongyi就是这里使用的通义千问大语言模型\\nPromptTemplate是提示词模板,用来给大模型做回答限制的,他会按照提示词模板的内容进行回答,跟模型的智慧程度有关,数据集越大的模型根据提示词做的回答越好,后期做Agent的效果越好.\\nLLMChain就是用来把LLM和prompt进行联系的\\ntemperature=1是调节文本多样性的,让回答更加丰富,为0时就会更加准确,大于0回答的就会带有llm的思维回答(可能会胡编乱造) res[\\'text\\']就是回答内容了,回答的一个字典包含了question和text\\n```\\n\\n![](./image/1.5.png)\\n\\n## 这里我们要完成什么呢\\n\\n![](./image/1.9.png)\\n\\n> 图来源于internet,请勿转载\\n\\n那么需要有点,基础的知识\\n\\n```text\\n// 创建LLM\\n在langchain中最基本的功能就是根据文本提示来生成新的文本\\n使用的方法是:`predict`\\nQuestion:\"帮我起一个具有中国特色的男孩名字\" => LLM.predict() => \"狗剩\"\\n\\n生成的结果根据你调用的模型不同会产生非常不同的结果差距,并且tempurature参数也会影响最终结果\\n```\\n\\n\\n\\n```text\\n// 自定义一个提示词模版\\n我们需要用上langchain提供的一个方法:`prompts`\\n使用方法:`langchain.prompts\\n\\n举例:\\nQuestion:\"帮我起一个具有{country}特色的男孩名字\" =>prompts.format(country = \"美国\") => \"山姆\"\\n```\\n\\n\\n\\n## All right:上面的话,补充点东西\\n\\n就是,刚一连解决两个可能的问题\\n\\n就是在定义这个`OPENAI-API-KEY`的时候,里面有个填入代理的地方\\n\\n原来我们是这样的\\n\\n```python\\nos.environ[\"OPENAI_API_BASE\"] = \"https://www.jcapikey.com\"\\n```\\n\\nbut,这个可能对下面的langchain的运行造成这个问题\\n\\n**<font color=\"red\">AttributeError</font>**\\n\\n```text\\n错误message\\nAttributeError                            Traceback (most recent call last)\\nCell In[8], line 12\\n      5 api_key = os.getenv(\"OPENAI_KEY\")\\n      6 llm = ChatOpenAI(\\n      7     model=\"gpt-3.5-turbo-instruct\",\\n      8     temperature=0,\\n      9     openai_api_key=api_key,\\n     10     openai_api_base=api_base\\n     11 )\\n---> 12 result = llm.predict(\"介绍下你自己\")\\n     13 print(type(result))  # 打印返回结果的类型\\n\\nFile c:\\\\Users\\\\xiele\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\langchain_core\\\\_api\\\\deprecation.py:148, in deprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper(*args, **kwargs)\\n    146     warned = True\\n    147     emit_warning()\\n--> 148 return wrapped(*args, **kwargs)\\n\\nFile c:\\\\Users\\\\xiele\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\langchain_core\\\\language_models\\\\chat_models.py:885, in BaseChatModel.predict(self, text, stop, **kwargs)\\n    883 else:\\n    884     _stop = list(stop)\\n--> 885 result = self([HumanMessage(content=text)], stop=_stop, **kwargs)\\n    886 if isinstance(result.content, str):\\n    887     return result.content\\n...\\n--> 461     response = response.dict()\\n    462 for res in response[\"choices\"]:\\n    463     message = convert_dict_to_message(res[\"message\"])\\n\\nAttributeError: \\'str\\' object has no attribute \\'dict\\'\\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\\n```\\n\\n解决方法:\\n\\n```python\\nos.environ[\"OPENAI_API_BASE\"] = \"https://www.jcapikey.com/v1\"\\n```\\n\\n**就是在这个代理地址结尾,添加`/v1`**\\n\\n这样,可以解决问题\\n\\n这里,把我成熟方案贴在这里,试下可否可行,因为我测试后,现在是`403 Error`,不过这个应该就是配额的问题了\\n\\n```python\\nimport os\\nfrom langchain.chat_models import ChatOpenAI\\n\\napi_base = os.getenv(\"OPENAI_API_BASE\")\\napi_key = os.getenv(\"OPENAI_KEY\")\\nllm = ChatOpenAI(\\n    model=\"gpt-3.5-turbo-instruct\",\\n    temperature=0,\\n    openai_api_key=api_key,\\n    openai_api_base=api_base\\n)\\nresult = llm.predict(\"介绍下你自己\")\\nprint(result)  # 打印返回结果的类型\\n\\n```\\n\\n参考链接:[点击访问](https://blog.csdn.net/jining11/article/details/134806188)\\n\\n##  然后我们对这个做个小项目\\n\\n### Project\\n\\n现在我们是测试完毕了,tongyi的模型,我们接着来实现一个起名大师\\n\\n依旧的,先是OpenAI的,但是没有额度,我们转译一下试试\\n\\n```python\\n#起名大师\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nimport os\\napi_base = os.getenv(\"OPENAI_API_BASE\")\\napi_key = os.getenv(\"OPENAI_KEY\")\\nllm = OpenAI(\\n    model=\"gpt-3.5-turbo-instruct\",\\n    temperature=0,\\n    openai_api_key=api_key,\\n    openai_api_base=api_base\\n    )\\nprompt = PromptTemplate.from_template(\"你是一个起名大师,请模仿示例起3个{county}名字,比如男孩经常被叫做{boy},女孩经常被叫做{girl}\")\\nmessage = prompt.format(county=\"中国特色的\",boy=\"狗蛋\",girl=\"翠花\")\\nprint(message)\\nllm.predict(message)\\n```\\n\\n> 🚧:以后相关的Openai的接口我也会同步的贴出来,需要自己买额度测试,按道理应该是可以跑通的\\n\\n下面的这个是使用了国产大语言模型**Tongyi**的(*通过对OpenAI的直接转译得到*)\\n\\n```python\\nfrom langchain_community.llms import Tongyi\\nfrom langchain.prompts import PromptTemplate\\nimport os\\n\\n# 获取环境变量中的 API 基本 URL 和密钥\\napi_base = os.getenv(\"OPENAI_API_BASE\")\\napi_key = os.getenv(\"OPENAI_KEY\")\\n\\n# 使用 Tongyi 模型\\nllm = Tongyi(\\n    model=\"gpt-3.5-turbo-instruct\",\\n    temperature=0,\\n    openai_api_key=api_key,\\n    openai_api_base=api_base\\n)\\n\\n# 创建 PromptTemplate\\nprompt = PromptTemplate.from_template(\"你是一个起名大师,请模仿示例起3个{county}名字,比如男孩经常被叫做{boy},女孩经常被叫做{girl}\")\\n\\n# 格式化消息\\nmessage = prompt.format(county=\"中国特色的\", boy=\"狗蛋\", girl=\"翠花\")\\nprint(message)\\n\\n# 调用 Tongyi 模型进行预测\\nresponse = llm.predict(message)\\nprint(response)\\n```\\n\\n但是发现,直接转译有问题,虽然结果是这样的\\n\\n![](./image/1.6.png)\\n\\n所以,我对这个进行相应修改\\n\\n```python\\nllm=Tongyi(temperature=0)\\ntemplate=\\'\\'\\'\\n        你是一个起名大师,请模仿示例起3个{county}名字,比如男孩经常被叫做{boy},女孩经常被叫做{girl}\\n    \\'\\'\\'\\nprompt=PromptTemplate(\\n        template=template,\\n        input_variables=[\"county\", \"boy\", \"girl\"]# 这个question就是用户输入的内容,这行代码不可缺少\\n)\\nchain = LLMChain(#将llm与prompt联系起来\\n        llm=llm,\\n        prompt=prompt\\n        )\\n\\n# 用户输入的问题\\ncounty = \"中国特色的\"\\nboy = \"狗蛋\"\\ngirl = \"翠花\"\\n\\n# 格式化消息\\nmessage = prompt.format(county=county, boy=boy, girl=girl)\\n\\n# 运行并打印结果\\nres = chain.invoke({\"county\": county, \"boy\": boy, \"girl\": girl})\\nprint(res[\\'text\\'])\\n\\n# 尝试打印message\\nprint(message)\\n# 输出llm的predict\\nllm.predict(message)\\n```\\n\\n这里需要解释一下\\n\\n> 在语言模型（如GPT-3或其他类似模型）中，`temperature` 参数是一个控制生成文本的随机性的超参数。它在生成模型输出时影响词的选择方式，具体如下：\\n>\\n> 1. **低 `temperature` 值（接近0）**：\\n> \\t- **更确定性**：模型更倾向于选择概率最高的词，因此生成的文本更有条理，连贯性较强，但也可能显得缺乏创意和多样性。\\n> \\t- **示例**：如果`temperature`设为0，模型将总是选择概率最高的词，这使得每次生成的结果都非常相似或相同。\\n> 2. **高 `temperature` 值（接近1）**：\\n> \\t- **更随机性**：模型在选择词时考虑更多的可能性，因此生成的文本更具创意和多样性，但也可能出现语义上不连贯或不合逻辑的内容。\\n> \\t- **示例**：如果`temperature`设为1，模型在生成文本时会有更多的自由度，选择概率较低的词的机会增加，从而生成更具创意的文本。\\n> 3. **中等 `temperature` 值（如0.7）**：\\n> \\t- **平衡性**：既有一定的随机性来生成多样化的内容，同时也保持一定的连贯性和逻辑性。\\n> \\t- **示例**：很多情况下，设定`temperature`为0.7是一个较好的选择，可以在生成连贯性和创意之间找到平衡。\\n\\n所以我在这一行上,对`temperature`设置了一个参数,让语言模型严格的按照我们的需求输出\\n\\n`llm=Tongyi(temperature=0)`\\n\\n结果如图\\n\\n![](./image/1.7.png)\\n\\nOK,到这里的话,我们的环境测试之类的,都没有问题了\\n\\nBut,我们知道在以后的生产环境中一定不能用上这种形式的OutPrintf\\n\\n我们要得到的,是里面的数据,数据那用数组去存储\\n\\n这里需要用上`Python`的一些相关知识\\n\\n```python\\n# 首先的话\\n# 需要导入Python中一个输出的基本类BaseOutputParser\\n# 导入到langchain中\\nfrom langchain.schema import BaseOutputParser\\n# 自定义类\\n# 继承了BaseOutputParser\\n# 重写了parse方法\\nclass CommaSeparatedListOutputParser(BaseOutputParser):\\n    def parse(self, text: str) -> str:\\n        # 输出结果 strip()去除空格\\n        # split()分割字符串\\n        return text.strip().split(\", \")\\n    \\nCommaSeparatedListOutputParser().parse(\"apple, banana, cherry\")\\n```\\n\\n```text\\nOutput:[\\'apple\\', \\'banana\\', \\'cherry\\']\\n```\\n\\n解释如下\\n\\n这里,先导入Python的一个输出的基本类`BaseOutputPaser`\\n\\n>`BaseOutputParser` 是一个基础类，用于解析语言模型（LLM）生成的输出。在使用 LLM 时，模型生成的原始输出可能需要进一步处理才能满足特定需求。`BaseOutputParser` 提供了一个统一的接口来实现这种处理。\\n>\\n>在 LangChain 中，`BaseOutputParser` 类的主要用途是定义一种方法，将 LLM 的原始输出转换为用户需要的格式。这种方法的实现可以是多种多样的，例如提取特定信息、格式化输出、分割字符串等。\\n>\\n>### `BaseOutputParser` 的主要方法\\n>\\n>- **`parse(self, text: str)`**：这是一个抽象方法，需要在子类中实现。它接受一个字符串（模型生成的原始输出）作为输入，并返回解析后的结果。\\n\\n然后来实现我们最终的目的\\n\\n```python\\n# 起名大师\\n# 导入相关包\\nimport os\\nfrom dotenv import find_dotenv, load_dotenv\\nload_dotenv(find_dotenv())\\nDASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")\\nfrom langchain_community.llms import Tongyi\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.schema import BaseOutputParser\\n\\n# 自定义类\\nclass CommaSeparatedListOutputParser(BaseOutputParser):\\n    def parse(self, text: str):\\n        # return text.strip().split(\", \")\\n        return [item.strip() for item in text.strip().split(\",\")]\\n\\nllm = Tongyi(\\n    temperature=0,\\n    openai_api_key=DASHSCOPE_API_KEY\\n)\\n\\ntemplate = \\'\\'\\'\\n你是一个起名大师,请模仿示例起3个{county}名字,比如男孩经常被叫做{boy},女孩经常被叫做{girl},请返回以逗号分隔的列表形式。仅返回逗号分隔的列表，不要返回其他内容。\\n\\'\\'\\'\\nprompt = PromptTemplate(\\n    template=template,\\n    input_variables=[\"county\", \"boy\", \"girl\"]\\n)\\n\\n# 设置解析器\\nparser = CommaSeparatedListOutputParser()\\n\\n# 将 LLM 与 Prompt 和解析器连接起来\\nchain = LLMChain(\\n    llm=llm,\\n    prompt=prompt,\\n    output_parser=parser\\n)\\n\\n# 用户输入的问题\\ncounty = \"中国特色的\"\\nboy = \"狗蛋\"\\ngirl = \"翠花\"\\n\\n# 格式化消息\\nmessage = prompt.format(county=county, boy=boy, girl=girl)\\n\\n# 运行并打印结果\\nres = chain.invoke({\"county\": county, \"boy\": boy, \"girl\": girl})\\nprint(res)  # 应该输出一个列表\\n\\n# 尝试打印message\\nprint(message)\\n\\n# 直接调用llm的预测\\nstrs = llm.predict(message)\\nparsed_output = parser.parse(strs)\\nprint(parsed_output)\\n```\\n\\n![](./image/1.8.png)\\n\\n那么,第一章就到这里,有问题的可以在下面的评论区评论,我看后会尽力帮你解决.\\n\\n然后,这个文章尚未完结,后面发现有什么问题会进行补充.\\n\\n', metadata={'source': './doc/初入Langchain.md'})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看起来我们遇到了编码错误的问题,这在以后很常见,所以我们统一指定我们的编码格式为Unicode\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./doc/初入Langchain.md', encoding='utf-8')\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面演示我们的csv加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Project: AI GC培训\\nDES: 培训课程\\nPrice: 500\\nPeople: 100\\nLocation: 北京', metadata={'source': '北京', 'row': 0}), Document(page_content='Project: AI工程师认证\\nDES: 微软AI认证\\nPrice: 6000\\nPeople: 200\\nLocation: 西安', metadata={'source': '西安', 'row': 1}), Document(page_content='Project: AI应用大会\\nDES: AI应用创新大会\\nPrice: 200门票\\nPeople: 300\\nLocation: 深圳', metadata={'source': '深圳', 'row': 2}), Document(page_content='Project: AI 应用咨询服务\\nDES: AI与场景结合\\nPrice: 1000/小时\\nPeople: 50\\nLocation: 香港', metadata={'source': '香港', 'row': 3}), Document(page_content='Project: AI项目可研\\nDES: 可行性报告\\nPrice: 20000\\nPeople: 60\\nLocation: 上海', metadata={'source': '上海', 'row': 4})]\n"
     ]
    }
   ],
   "source": [
    "# 首先还是导入我们的模块\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# 创建 CsvLoader 实例并指定文件路径和编码\n",
    "# loader = CSVLoader(file_path='loader.csv', encoding='utf-8')\n",
    "# 这里的话,我们可以指定我们想要加载的某一列数据\n",
    "loader = CSVLoader(file_path='loader.csv', encoding='utf-8', source_column='Location')\n",
    "\n",
    "# 加载数据\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一个就是Excel表格的解析\n",
    "需要安装一个安装包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"unstructured[xlsx]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='\\n\\n\\n名称\\n宏图科技发展有限公司\\n\\n\\n注册地址\\n江苏省南京市雨花台区软件大道101号\\n\\n\\n成立日期\\n40679\\n\\n\\n法定代表人\\n李强\\n\\n\\n注册资本\\n人民币5000万元\\n\\n\\n员工人数\\n约200人\\n\\n\\n联系电话\\n025-88888888\\n\\n\\n电子邮箱\\ninfo@hongtutech.cn\\n\\n\\n资产总额\\n人民币1.2亿元，较上年同期下降30%\\n\\n\\n负债总额\\n人民币1.8亿元，较上年同期上升50%，资不抵债\\n\\n\\n营业收入\\n人民币3000万元，较上年同期下降60%\\n\\n\\n净利润\\n亏损人民币800万元，去年同期为盈利人民币200万元\\n\\n\\n现金流量\\n公司现金流量紧张，现金及现金等价物余额为人民币500万元，难以支撑日常运营\\n\\n\\n', metadata={'source': 'example\\\\fake.xlsx'})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 某个目录下,有Excel文件,我们需要把目录下所有的xlsx文件都加载进来\n",
    "# 这里需要先安装 pip install \"unstructured[xlsx]\"\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# 这里需要注意的是目录下的.html文件和.rst文件不会被这种loader加载\n",
    "\n",
    "# loader = DirectoryLoader(\"目录地址\", glob=\"指定加载说明格式的文件\")\n",
    "\n",
    "loader = DirectoryLoader(\"./example\", glob=\"*.xlsx\")\n",
    "docs = loader.load()\n",
    "print(docs)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来的是解析html文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xiele\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\xiele\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='首发于\\n\\n自然语言处理算法与实践\\n\\n切换模式\\n\\n烛之文\\n\\n1、前言\\n\\n在上一篇提及到文中提出kNN-NER框架是一种检索式增强的方法（retrieval\\r\\n                                                                augmented methods），就去查看有关retrieval\\r\\n                                                                augmented的论文，了解其核心思想，觉得检索式增强的方法很适合许多业务场景使用，联盟简捷的方式将外部知识融于模型中去。今天就分享一篇来自Facebook\\r\\n                                                                AI Research的论文，论文提出一种检索式增强生成方法，应用于知识密集型的NLP任务（如问答生成），该篇论文被2020年NeurIPS会议接收。\\n\\n文中说到，以BERT宣布大规模预训练模型将很多事实知识信息存入模型中，可以看着是预训练的参数类型，尽管以fine-tuned方式在下游任务取得显著的成效，但创新方法仍存在无法精准地获取和操作知识的缺陷。而在上述问题上提及，传统知识检索的方法能很好的应对，创新方法可以看着是非参数于是，论文提出检索式增强生成方法（retrieval-augmentedgeneration，RAG），主要思想就是将预训练的参数与非参数记忆分为语言生成任务，将两类模型集成起来提高任务处理效果。\\n\\n2、RAG方法\\n\\n上图为论文提出RAG模型的整体示意图。主要包括两大模块：一个检索器（Retriever， p_\\\\eta(z|x) ） +\\r\\n                                                                一个生成器（Generator， p_\\\\theta(y_i|x,z,y_{1:i-1})\\r\\n                                                                ）。前者包括querycoder和document\\r\\n                                                                index，分别负责query的编码和文档的索引；后者是一个seq2seq的生成模型。在检索中，可以使用最大内积搜索的方法（MIPS）来检索top-K相关文档。\\n\\n最后，把检索器输出的信息当成额外的文本信息，通过边际化的方式（marginalize）融于生成器中，生成最终的序列。在融合过程中，论文提出两种不同的方式：RAG-Sequence\\r\\n                                                                Model和RAG-Token\\r\\n                                                                Model，主要区别在于前者利用同一篇文档来生成所有序列；后者用检索到的所有文档来生成序列。其计算方式如下：\\n\\n关于检索器 p_\\\\eta(z|x) ，由输入查询\\r\\n                                                                x经过编码器得到编码向量q(x)，另外将知识库里的文档事先通过编码器得到文档编码向量d(z)，然后q(x)与d(z)做最大内积搜索到top-K相关文档，输出作为\\r\\n                                                                p_\\\\eta(z|x) 。\\n\\n关于生成器p_\\\\theta(y_i|x,z,y_{1:p_\\\\eta(z|x)\\r\\n                                                                    ，论文中是用BART-large作为训练模型，然后将查询 x和检索到的z输入其中得到生成的序列文本。\\n\\n在解码过程中，RAG-Token\\r\\n                                                                Model可视为一个标准的自回归生成模型，按常规的beam\\r\\n                                                                search方式就可以解码；而在RAG-Sequence\\r\\n                                                                Model中，因为每个文档都生成一个序列，不能正常的beam search方式来解码。文中是对每个文档按beam\\r\\n                                                                search解码出一个序列，得到解码序列集合，针对每个生成序列，用其生成概率与 p_\\\\eta(z|x)\\r\\n                                                                点乘得到一个概率得分，取最大值对应的序列为最终输出。\\n\\n3、实验\\n\\n论文在四类知识密集型任务上进行实验，具体包括开放问答、摘要式问答、开放问题生成、事实判断，并结合维基百科（包含2100万个文档）作为检索库。\\n\\n上表显示：在开放域问答任务上，论文指出两个方法在4个数据集都取得了新的最佳结果。\\n\\n上表显示，在Jeopardy Question\\r\\n                                                                Generation任务（Jeopardy数据集）上，RAG-Tok取得了最优结果，且RAG都超过BART的风格；\\n\\n在Abstractive Question\\r\\n                                                                    Answering任务（MSMARCO数据集）上，RAG模型都优于BART模型，但接近已有的最佳模型，其现有的论文在实验中没有利用的数据集包含文档黄金访问信息；\\n\\n在Fact\\r\\n                                                                    Verification任务上（FVR3,FVR2数据集）上，对于3路分类（FVR3），RAG比最优模型差4.3%，然而最优模型都是基于复杂的pipeline方法，需要大量的中间特征工程，而RAG不需要这些特征工程就可以达到接近的效果。\\n\\n此外，论文显示对比BART，RAG模型生成的文本更符合事实，更准确，且适合您。\\n\\n4、结语\\n\\n本次分享基于检索增强方式将外部知识融于生成任务中一个新的框架——RAG。对比T5和BART创新擅长处理生成任务的模型来说，RAG更新外部知识是不需要重新预训练，成本低；而对比pipeline方法，RAG利用外部知识并不需要构造负责的工程。总的来说，RAG方法可作为外部知识融合框架的一个有效实例。\\n\\n有兴趣关注笔者公众号：自然语言处理算法与实践\\n\\n编辑于 2022-04-06 10:47\\n\\n自然语言处理算法与实践', metadata={'source': './loader.html'})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "loader = UnstructuredHTMLLoader(\"./loader.html\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='RAG:将检索与生成方式的影响来做生成任务 - 知乎\\n\\n\\n\\nRAG:将检索与生成方式的影响来做生成任务 - 知乎\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n首发于自然语言处理算法与实践\\n\\n\\n\\n\\n\\n\\n                                                切换模式\\n\\n\\n\\n写文章\\n\\n\\n\\n\\n\\n\\n\\n\\n烛之文\\n\\n\\u200b\\n\\n\\n\\n1、前言\\n在上一篇提及到文中提出kNN-NER框架是一种检索式增强的方法（retrieval\\n                                                                augmented methods），就去查看有关retrieval\\n                                                                augmented的论文，了解其核心思想，觉得检索式增强的方法很适合许多业务场景使用，联盟简捷的方式将外部知识融于模型中去。今天就分享一篇来自Facebook\\n                                                                AI Research的论文，论文提出一种检索式增强生成方法，应用于知识密集型的NLP任务（如问答生成），该篇论文被2020年NeurIPS会议接收。\\n                                                            \\n\\n                                                                文中说到，以BERT宣布大规模预训练模型将很多事实知识信息存入模型中，可以看着是预训练的参数类型，尽管以fine-tuned方式在下游任务取得显著的成效，但创新方法仍存在无法精准地获取和操作知识的缺陷。而在上述问题上提及，传统知识检索的方法能很好的应对，创新方法可以看着是非参数于是，论文提出检索式增强生成方法（retrieval-augmentedgeneration，RAG），主要思想就是将预训练的参数与非参数记忆分为语言生成任务，将两类模型集成起来提高任务处理效果。\\n                                                            \\n2、RAG方法\\n\\n\\n\\n                                                                上图为论文提出RAG模型的整体示意图。主要包括两大模块：一个检索器（Retriever， p_\\\\eta(z|x) ） +\\n                                                                一个生成器（Generator， p_\\\\theta(y_i|x,z,y_{1:i-1})\\n                                                                ）。前者包括querycoder和document\\n                                                                index，分别负责query的编码和文档的索引；后者是一个seq2seq的生成模型。在检索中，可以使用最大内积搜索的方法（MIPS）来检索top-K相关文档。\\n                                                            \\n\\n                                                                最后，把检索器输出的信息当成额外的文本信息，通过边际化的方式（marginalize）融于生成器中，生成最终的序列。在融合过程中，论文提出两种不同的方式：RAG-Sequence\\n                                                                Model和RAG-Token\\n                                                                Model，主要区别在于前者利用同一篇文档来生成所有序列；后者用检索到的所有文档来生成序列。其计算方式如下：\\n                                                            \\n\\n\\n\\n\\n关于检索器 p_\\\\eta(z|x) ，由输入查询\\n                                                                x经过编码器得到编码向量q(x)，另外将知识库里的文档事先通过编码器得到文档编码向量d(z)，然后q(x)与d(z)做最大内积搜索到top-K相关文档，输出作为\\n                                                                p_\\\\eta(z|x) 。\\n\\n\\n关于生成器p_\\\\theta(y_i|x,z,y_{1:p_\\\\eta(z|x)\\n                                                                    ，论文中是用BART-large作为训练模型，然后将查询 x和检索到的z输入其中得到生成的序列文本。\\n                                                            \\n在解码过程中，RAG-Token\\n                                                                Model可视为一个标准的自回归生成模型，按常规的beam\\n                                                                search方式就可以解码；而在RAG-Sequence\\n                                                                Model中，因为每个文档都生成一个序列，不能正常的beam search方式来解码。文中是对每个文档按beam\\n                                                                search解码出一个序列，得到解码序列集合，针对每个生成序列，用其生成概率与 p_\\\\eta(z|x)\\n                                                                点乘得到一个概率得分，取最大值对应的序列为最终输出。\\n3、实验\\n\\n                                                                论文在四类知识密集型任务上进行实验，具体包括开放问答、摘要式问答、开放问题生成、事实判断，并结合维基百科（包含2100万个文档）作为检索库。\\n                                                            \\n\\n\\n\\n                                                                上表显示：在开放域问答任务上，论文指出两个方法在4个数据集都取得了新的最佳结果。\\n上表显示，在Jeopardy Question\\n                                                                Generation任务（Jeopardy数据集）上，RAG-Tok取得了最优结果，且RAG都超过BART的风格；\\n                                                                \\n                                                                在Abstractive Question\\n                                                                    Answering任务（MSMARCO数据集）上，RAG模型都优于BART模型，但接近已有的最佳模型，其现有的论文在实验中没有利用的数据集包含文档黄金访问信息；\\n                                                                \\n在Fact\\n                                                                    Verification任务上（FVR3,FVR2数据集）上，对于3路分类（FVR3），RAG比最优模型差4.3%，然而最优模型都是基于复杂的pipeline方法，需要大量的中间特征工程，而RAG不需要这些特征工程就可以达到接近的效果。\\n                                                                \\n\\n\\n\\n                                                                    此外，论文显示对比BART，RAG模型生成的文本更符合事实，更准确，且适合您。\\n4、结语\\n\\n                                                                    本次分享基于检索增强方式将外部知识融于生成任务中一个新的框架——RAG。对比T5和BART创新擅长处理生成任务的模型来说，RAG更新外部知识是不需要重新预训练，成本低；而对比pipeline方法，RAG利用外部知识并不需要构造负责的工程。总的来说，RAG方法可作为外部知识融合框架的一个有效实例。\\n                                                                \\n\\n有兴趣关注笔者公众号：自然语言处理算法与实践\\n\\n\\n\\n\\n编辑于 2022-04-06 10:47\\n\\n\\n\\n\\n\\n\\n\\n\\n自然语言处理算法与实践\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "# 通过上面的UnstructuredHTMLLoader加载出来的html文件,不是关键信息很多,我们通常不需要这些文件\n",
    "# 使用BSHTMLLoader来加载html文件,然后提取关键信息\n",
    "\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.schema import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class CustomBSHTMLLoader(BSHTMLLoader):\n",
    "    def __init__(self, file_path: str, encoding: str = \"utf-8\", **kwargs):\n",
    "        super().__init__(file_path, **kwargs)\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def lazy_load(self):\n",
    "        with open(self.file_path, \"r\", encoding=self.encoding) as f:\n",
    "            soup = BeautifulSoup(f, **self.bs_kwargs)\n",
    "            text = soup.get_text(self.get_text_separator)\n",
    "            if soup.title:\n",
    "                text = f\"{soup.title.string}\\n{text}\"\n",
    "            yield Document(page_content=text)\n",
    "\n",
    "# 创建 CustomBSHTMLLoader 实例并指定文件路径和编码\n",
    "loader = CustomBSHTMLLoader(file_path=\"./loader.html\", encoding=\"utf-8\")\n",
    "\n",
    "# 加载数据\n",
    "docs = loader.load()\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用loader加载JSON文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "jq package not found, please install it with `pip install jq`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\document_loaders\\json_loader.py:54\u001b[0m, in \u001b[0;36mJSONLoader.__init__\u001b[1;34m(self, file_path, jq_schema, content_key, is_content_key_jq_parsable, metadata_func, text_content, json_lines)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjq\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjq \u001b[38;5;241m=\u001b[39m jq\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jq'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONLoader\n\u001b[1;32m----> 2\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mJSONLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple_prompt.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mjq_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.template\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\Users\\xiele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\document_loaders\\json_loader.py:58\u001b[0m, in \u001b[0;36mJSONLoader.__init__\u001b[1;34m(self, file_path, jq_schema, content_key, is_content_key_jq_parsable, metadata_func, text_content, json_lines)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjq \u001b[38;5;241m=\u001b[39m jq\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjq package not found, please install it with `pip install jq`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m     )\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m Path(file_path)\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jq_schema \u001b[38;5;241m=\u001b[39m jq\u001b[38;5;241m.\u001b[39mcompile(jq_schema)\n",
      "\u001b[1;31mImportError\u001b[0m: jq package not found, please install it with `pip install jq`"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "loader = JSONLoader(\n",
    "    file_path = \"simple_prompt.json\",jq_schema=\".template\",text_content=True\n",
    ")\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='', metadata={'source': 'D:\\\\Documents\\\\项目\\\\langchain\\\\langchain学习\\\\prompt_with_ouput_parser.json', 'seq_num': 1, 'input_variables': ['question', 'student_answer'], 'template': 'Given the following question and student answer, provide a correct answer and score the student answer.\\nQuestion: {question}\\nStudent Answer: {student_answer}\\nCorrect Answer:'})]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    " \n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    " \n",
    "class JSONLoader(BaseLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: Union[str, Path],\n",
    "        content_key: Optional[str] = None,\n",
    "        metadata_func: Optional[Callable[[Dict, Dict], Dict]] = None,\n",
    "        text_content: bool = False,\n",
    "        json_lines: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the JSONLoader with a file path, an optional content key to extract specific content,\n",
    "        and an optional metadata function to extract metadata from each record.\n",
    "        \"\"\"\n",
    "        self.file_path = Path(file_path).resolve()\n",
    "        self._content_key = content_key\n",
    "        self._metadata_func = metadata_func\n",
    "        self._text_content = text_content\n",
    "        self._json_lines = json_lines\n",
    " \n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load and return documents from the JSON file.\"\"\"\n",
    "        docs: List[Document] = []\n",
    "        if self._json_lines:\n",
    "            with self.file_path.open(encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        self._parse(line, docs)\n",
    "        else:\n",
    "            self._parse(self.file_path.read_text(encoding=\"utf-8\"), docs)\n",
    "        return docs\n",
    " \n",
    "    def _parse(self, content: str, docs: List[Document]) -> None:\n",
    "        \"\"\"Convert given content to documents.\"\"\"\n",
    "        data = json.loads(content)\n",
    " \n",
    "        # 假设 data 是字典而不是列表\n",
    "        if isinstance(data, dict):\n",
    "            data = [data]  # 将字典转换为单元素列表以便统一处理\n",
    " \n",
    "        # 确保 data 是列表\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"Data is not a list!\")\n",
    " \n",
    "        # 验证和处理每个记录\n",
    "        for i, sample in enumerate(data, len(docs) + 1):\n",
    "            text = self._get_text(sample=sample)\n",
    "            metadata = self._get_metadata(sample=sample, source=str(self.file_path), seq_num=i)\n",
    "            docs.append(Document(page_content=text, metadata=metadata))\n",
    " \n",
    "    def _get_text(self, sample: Any) -> str:\n",
    "        \"\"\"Convert sample to string format\"\"\"\n",
    "        if self._content_key is not None:\n",
    "            content = sample.get(self._content_key)\n",
    "        else:\n",
    "            content = sample\n",
    " \n",
    "        if self._text_content and not isinstance(content, str):\n",
    "            raise ValueError(\n",
    "                f\"Expected page_content is string, got {type(content)} instead. \\\n",
    "                    Set `text_content=False` if the desired input for \\\n",
    "                    `page_content` is not a string\"\n",
    "            )\n",
    " \n",
    "        # In case the text is None, set it to an empty string\n",
    "        elif isinstance(content, str):\n",
    "            return content\n",
    "        elif isinstance(content, dict):\n",
    "            return json.dumps(content) if content else \"\"\n",
    "        else:\n",
    "            return str(content) if content is not None else \"\"\n",
    " \n",
    "    def _get_metadata(self, sample: Dict[str, Any], **additional_fields: Any) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Return a metadata dictionary base on the existence of metadata_func\n",
    "        :param sample: single data payload\n",
    "        :param additional_fields: key-word arguments to be added as metadata values\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self._metadata_func is not None:\n",
    "            return self._metadata_func(sample, additional_fields)\n",
    "        else:\n",
    "            return additional_fields\n",
    " \n",
    "    def _validate_content_key(self, data: Any) -> None:\n",
    "        \"\"\"Check if a content key is valid, assuming data is a list of dictionaries.\"\"\"\n",
    "        # Assuming data should be a list of dicts, we take the first dict to examine.\n",
    "        # Make sure to verify that data is list and it is not empty, and its elements are dicts.\n",
    "        if isinstance(data, list) and data:\n",
    "            sample = data[0]\n",
    "            if not isinstance(sample, dict):\n",
    "                raise ValueError(\n",
    "                    f\"Expected the data schema to result in a list of objects (dict), \"\n",
    "                    \"so sample must be a dict but got `{type(sample)}`.\"\n",
    "                )\n",
    " \n",
    "            if self._content_key not in sample:\n",
    "                raise ValueError(\n",
    "                    f\"The content key `{self._content_key}` is missing in the sample data.\"\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\"Data is empty or not a list!\")\n",
    " \n",
    "    def _validate_metadata_func(self, data: Any) -> None:\n",
    "        \"\"\"Check if the metadata_func output is valid, assuming data is a list of dictionaries.\"\"\"\n",
    "        if isinstance(data, list) and data:\n",
    "            sample = data[0]\n",
    "            if self._metadata_func is not None:\n",
    "                sample_metadata = self._metadata_func(sample, {})\n",
    "                if not isinstance(sample_metadata, dict):\n",
    "                    raise ValueError(\n",
    "                        f\"Expected the metadata_func to return a dict but got `{type(sample_metadata)}`.\"\n",
    "                    )\n",
    "        else:\n",
    "            raise ValueError(\"Data is empty or not a list!\")\n",
    " \n",
    "def item_metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    # metadata[\"_type\"] = record.get(\"_type\")\n",
    "    metadata[\"input_variables\"] = record.get(\"input_variables\")\n",
    "    metadata[\"template\"] = record.get(\"template\")\n",
    "    return metadata\n",
    " \n",
    "loader = JSONLoader(file_path='./simple_prompt.json', content_key='description', metadata_func=item_metadata_func)\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '请讲一个关于{name}的{what}的故事'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jmespath\n",
    "from pathlib import Path\n",
    "\n",
    "class CustomJSONLoader:\n",
    "    def __init__(self, file_path, jq_schema, text_content=True, encoding=\"utf-8\"):\n",
    "        self.file_path = Path(file_path).resolve()\n",
    "        self.jq_schema = jq_schema\n",
    "        self.text_content = text_content\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def load(self):\n",
    "        # 读取 JSON 文件\n",
    "        with open(self.file_path, 'r', encoding=self.encoding) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # 使用 jmespath 解析 JSON 数据\n",
    "        result = jmespath.search(self.jq_schema, data)\n",
    "\n",
    "        if self.text_content:\n",
    "            return {\"content\": result}\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "# 使用自定义的 JSONLoader 类\n",
    "loader = CustomJSONLoader(\n",
    "    file_path=\"simple_prompt.json\", jq_schema=\"template\", text_content=True\n",
    ")\n",
    "data = loader.load()\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着是PDF的加载\n",
    "首先你需要安装一个包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='蒂法介绍蒂法·洛克哈特（⽇语：ティファ・ロックハート，Tifa Rokkuhāto，英语：Tifa Lockhart）为电⼦游戏《最终幻想VII》及《最终幻想VII补完计划》相关作品中的虚构⻆⾊，由野村哲也创作和设计，此后也在多个游戏中客串登场。2014年东京电玩展上，星名美津纪cosplay《最终幻想VII 降临之⼦》中的蒂法·洛克哈特蒂法是克劳德的⻘梅⽵⻢，两⼈同为尼布鲁海姆出身。在⽶德加经营作为反抗组织“雪崩”根据地的酒馆“第七天堂”，并且是⼩有名⽓的招牌店员。擅⻓格⽃，以拳套为武器。本传7年前克劳德离开故乡从军时，曾许下约定“如果有危机时⼀定会保护她”。与爱丽丝相识之后，两⼈成为好友。第⼀个察觉克劳德记忆混乱的⼈，后来协助精神崩溃的克劳德重新找回真正的⾃⼰。本传的⼤战结束后，依⼤家的期待在战后新⽣的⽶德加再次开设第七天堂（原第七天堂因第柒区圆盘崩塌遭压毁），同时也照顾⼀群受到星痕症候群折磨的孩⼦们。蒂法被《纽约时报》称为“⽹络⼀代”的海报⼥郎，与劳拉·克罗夫特相⽐，她是电⼦游戏中坚强、独⽴和有吸引⼒的⼥性⻆⾊的典型代表。媒体普遍称赞其实⼒和外表，并称她为游戏世界中最好的⼥性⻆⾊之⼀。在《最终幻想VII》本传中，蒂法年龄20岁、身⾼167cm、⽣⽇5⽉3⽇、⾎型B型、出⽣地尼布尔海姆。登场《最终幻想VII》蒂法在《最终幻想VII》原版中⾸次亮相，是克劳德的⻘梅⽵⻢、第七天堂酒吧的看板娘、极端环境组织“雪崩”成员，该组织反抗巨型企业“神罗”因其⼤量抽取魔晄⽤作动⼒能源。在注意到克劳德的性格改变后，她说服克劳德加⼊雪崩，以密切关注他，并且跟随他追寻游戏中的对⼿萨菲罗斯。她⽆法阻⽌克劳德被萨菲罗斯操纵，在他的精神崩溃后，她帮助克劳德康复，并且两⼈意识到彼此间的相互感觉，最后与伙伴们⼀同击败了萨菲罗斯。[2]在闪回中可知，⼉时的蒂法⼀直是村中⼩孩的⼈⽓王。在⺟亲过世后，思念⺟亲的蒂法决定沿着⼩路⾛到他们故乡尼布尔海姆附近的⼀座⼭上，认为这样就能⻅到过世的⺟亲，原本跟着蒂法的其他⼩孩都在半路上因害怕⽽放弃，唯独克劳德仍坚定的在后⾯跟随，希望能在危机时保护蒂法。然⽽，他们俩都从⼭上跌落受伤，蒂法昏迷了⼀个星期，她的⽗亲认为克劳德对此负有责任[3]，甚为严令禁⽌克劳德再接近蒂法，但蒂法反⽽从此更在意克劳德，两⼈成为要好的玩伴。为了使⾃⼰变得更强⼤，克劳德最终选择离开尼布尔海姆，加⼊神罗，想要成为神罗的精英战⼠“神罗战⼠”（SOLDIER），但后来透露他主要是为了吸引蒂法的注意⼒。离开之前，蒂法与克劳德约定，当蒂法处于困境之中时，克劳德会回来救她。从克劳德离开之后，蒂法便开始留意神罗战⼠的消息，因为神罗战⼠都成为声名远播的知名⼈物，如果克劳德成为神罗战⼠，他的活跃也会⽴刻传回尼布尔海姆。数年后，在萨菲罗斯摧毁了尼布尔海姆之后，克劳德为了救蒂法，被萨菲罗斯刺⾄重伤。蒂法被她的武术教练赞⼲带到安全地带，幸存下来，最终到达⽶德加并遇⻅了“雪崩”的领导⼈巴雷特·华莱⼠。病愈后，蒂法加⼊了“雪崩”，为了给家乡被毁⼀事报仇。⼀天，她在⽕⻋站遇到了从魔晄炉中逃出来、精神⼀⽚混乱的克劳德，蒂法说服了他为巴雷特⼯作，以保证克劳德的安全以及和克劳德保持紧密关系。这是游戏开始的地⽅。在原版《最终幻想VII》中蒂法与爱丽丝关系友好，但会在某些时候争⻛吃醋，例如在神罗总部营救爱丽丝时，蒂法及巴雷特等⼀⾏失⼿被擒，若克劳德选择关⼼爱丽丝的话蒂法的对话中明显带有妒忌。在重制版中虽然删去这段情节，但保留了这种关系。在《最终幻想VII》的初稿中，蒂法是背景⼈物。她在“雪崩”中的作⽤是在幕后⽀持，在执⾏任务后为所有⼈加油⿎劲，并且对克劳德有特别的关⼼。据推测，她的背上有⼀块⼤的疤', metadata={'source': './loader.pdf', 'page': 0}),\n",
       " Document(page_content='痕，是由克劳德造成的，并在事件发⽣时因巨⼤冲击⽽部分失忆。[4]原版策划者之⼀的加藤正⼈提出了⼀个旨在暗示蒂法和克劳德发⽣性关系的场景，但被北濑佳范⽤⼀个变淡的⾊调所取代。野岛⼀成在接受采访时说，没有⼀个开发团队⼈员认为当时的场景会成为⼀个问题。[5]《最终幻想VII补完计划》2005年，蒂法出现在CG电影《最终幻想VII 降临之⼦》中，故事发⽣在原版游戏剧情结束两年后。在其中，她试图给予克劳德情感上的⽀持，敦促克劳德放下他对⾃⼰施加的不必要的罪恶感。此外，她还照顾巴雷特的养⼥玛琳和克劳德在爱丽丝的教堂发⻔⼝救下的孩⼦丹泽尔。在电影中，她与萨菲罗斯的其中⼀个思念体罗兹战⽃，后来她帮助与被召唤的⽣物巴哈姆特战⽃。编剧野岛⼀成将她在视频中的⻆⾊描述为“⾮常像任何被男⼈抛弃的⼥⼈”，并表示尽管他们不希望她显得笨拙，但他们也想描绘出从她受到克劳德的情感伤害出发。[6]在视频的初稿中，她原本计划在当时的短⽚中扮演更重要的⻆⾊，该短⽚仅以蒂法，克劳德和⼏个孩⼦为主。蒂法也在前传游戏《最终幻想VII 危机之前》和《最终幻想VII 核⼼危机》以及OVA《最终幻想VII 最终命令》中登场。每次登场时，她的出现都与尼布尔海姆的毁灭有关。[2]官⽅⼩说《通向微笑之路》中有专⻔的《蒂法篇》，讲述了原版游戏和降临之⼦两段之间的故事。从蒂法的⻆度出发，详细讲述了她如何在Edge City创建⼀个新的第七天堂酒吧，并试图坚持⾃⼰和克劳德的正常家庭观念，尽管克劳德逐渐开始逃避与他⼈接触。蒂法还短暂出现在游戏《最终幻想VII 地狱⽝的挽歌》中，该游戏的剧情在降临之⼦故事⼀年后发⽣，她在游戏中帮助主⻆⽂森特·瓦伦丁捍卫星球，抵抗怪物欧⽶茄和“兵器”。她后来出现在游戏的结尾中，讨论着⽂森特的失踪。[2]其他登场在《最终幻想VII补完计划》之外，蒂法还出现在格⽃游戏《神佑擂台》中，作为可解锁的⻆⾊和可选的Boss。[7]她后来出现在电⼦棋盘游戏《富豪街》中。在《王国之⼼II》中，她穿着⾃⼰在降临之⼦中的服装，寻找克劳德，然后与该系列的怪物“⽆⼼”战⽃。她原本计划出现在原版《王国之⼼》的最终合辑中，但由于时间限制，⼯作⼈员选择改⽤萨菲罗斯。[8]蒂法也是格⽃游戏《最终幻想 纷争012》中的玩家⻆⾊之⼀，该游戏的⻆⾊来⾃各种《最终幻想》游戏。[9]她身着《最终幻想VII》中的服装，但该玩家还可以使⽤她在《降临之⼦》中的服装和在尼布尔海姆出场时展示的第三种服装。[10][11]游戏的第⼀版展示内容是在天野喜孝的美术原型基础上展现的另⼀种形式。[12]在《⼩⼩⼤星球2》中，蒂法是可下载的⻆⾊模型。[13]韩国歌⼿Ivy在2007年的歌曲“ਬ\\u0d11\\u0a44ਬ\\u0d11\\u0a44”（《诱惑奏鸣曲》）MV中描绘了⻆⾊。因为重现了《降临之⼦》中的战⽃场⾯，在史克威尔艾尼克斯提出版权诉讼后，该视频被禁⽌在韩国电视上播出。[14] 2015年，蒂法作为可玩⻆⾊被添加到iOS游戏《最终幻想 记录者》中。[15]创作与发展蒂法在《最终幻想VII 核⼼危机》中的形象。蒂法由野村哲也设计，最初版本的《最终幻想VII》中没有蒂法，因为最初该游戏只有三个可玩⻆⾊。主⻆克劳德、爱丽丝和巴雷特。但是，在给游戏总监北濑佳范打电话时，有⼈建议游戏中的某个主⻆应该死掉，在对究竟是巴雷特还是爱丽丝需要死掉进⾏⼤量讨论之后，制⽚⼈选择了爱丽丝。[3]野村哲也后来开玩笑说这是他的主意，以便使他能够将蒂法引⼊游戏中。[16]⽆论如何，北濑佳范喜欢有两个的⼥主⼈公，并且在他们之间摇摆不定，将其称之为是', metadata={'source': './loader.pdf', 'page': 1}),\n",
       " Document(page_content='《最终幻想》系列中的新事物。[5]野村哲也将蒂法的⻆⾊在《降临之⼦》中的表现分为多个⽅⾯，称她“像⺟亲，爱⼈，战⽃中的亲密盟友”，“不仅在情感上，⽽且在身体上也⾮常坚强”。[6]蒂法被设计为该系列以前的游戏中出现的“Monk”级别的⻆⾊。她有⼀头乌⿊的⻓发，像⼀头海豚的尾巴，穿着简单单调的服装，包括⽩⾊的上⾐和⿊⾊的超短裙。她还穿着红⾊的靴⼦和⼿套，⿊⾊的袖⼦从⼿腕延伸到肘部。她的裙⼦被⼀对狭窄的⿊⾊吊带裤撑起，⼤⾯积的⾦属护罩覆盖了她的左肘。她身⾼约167厘⽶[17]，三围为92-60-88厘⽶。[4]最初，野村哲也很难决定选择搭配迷你裙还是搭配⻓裤。为了征求意⻅，他在史克威尔的办公室传递了草图，⼤多数⼯作⼈员都认同超短裙设计。[16]这与爱丽丝的标志形成鲜明对⽐，爱丽丝的⻓裙是她的标志。服装在游戏中被解释为赋予了她⾏动⾃由的能⼒，这是由于她的近距离战⽃的亲和⼒，⽽裙⼦据称“相当短，具有相当⼤的曝光度”[4]。开发者还指出，由于她的身材，她穿着其他便⾐的外观也令⼈愉悦。[4]在制作《最终幻想VII 降临之⼦》时，联合导演野末武志难以为蒂法的身材构建出⼀个“平衡，但⼜展现了她的⼥性特质”的框架。在这⼀点上，她的服装也进⾏了重新设计，着重于表达这些品质，同时仍然令⼈赏⼼悦⽬。⽩⾊背⼼覆盖⿊⾊背⼼，⽩⾊缎带背⼼包裹着她的左⼆头肌，粉红丝带包裹着她的脚。⿊⾊系扣裙⼦遮盖了她的⼤腿，并且在下⾯穿了短裤，⽤⼀块类似于⼤⾐的布从裙⼦的腰带后部延伸到脚踝。她不再使⽤吊带撑起裙⼦，只在电影的战⽃场景中戴着⼿套。她的发型被更改为在她的背部中部结束，从原来的设计中删除了海豚的尾巴。[6]造成这种变化的原因是，难以为她的原始头发设置动画效果，以及由于其⿊⾊和亮光⽽引起的问题。另外，瞳孔的颜⾊亦由红⾊改为深褐⾊。蒂法在《最终幻想VII 重制版》中的形象。开发《最终幻想VII 重制版》时，史克威尔修改了蒂法的原始外观，以使她的外观更加逼真，因为⼯作⼈员意识到她的设计不适合战⽃场景。她因此得到了⿊⾊运动内⾐并贴合她的身体，给她⼀种运动的感觉。[18]在这个版本中的蒂法虽然保留着“海豚尾巴”发型，但原版中其末端如海豚尾鳍已变为更⾃然的垂直模样。另外,在第三章中,玩家可选三种服饰中的⼀种让蒂法在以后的篇章穿上。除了“成熟⻛”、“格⽃家⻛”及“异国⻛”外，开发团队亦考虑过“⼥仆⻛”、“警察服”、“舞蹈家⻛”、“陆⾏⻦⻛”等[19]。由于克劳德与蒂法和爱丽丝之间存在的特殊关系，开发团队观察到粉丝认为史克威尔更偏爱这两个⼥主⻆。以往，史克威尔表示爱丽丝可能是游戏的真正主⻆，⽽蒂法对于帮助克劳德这个⻆⾊的发展很重要。最终，史克威尔表示蒂法和爱丽丝都是这次重制版中的⼥主⻆。[20]野村哲也表示，他喜欢⼥演员伊藤步，并希望与她在《降临之⼦》中合作。在爱丽丝的配⾳演员已经决定之后，野村让伊藤步为蒂法配⾳，觉得她的“沙哑的声⾳”将与坂本真绫配⾳的轻声细语的爱丽丝形成很好的对⽐。[6]野村哲也还指出，在完成蒂法的更新设计后，制⽚⼈对她的最终细节进⾏了辩论，但伊藤步饰演⻆⾊后，便选择将配⾳演员的许多特征融合到⻆⾊的最终形象中。[6]英国配⾳演员瑞秋·莱·寇克（Rachael Leigh Cook）在《王国之⼼II》的⼀次采访中说，她喜欢玩蒂法，并形容她“身体和情感上都很强壮，但也⾮常敏感”，并且“⾮常多维”。[21]在给⻆⾊配⾳时，寇克听取了伊藤步的配⾳。在《降临之⼦》制作之后，寇克感谢野村哲也所创作的视频并表示⾮常喜欢。[22]布⾥特·巴伦（Britt Baron）代替她参加了《最终幻想VII 重制版》的配⾳。[23]蒂法幼年时候由Glory Curda配⾳。[24]评价⾃登场以来，蒂法受到评论家和粉丝的极⼤好评。在2000年，GameSpot的读者将她评为电⼦游戏中的第五好的⼥性⻆⾊，该⽹站的编辑指出他们对此表示同意。[25] 2004年，', metadata={'source': './loader.pdf', 'page': 2}),\n",
       " Document(page_content=\"《Play》杂志在《游戏⼥孩》年度期刊的第⼀期中介绍了蒂法，并称她为“近代史上最受崇拜的⼥性。”2007年，蒂法被《电击PlayStation》评为在原版PlayStation平台上有史以来第⼋好的⼈物，也是《最终幻想VII》中排名第三的⻆⾊。[26]同年，Tom's Hardware将她列为视频游戏历史上50个最伟⼤的⼥性⻆⾊之⼀，并称她为“周围⼈物中更丰富，更复杂的⼥性⻆⾊之⼀。”[27]在2008年，UGO将她列为电⼦游戏史上最伟⼤的⼥性⻆⾊之⼀，排在第五位，并表示对她的偏好超过了爱丽丝，并补充说：“ 蒂法的服装是轻描淡写的奇迹-但正是她的天⽣丽质和令⼈难忘的性格使她在榜单上名列前茅。”[28]同年，Chip杂志将她列为“游戏⼥孩”第⼗名。[29]2009年，IGN将蒂法列为游戏中⼗⼤最佳⼥主⻆之⼀，并称她为“毫⽆疑问的最终幻想宇宙中的传奇⼥主⻆。”[30] Fami通在2010年进⾏的⼀项⺠意调查将她评为第19⼤最受⽇本玩家欢迎的视频游戏⻆⾊。[31]在2013年，Complex将她列为电⼦游戏历史上第13⼤⼥主⻆。[32]2001年，随着劳拉·克罗夫特的推出，Beaumont Enterprise列举了蒂法作为视频游戏中强⼤⼥性⻆⾊的榜样。2008年，Joystiq将她列为他们希望在史克威尔艾尼克斯的跨界格⽃游戏《最终幻想 纷争》中看到的《最终幻想》系列中20个⻆⾊中的佼佼者，并将她描述为该系列“最伟⼤的⼥主⻆”之⼀。[33]IGN在2008年将蒂法列⼊有史以来第13部最佳的《最终幻想》⻆⾊之⼀，描述她是史克威尔试图“赋予《最终幻想》⻆⾊真实的性感”的尝试，以及“可以在紧要关头照顾好⾃⼰”的⼈；在后续的读者选择榜单中，蒂法排名第⼀，⼯作⼈员在将其在榜单上的位置归因于她的胸部时重复了先前的评论。[34]在2009年IGN的⼀篇⽂章中，仅关注《最终幻想VII》中的⻆⾊，她排在第四位，并评论说，尽管她的性感提⾼了她的知名度，但“ 蒂法推动了坚强，独⽴的RPG⼥主⻆的传统。”[35]其他杂志也称赞蒂法的性格特征。Mania Entertainment在2010年“了不起的视频游戏⼥性”名单中，蒂法排名第⼗，并指出，虽然《最终幻想》系列的后续游戏引⼊了其他令⼈难忘的⼥性⻆⾊，但“蒂法是我们的第⼀个《最终幻想》⼥孩，并在我们⼼中有特殊的地位”。[36] 2013年，《Complex》的Gus Turner将蒂法列为有史以来⼗⼆⼤最终幻想⻆⾊之⼀，并指出“除了劳拉和萨姆斯·艾仁之外，蒂法也代表了游戏中最独⽴，最有能⼒的⼥性之⼀。” [37]许多评论都认为蒂法⾮常性感。 1998年，《纽约时报》将她列为“⽹络⼀代”的炙⼿可热的海报⼥郎。[38]同年，《电⼦游戏⽉刊》授予她1997年的“最热⻔游戏宝⻉”，称她“⽐例匀称”，并称赞她是劳拉的另⼀可⾏选择。 UGO在2008年“电⼦游戏热⻔榜”中将她排在第24位，并补充说他们⽆法“克服之后的每个游戏版本中她的表现都更好”。[39]同年，GameDaily在“最热游戏辣妹”名单中将她排在第31位，分享了UGO对她的偏爱，并赞扬了她的外表和战⽃能⼒。[40]MSN也有类似的看法，当他们将“这个充满爱⼼，关怀，超级性感的⼥孩”列⼊“游戏界最热⻔的辣妹”名单中时，她名列第六，并指出她在该系列中的存在“有些微妙，给与了她更像是⼀种情感上的暗示。”⽽且没有她，这个系列就不会那么特别。[41]Manolith在2009年“最热⻔”⼥性视频游戏主⻆名单中将她排名第⼆。[42] 2010年，VideoGamer.com将她列为“⼗⼤电⼦游戏⼥神”，[43]⽽AfterEllen的塞拉·沃恩（Sarah Warn）将她评为“第九⼤最热”⼥性电⼦游戏⻆⾊。[44]2011年，Complex将她列为“游戏中最漂亮的⼥性⻆⾊”第16名，[45]⽽UGO仅因她在《神佑擂台》中的出现⽽将她列为“格⽃游戏中最优秀的⼥性”中的第13位。[46]同年，GameFront将她的胸部排在“电⼦游戏史上最伟⼤的胸部”名单的第九位，称她为“存在的危机版劳拉；” [47]她也被Village Voice Media列⼊“不可思议的胸部名单”，但评论说她“不仅仅具有性感。”[48]2012年，Complex将她列为整体上“第⼆热⻔”视频游戏⻆⾊，[49]MSN将她列为“电⼦游戏史上最热⻔的20位⼥性”之⼀，并补充说“她是历史上最著名的游戏⼥孩之⼀，并且具有永恒的吸引⼒。” [50]2013年，《每⽇纪事报》的Scott Marley将她排在“最有魅⼒的⼥性电⼦游戏⻆⾊”第⼆位，[51]⽽CheatCodes.com则宣布她是有史以来“最热⻔的⼥性\", metadata={'source': './loader.pdf', 'page': 3}),\n",
       " Document(page_content='电⼦游戏⻆⾊”。[52]同样，《La NuevaEspaña》在2014年将“性感，独⽴和坚强”的蒂法列为男⼥最性感的⼗⼤电⼦游戏⻆⾊中，[53]⽽《ThanhNiên》在2015年将她评为最性感的⼥性电⼦游戏⻆⾊。[54]姓名由来蒂法（Tifa）的英⽂名字来源于⼀个古希伯来语单词“Tiferet” ，⽽姓⽒洛克哈特（Lockhart）则是英语单词Lock和Hard的结合；Tiferet是⽣命之树的⼀个成分（⽣命之树⼤概可以分为三⽀柱、⼗个原质、四个世界、⼆⼗⼆路径等基本结构），象征着爱情、美丽和⾃我牺牲，从游戏⾥可以看出，这⼏点都符合Tifa的情况。', metadata={'source': './loader.pdf', 'page': 4})]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入模块\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./loader.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages\n",
    "# 这里可以用下标的方式将文本给取出来"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
